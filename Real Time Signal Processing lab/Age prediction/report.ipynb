{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talluri Surya Teja \n",
    "# 15EE35028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have manually chose data for classes (online standard datasets are of huge size... around 2GB)  \n",
    "- I chose 2 classes (child and adult) of age 2 and 20.\n",
    "- each class have a data of around 30 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After getting images i generated face log for those images and saved them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![31.jpg](attachment:31.jpg) \n",
    "![6.jpg](attachment:6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I split this face logs into train and validation data\n",
    "- I have extracted HOG features\n",
    "- Using this features, I have trained a SVM model and acheived an accuracy of 100% on both Train and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Time Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I have used Python 3.7 version to write the code (.py file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After training the model I have used Pickle library to save the model and reload during test phase\n",
    "- I have used arguments for the file\n",
    "- For testing we have to run:  python code_file.py --train \"path to trained model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Oriented Gradients\n",
    "\n",
    "In the HOG feature descriptor, the distribution ( histograms ) of directions of gradients ( oriented gradients ) are used as features. Gradients ( x and y derivatives ) of an image are useful because the magnitude of gradients is large around edges and corners ( regions of abrupt intensity changes ) and we know that edges and corners pack in a lot more information about object shape than flat regions.  \n",
    "\n",
    "To calculate a HOG descriptor, we need to first calculate the horizontal and vertical gradients; after all, we want to calculate the histogram of gradients. This is easily achieved by filtering the image with the following kernels.\n",
    "\n",
    "    [-1 0 1], [-1 0 1].T\n",
    "\n",
    "Next, we can find the magnitude and direction of gradient using the following formula\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shoews gradient\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the gradient angles are between 0 and 180 degrees instead of 0 to 360 degrees. These are called “unsigned” gradients because a gradient and it’s negative are represented by the same numbers.   \n",
    "\n",
    "Next we will calculate histograms, we will divide image into grids and divide each grid into bins. Based on angle value add magnitude to different bin\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we will normalize the histograms by taking blocks which contains 4 or more small blocks. we would like to “normalize” the histogram so they are not affected by lighting variations.\n",
    "\n",
    "After the normalization, we will append them to generate final features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have taken color image of size (150,150) for feature caluculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane/ line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the SVM algorithm we find the points closest to the line from both the classes.These points are called support vectors. Now, we compute the distance between the line and the support vectors. This distance is called the margin. Our goal is to maximize the margin. The hyperplane for which the margin is maximum is the optimal hyperplane.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from skimage import feature\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-model\", \"--test\",\n",
    "                help=\"saved model\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "data_paths = [os.path.join(r, n) for r, _, f in os.walk('data') for n in f]\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect(gray, frame, model, save = False):\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        age =  model.predict(hog(cv2.resize(gray[y:y+h, x:x+w], (150,150))).reshape(1,-1))\n",
    "        cv2.putText(frame, age[0], (x+w,y-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n",
    "        if save:\n",
    "            cv2.imwrite('data/'+str(i)+'.jpg', frame[y:y+h, x:x+w])\n",
    "    return frame\n",
    "\n",
    "def hog(image):\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.resize(image, (150,150))\n",
    "\n",
    "    image = feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), \n",
    "                        visualize=False, visualise=None, transform_sqrt=True, feature_vector=True, block_norm ='L1')\n",
    "    return image\n",
    "\n",
    "\n",
    "data = np.asarray([hog(i) for i in tqdm(data_paths)])\n",
    "labels = np.asarray([i.split('\\\\')[1] for i in tqdm(data_paths)])\n",
    "\n",
    "arr = np.random.permutation(len(labels))\n",
    "\n",
    "data = data[arr]\n",
    "labels = labels[arr]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size = 0.7, random_state = 42)\n",
    "\n",
    "model = LinearSVC(random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "print('train accuracy: ', model.score(x_train, y_train))\n",
    "print('test accuracy: ', model.score(x_test, y_test))\n",
    "pkl.dump(model, open('svm_model_hog', 'wb'))\n",
    "\n",
    "if args['test'] is not None:\n",
    "    model = pkl.load(open(args['test'], 'rb'))#### Support Vector Machine\n",
    "\n",
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane/ line).\n",
    "\n",
    "According to the SVM algorithm we find the points closest to the line from both the classes.These points are called support vectors. Now, we compute the distance between the line and the support vectors. This distance is called the margin. Our goal is to maximize the margin. The hyperplane for which the margin is maximum is the optimal hyperplane.\n",
    "![image.png](attachment:image.png)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        _, frame = video_capture.read()\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        canvas = detect(gray, frame, model)\n",
    "        cv2.imshow('Video', canvas)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
