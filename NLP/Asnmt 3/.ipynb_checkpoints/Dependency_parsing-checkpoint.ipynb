{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph, DependencyEvaluator, ParserI\n",
    "from nltk.parse.transitionparser import Transition\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import tempfile, pickle\n",
    "from os import remove\n",
    "from numpy import array\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "graph = DependencyGraph.load('UD_Hindi/hi-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Transition(object):\n",
    "#     \"\"\"\n",
    "#     This class defines a set of transition which is applied to a configuration to get another configuration\n",
    "#     Note that for different parsing algorithm, the transition is different.\n",
    "#     \"\"\"\n",
    "#     # Define set of transitions\n",
    "#     LEFT_ARC = 'LEFTARC'\n",
    "#     RIGHT_ARC = 'RIGHTARC'\n",
    "#     SHIFT = 'SHIFT'\n",
    "#     REDUCE = 'REDUCE'\n",
    "\n",
    "#     def __init__(self, alg_option):\n",
    "#         \"\"\"\n",
    "#         :param alg_option: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "#         :type alg_option: str\n",
    "#         \"\"\"\n",
    "#         self._algo = alg_option\n",
    "#         if alg_option not in [\n",
    "#                 TransitionParser.ARC_STANDARD,\n",
    "#                 TransitionParser.ARC_EAGER]:\n",
    "#             raise ValueError(\" Currently we only support %s and %s \" %\n",
    "#                                         (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER))\n",
    "\n",
    "#     def left_arc(self, conf, relation):\n",
    "#         \"\"\"\n",
    "#         Note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager\n",
    "#             :param configuration: is the current configuration\n",
    "#             :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "#         \"\"\"\n",
    "#         if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "#             return -1\n",
    "#         if conf.buffer[0] == 0:\n",
    "#             # here is the Root element\n",
    "#             return -1\n",
    "\n",
    "#         idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "\n",
    "#         flag = True\n",
    "#         if self._algo == TransitionParser.ARC_EAGER:\n",
    "#             for (idx_parent, r, idx_child) in conf.arcs:\n",
    "#                 if idx_child == idx_wi:\n",
    "#                     flag = False\n",
    "\n",
    "#         if flag:\n",
    "#             conf.stack.pop()\n",
    "#             idx_wj = conf.buffer[0]\n",
    "#             conf.arcs.append((idx_wj, relation, idx_wi))\n",
    "#         else:\n",
    "#             return -1\n",
    "\n",
    "#     def right_arc(self, conf, relation):\n",
    "#         \"\"\"\n",
    "#         Note that the algorithm for right-arc is DIFFERENT for arc-standard and arc-eager\n",
    "#             :param configuration: is the current configuration\n",
    "#             :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "#         \"\"\"\n",
    "#         if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "#             return -1\n",
    "#         if self._algo == TransitionParser.ARC_STANDARD:\n",
    "#             idx_wi = conf.stack.pop()\n",
    "#             idx_wj = conf.buffer[0]\n",
    "#             conf.buffer[0] = idx_wi\n",
    "#             conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "#         else:  # arc-eager\n",
    "#             idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "#             idx_wj = conf.buffer.pop(0)\n",
    "#             conf.stack.append(idx_wj)\n",
    "#             conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "\n",
    "#     def reduce(self, conf):\n",
    "#         \"\"\"\n",
    "#         Note that the algorithm for reduce is only available for arc-eager\n",
    "#             :param configuration: is the current configuration\n",
    "#             :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "#         \"\"\"\n",
    "\n",
    "#         if self._algo != TransitionParser.ARC_EAGER:\n",
    "#             return -1\n",
    "#         if len(conf.stack) <= 0:\n",
    "#             return -1\n",
    "\n",
    "#         idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "#         flag = False\n",
    "#         for (idx_parent, r, idx_child) in conf.arcs:\n",
    "#             if idx_child == idx_wi:\n",
    "#                 flag = True\n",
    "#         if flag:\n",
    "#             conf.stack.pop()  # reduce it\n",
    "#         else:\n",
    "#             return -1\n",
    "\n",
    "#     def shift(self, conf):\n",
    "#         \"\"\"\n",
    "#         Note that the algorithm for shift is the SAME for arc-standard and arc-eager\n",
    "#             :param configuration: is the current configuration\n",
    "#             :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "#         \"\"\"\n",
    "#         if len(conf.buffer) <= 0:\n",
    "#             return -1\n",
    "#         idx_wi = conf.buffer.pop(0)\n",
    "#         conf.stack.append(idx_wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        result = []\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            # if 'feats' in token and self._check_informative(token['feats']):\n",
    "            #     feats = token['feats'].split(\"|\")\n",
    "            #     for feat in feats:\n",
    "            #         result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            # if 'feats' in token and self._check_informative(token['feats']):\n",
    "            #     feats = token['feats'].split(\"|\")\n",
    "            #     for feat in feats:\n",
    "            #         result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, model_name = 'SVM', verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            if model_name == 'Logistic':\n",
    "                model = logistic()\n",
    "            elif model_name == \"Mlp\":\n",
    "                model = MLP()\n",
    "            else:\n",
    "                model = svm.SVC(\n",
    "                    kernel='poly',\n",
    "                    degree=2,\n",
    "                    coef0=0,\n",
    "                    gamma=0.2,\n",
    "                    C=0.5,\n",
    "                    verbose=verbose,\n",
    "                    probability=True)\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic():\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    return MLPClassifier(hidden_layer_sizes=(20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gettestdata(data):\n",
    "    result = []\n",
    "    for g in data:\n",
    "        new_g = deepcopy(g)\n",
    "        for key in new_g.nodes:\n",
    "            node = new_g.nodes[key]\n",
    "            node['head'] = None\n",
    "        result.append(new_g)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arc Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_stan = TransitionParser('arc-standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM:\n",
      "\n",
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "print(\"For SVM:\\n\")\n",
    "trans_eager.train(graph,'temp.arcstd.model', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic:\n",
      "\n",
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "print(\"For Logistic:\\n\")\n",
    "trans_stan.train(graph,'temp.arcstd.model_logistic', 'Logistic', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MLP:\n",
      "\n",
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "print(\"For MLP:\\n\")\n",
    "trans_stan.train(graph,'temp.arcstd.model_mlp', 'Mlp', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arc Eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_eager = TransitionParser('arc-eager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM:\n",
      "\n",
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "print(\"For SVM:\\n\")\n",
    "trans_eager.train(graph,'temp.arceager.model', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic:\n",
      "\n",
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "print(\"For Logistic:\\n\")\n",
    "trans_eager.train(graph,'temp.arceager.model_logistic', 'Logistic', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MLP:\n",
      "\n",
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "print(\"For MLP:\\n\")\n",
    "trans_eager.train(graph,'temp.arceager.model_mlp', 'Mlp', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "test_data = DependencyGraph.load('UD_Hindi/hi-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test1 = gettestdata(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arc-Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_stan = trans_stan.parse(test1, 'temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.800453514739229, 0.6923658352229781)\n"
     ]
    }
   ],
   "source": [
    "stan = DependencyEvaluator(result_stan, test_data)\n",
    "print(stan.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_stan_log = trans_stan.parse(test1, 'temp.arcstd.model_logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.800453514739229, 0.6923658352229781)\n"
     ]
    }
   ],
   "source": [
    "stan_log = DependencyEvaluator(result_stan_log, test_data)\n",
    "print(stan_log.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arc-Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_stan_mlp = trans_stan.parse(test1, 'temp.arcstd.model_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.799697656840514, 0.6764928193499622)\n"
     ]
    }
   ],
   "source": [
    "stan_mlp = DependencyEvaluator(result_stan_mlp, test_data)\n",
    "print(stan_mlp.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arc-Eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_eager_log = trans_eager.parse(test1, 'temp.arceager.model_logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8510959939531368, 0.7377173091458806)\n"
     ]
    }
   ],
   "source": [
    "eager_log = DependencyEvaluator(result_eager_log, test_data)\n",
    "print(eager_log.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arc-Eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_eager_mlp = trans_eager.parse(test1, 'temp.arceager.model_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8314436885865457, 0.7105064247921391)\n"
     ]
    }
   ],
   "source": [
    "eager_mlp = DependencyEvaluator(result_eager_mlp, test_data)\n",
    "print(eager_mlp.eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
