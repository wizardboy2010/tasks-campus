{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM3EU2xZm7EL"
   },
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbHcPWVIm7EO"
   },
   "source": [
    "### Write the following details here\n",
    "** Name: ** `Talluri Surya Teja`<br/>\n",
    "** Roll Number: ** `15EE35028`<br/>\n",
    "** Department: ** `Electrical Engineering`<br/>\n",
    "** Email: ** `tsuryateja.iitkgp@gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q1Xtnc8m7ES"
   },
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ku7HUEtLm7EU"
   },
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EV5mRpGm7EX"
   },
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "597wDiAvGvuB",
    "outputId": "e8c73aa2-121c-4a1b-eb51-9c48b2009e9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "dbaa766d-9595-495e-b843-5fe51edbcd22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "# print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "801aed13-5dd8-4a14-a0d8-443a594a59c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.normal(0,0.1,[input_dim, hidden_1_dim])\n",
    "W2 = np.random.normal(0,0.1,[hidden_1_dim, hidden_2_dim])\n",
    "W3 = np.random.normal(0,0.1,[hidden_2_dim, output_dim])\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "    z = np.exp(x)\n",
    "    y_pred = z / np.array([np.sum(z, axis=1)]).T\n",
    "    \n",
    "    ####################################################################################\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1220
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "1b84430f-e134-41a3-f9fd-1460c9cb60ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 3.1354 \n",
      " Epoch: 1, iteration: 937, Loss: 0.3345 \n",
      " Epoch: 2, iteration: 1874, Loss: 0.2556 \n",
      " Epoch: 3, iteration: 2811, Loss: 0.2130 \n",
      " Epoch: 4, iteration: 3748, Loss: 0.1857 \n",
      " Epoch: 5, iteration: 4685, Loss: 0.1664 \n",
      " Epoch: 6, iteration: 5622, Loss: 0.1531 \n",
      " Epoch: 7, iteration: 6559, Loss: 0.1425 \n",
      " Epoch: 8, iteration: 7496, Loss: 0.1336 \n",
      " Epoch: 9, iteration: 8433, Loss: 0.1270 \n",
      " Epoch: 10, iteration: 9370, Loss: 0.1205 \n",
      " Epoch: 11, iteration: 10307, Loss: 0.1145 \n",
      " Epoch: 12, iteration: 11244, Loss: 0.1092 \n",
      " Epoch: 13, iteration: 12181, Loss: 0.1040 \n",
      " Epoch: 14, iteration: 13118, Loss: 0.0987 \n",
      " Epoch: 15, iteration: 14055, Loss: 0.0937 \n",
      " Epoch: 16, iteration: 14992, Loss: 0.0890 \n",
      " Epoch: 17, iteration: 15929, Loss: 0.0848 \n",
      " Epoch: 18, iteration: 16866, Loss: 0.0810 \n",
      " Epoch: 19, iteration: 17803, Loss: 0.0775 \n",
      " Epoch: 20, iteration: 18740, Loss: 0.0736 \n",
      " Epoch: 21, iteration: 19677, Loss: 0.0700 \n",
      " Epoch: 22, iteration: 20614, Loss: 0.0666 \n",
      " Epoch: 23, iteration: 21551, Loss: 0.0635 \n",
      " Epoch: 24, iteration: 22488, Loss: 0.0607 \n",
      " Epoch: 25, iteration: 23425, Loss: 0.0582 \n",
      " Epoch: 26, iteration: 24362, Loss: 0.0555 \n",
      " Epoch: 27, iteration: 25299, Loss: 0.0533 \n",
      " Epoch: 28, iteration: 26236, Loss: 0.0515 \n",
      " Epoch: 29, iteration: 27173, Loss: 0.0498 \n",
      " Epoch: 30, iteration: 28110, Loss: 0.0481 \n",
      " Epoch: 31, iteration: 29047, Loss: 0.0464 \n",
      " Epoch: 32, iteration: 29984, Loss: 0.0449 \n",
      " Epoch: 33, iteration: 30921, Loss: 0.0434 \n",
      " Epoch: 34, iteration: 31858, Loss: 0.0421 \n",
      " Epoch: 35, iteration: 32795, Loss: 0.0458 \n",
      " Epoch: 36, iteration: 33732, Loss: 0.0462 \n",
      " Epoch: 37, iteration: 34669, Loss: 0.0463 \n",
      " Epoch: 38, iteration: 35606, Loss: 0.0464 \n",
      " Epoch: 39, iteration: 36543, Loss: 0.0463 \n",
      " Epoch: 40, iteration: 37480, Loss: 0.0463 \n",
      " Epoch: 41, iteration: 38417, Loss: 0.0462 \n",
      " Epoch: 42, iteration: 39354, Loss: 0.0461 \n",
      " Epoch: 43, iteration: 40291, Loss: 0.0460 \n",
      " Epoch: 44, iteration: 41228, Loss: 0.0459 \n",
      " Epoch: 45, iteration: 42165, Loss: 0.0458 \n",
      " Epoch: 46, iteration: 43102, Loss: 0.0457 \n",
      " Epoch: 47, iteration: 44039, Loss: 0.0456 \n",
      " Epoch: 48, iteration: 44976, Loss: 0.0456 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAFMCAYAAABs7QAAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKkATOASZIkDdj/B6sPWo8i9FwYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 45000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    y_batchinput = y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 = x_batchinput.dot(W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1,0)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = h1.dot(W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2,0)\n",
    "    #implement linear output layer\n",
    "    a3 = h2.dot(W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "\n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%35 == 0:\n",
    "            learning_rate /= 10\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    grad_softmax_score = -y_batchinput+softmax_score\n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = h2.T.dot(grad_softmax_score)/len(y_batchinput)\n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 = grad_softmax_score.dot(W3.T)\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = np.where(a2>0, grad_h2, 0)\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = h1.T.dot(grad_h2)/len(y_batchinput)\n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = grad_a2.dot(W2.T)\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = np.where(a1>0, grad_h1, 0)\n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = x_batchinput.T.dot(grad_h1)/len(y_batchinput)\n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQjp-QM2m7F3"
   },
   "outputs": [],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LMoyvAvym7F8",
    "outputId": "03f69ca6-6034-4a8b-b959-8bc4e371624d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 97.10 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    a1 = x_batchinput.dot(W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1,0)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = h1.dot(W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2,0)\n",
    "    #implement linear output layer\n",
    "    a3 = h2.dot(W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_01_problem_02.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
