{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of aae.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1ev-hJqT-DiR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions.normal import Normal\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdXcdVlT-DiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "b18f92c5-a57c-49e6-b672-fafb1498a62d"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset \n",
        "mnist = dsets.MNIST(root='./data', \n",
        "                      train=True, \n",
        "                      transform=transforms.ToTensor(),  \n",
        "                      download=True)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 21345371.78it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 329246.36it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5144760.04it/s]                           \n",
            "8192it [00:00, 130841.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T9lV4Xqm-Dif",
        "colab_type": "code",
        "outputId": "28f1d6d6-0004-4fa2-842b-813be3201cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  ################# uses gpu if available\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sblEKIOV-Dip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_np(x):\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "def to_var(x):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycBcHLJW-Div",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class aae_encoder(nn.Module):\n",
        "    def __init__(self, X_dim):\n",
        "        super(aae_encoder, self).__init__()\n",
        "        self.input_size = X_dim\n",
        "        \n",
        "        \n",
        "        self.layer1 = nn.Linear(X_dim, 400)\n",
        "        self.layer2 = nn.Linear(400, 100)\n",
        "        \n",
        "        ######## mean and variance for gauss distribution\n",
        "        self.mean = nn.Linear(100, 2)\n",
        "        self.variance = nn.Linear(100, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.layer1(x), p=0.2)\n",
        "        x = F.leaky_relu(x, 0.2)\n",
        "        x = F.dropout(self.layer2(x), p=0.2)\n",
        "        x = F.leaky_relu(x, 0.2)\n",
        "        \n",
        "        ####### gaussion distribution\n",
        "        mean_layer = self.mean(x)\n",
        "        variance_layer = self.variance(x)\n",
        "        gauss_layer = Normal(torch.tensor(mean_layer), torch.tensor(variance_layer))\n",
        "        xgauss = gauss_layer.sample()+mean_layer\n",
        "        \n",
        "        return xgauss\n",
        "    \n",
        "class aae_decoder(nn.Module):\n",
        "    def __init__(self, X_dim):\n",
        "        super(aae_decoder, self).__init__()\n",
        "        self.output_size = X_dim\n",
        "        \n",
        "        self.layer1 = nn.Linear(12, 100)\n",
        "        self.layer2 = nn.Linear(100, 400)\n",
        "        self.layer3 = nn.Linear(400, X_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.layer1(x), p=0.2)\n",
        "        x = F.leaky_relu(x, 0.2)\n",
        "        x = F.dropout(self.layer2(x), p=0.2)\n",
        "        x = F.leaky_relu(x, 0.2)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return torch.sigmoid(x)\n",
        "    \n",
        "class aae_discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(aae_discriminator, self).__init__()\n",
        "        \n",
        "        self.layer1 = nn.Linear(2, 10)\n",
        "        self.layer2 = nn.Linear(10, 10)\n",
        "        self.layer3 = nn.Linear(10, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.layer1(x), p=0.2)\n",
        "        x = F.leaky_relu(x, 0.2)\n",
        "        x = F.dropout(self.layer2(x), p=0.2)\n",
        "        x = F.leaky_relu(x, 0.2)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return torch.sigmoid(x)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dF_a9HZo-Di1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AAE(nn.Module):\n",
        "    def __init__(self, encoder, decoder, discriminator):\n",
        "        super(AAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.discriminator = discriminator\n",
        "        \n",
        "    def forward(self, x):\n",
        "        encoder_output = self.encoder(x)\n",
        "        decoder_output = self.decoder(encoder_output)\n",
        "        discriminator_output = self.discriminator(encoder_output)\n",
        "        \n",
        "        return decoder_output, discriminator_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuqtK6RMPcVn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_categorical(labels, n_classes=10):\n",
        "    cat = np.array(labels.data.tolist())\n",
        "    cat = np.eye(n_classes)[cat].astype('float32')\n",
        "    cat = torch.from_numpy(cat)\n",
        "    return Variable(cat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFLhMPRV-Di5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPS = 1e-15\n",
        "N = 512\n",
        "latent_dim = 2\n",
        "encoder = aae_encoder(784)\n",
        "decoder = aae_decoder(784)\n",
        "discriminator = aae_discriminator()\n",
        "\n",
        "aae = AAE(encoder, decoder, discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQKmx0uK-Di_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set learning rates\n",
        "gen_lr = 0.00005\n",
        "reg_lr = 0.00005\n",
        "\n",
        "#encode/decode optimizers\n",
        "encoder_optim = torch.optim.Adam(encoder.parameters(), lr=gen_lr)\n",
        "decoder_optim = torch.optim.Adam(decoder.parameters(), lr=gen_lr)\n",
        "#regularizing optimizers\n",
        "encoder_gen_optim = torch.optim.Adam(decoder.parameters(), lr=reg_lr)\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=reg_lr)\n",
        "\n",
        "data_iter = iter(data_loader)\n",
        "iter_per_epoch = len(data_loader)\n",
        "total_step = 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I3UbRGRWE3s2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "    discriminator.cuda()\n",
        "    aae.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_Civg_n-DjH",
        "colab_type": "code",
        "outputId": "513d2eec-5a66-4bf2-f4ef-b9b9a0f75207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88943
        }
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "\n",
        "reconstruction_loss = []\n",
        "discriminator_loss = []\n",
        "generator_loss = []\n",
        "gen_learning_rate = []\n",
        "reg_learning_rate = []\n",
        "\n",
        "# z_real_gauss = Variable(torch.randn(batch_size, latent_dim) * 5.)\n",
        "\n",
        "for step in range(total_step):\n",
        "    \n",
        "    if (step+1)%10000 and step<30000 == 0:\n",
        "        gen_lr /= 2\n",
        "        reg_lr /= 2\n",
        "\n",
        "    gen_learning_rate.append(gen_lr)\n",
        "    reg_learning_rate.append(reg_lr)\n",
        "    \n",
        "    # Reset the data_iter\n",
        "    if (step+1) % iter_per_epoch == 0:\n",
        "        data_iter = iter(data_loader)\n",
        "    try:\n",
        "      # Fetch the images and labels and convert them to variables\n",
        "      images, labels = next(data_iter)\n",
        "      images, labels = to_var(images.view(batch_size, -1)), to_var(labels)\n",
        "\n",
        "      #reconstruction loss\n",
        "      decoder.zero_grad()\n",
        "      encoder.zero_grad()\n",
        "      discriminator.zero_grad()\n",
        "\n",
        "\n",
        "  #       X_sample, pred = aae(images)\n",
        "\n",
        "      z_gauss = encoder(images)\n",
        "      z_cat = get_categorical(labels, n_classes=10)\n",
        "      if torch.cuda.is_available():\n",
        "          z_cat = z_cat.cuda()\n",
        "\n",
        "      z_sample = torch.cat((z_cat, z_gauss), 1)\n",
        "\n",
        "      X_sample = decoder(z_sample)\n",
        "\n",
        "      recon_loss = F.binary_cross_entropy(X_sample+EPS,images+EPS)\n",
        "\n",
        "      recon_loss.backward()\n",
        "      decoder_optim.step()\n",
        "      encoder_optim.step()\n",
        "\n",
        "    # Discriminator\n",
        "    ## true prior is random normal (randn)\n",
        "    ## this is constraining the Z-projection to be normal!\n",
        "      encoder.eval()\n",
        "  #     z_real_gauss = Variable(torch.randn(images.size()[0], latent_dim) * 5.).cuda()\n",
        "  #     z_real_gauss = Variable(torch.randn(batch_size, latent_dim) * 5., )\n",
        "\n",
        "      z_gauss = encoder(images)\n",
        "      pred = discriminator(z_gauss)\n",
        "      z_real_gauss = torch.randn(images.size()[0], latent_dim, device = device) * 5.\n",
        "      Disc_real_gauss = discriminator(z_real_gauss)\n",
        "\n",
        "      Disc_loss = -torch.mean(torch.log(Disc_real_gauss + EPS) + torch.log(1 - pred + EPS))\n",
        "\n",
        "      Disc_loss.backward(retain_graph=True)\n",
        "      discriminator_optim.step()\n",
        "\n",
        "    # Generator\n",
        "      encoder.train()\n",
        "      z_fake_gauss = encoder(images)\n",
        "      Disc_fake_gauss = discriminator(z_fake_gauss)\n",
        "\n",
        "      Gen_loss = -torch.mean(torch.log(Disc_fake_gauss + EPS))\n",
        "\n",
        "      Gen_loss.backward()\n",
        "      encoder_gen_optim.step()   \n",
        "\n",
        "\n",
        "      if (step+1) % 10 == 0:\n",
        "        print('For Step:', step+1 ,'recon_loss:', recon_loss.item(),\n",
        "        '\\tdiscriminator_loss:', Disc_loss.item(),\n",
        "        '\\tgenerator_loss:', Gen_loss.item())\n",
        "        reconstruction_loss.append(recon_loss.item())\n",
        "        discriminator_loss.append(Disc_loss.item())\n",
        "        generator_loss.append(Gen_loss.item())\n",
        "    except:\n",
        "      pass\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "For Step: 10 recon_loss: 0.6854108572006226 \tdiscriminator_loss: 1.4154573678970337 \tgenerator_loss: 0.7838004231452942\n",
            "For Step: 20 recon_loss: 0.6743863821029663 \tdiscriminator_loss: 1.4732321500778198 \tgenerator_loss: 0.7836883068084717\n",
            "For Step: 30 recon_loss: 0.6603026390075684 \tdiscriminator_loss: 1.3835822343826294 \tgenerator_loss: 0.7872207760810852\n",
            "For Step: 40 recon_loss: 0.634096086025238 \tdiscriminator_loss: 1.3958449363708496 \tgenerator_loss: 0.7769699692726135\n",
            "For Step: 50 recon_loss: 0.5906427502632141 \tdiscriminator_loss: 1.3698089122772217 \tgenerator_loss: 0.741889238357544\n",
            "For Step: 60 recon_loss: 0.523400604724884 \tdiscriminator_loss: 1.4321684837341309 \tgenerator_loss: 0.6854424476623535\n",
            "For Step: 70 recon_loss: 0.42255887389183044 \tdiscriminator_loss: 1.5095325708389282 \tgenerator_loss: 0.6356086730957031\n",
            "For Step: 80 recon_loss: 0.36369454860687256 \tdiscriminator_loss: 1.5137591361999512 \tgenerator_loss: 0.6097424030303955\n",
            "For Step: 90 recon_loss: 0.32127875089645386 \tdiscriminator_loss: 1.5747545957565308 \tgenerator_loss: 0.5749765634536743\n",
            "For Step: 100 recon_loss: 0.30553075671195984 \tdiscriminator_loss: 1.58897864818573 \tgenerator_loss: 0.5788840055465698\n",
            "For Step: 110 recon_loss: 0.3025764524936676 \tdiscriminator_loss: 1.5803329944610596 \tgenerator_loss: 0.5590001344680786\n",
            "For Step: 120 recon_loss: 0.310918927192688 \tdiscriminator_loss: 1.5573067665100098 \tgenerator_loss: 0.5859763622283936\n",
            "For Step: 130 recon_loss: 0.282848984003067 \tdiscriminator_loss: 1.5946288108825684 \tgenerator_loss: 0.588688850402832\n",
            "For Step: 140 recon_loss: 0.28415340185165405 \tdiscriminator_loss: 1.596005916595459 \tgenerator_loss: 0.617696225643158\n",
            "For Step: 150 recon_loss: 0.28510528802871704 \tdiscriminator_loss: 1.5464897155761719 \tgenerator_loss: 0.578802764415741\n",
            "For Step: 160 recon_loss: 0.28101596236228943 \tdiscriminator_loss: 1.4882670640945435 \tgenerator_loss: 0.6242971420288086\n",
            "For Step: 170 recon_loss: 0.2760094404220581 \tdiscriminator_loss: 1.5828452110290527 \tgenerator_loss: 0.6236180067062378\n",
            "For Step: 180 recon_loss: 0.27739080786705017 \tdiscriminator_loss: 1.4683821201324463 \tgenerator_loss: 0.6612337827682495\n",
            "For Step: 190 recon_loss: 0.2993921637535095 \tdiscriminator_loss: 1.5799365043640137 \tgenerator_loss: 0.6299864649772644\n",
            "For Step: 200 recon_loss: 0.26957541704177856 \tdiscriminator_loss: 1.487661361694336 \tgenerator_loss: 0.6681534051895142\n",
            "For Step: 210 recon_loss: 0.27764055132865906 \tdiscriminator_loss: 1.5770132541656494 \tgenerator_loss: 0.6432517766952515\n",
            "For Step: 220 recon_loss: 0.2830527424812317 \tdiscriminator_loss: 1.422843098640442 \tgenerator_loss: 0.6981985569000244\n",
            "For Step: 230 recon_loss: 0.2867349088191986 \tdiscriminator_loss: 1.461730718612671 \tgenerator_loss: 0.642342209815979\n",
            "For Step: 240 recon_loss: 0.2783242166042328 \tdiscriminator_loss: 1.4595615863800049 \tgenerator_loss: 0.6576797962188721\n",
            "For Step: 250 recon_loss: 0.2695886194705963 \tdiscriminator_loss: 1.4184467792510986 \tgenerator_loss: 0.6384705305099487\n",
            "For Step: 260 recon_loss: 0.2925589680671692 \tdiscriminator_loss: 1.4524619579315186 \tgenerator_loss: 0.6770812273025513\n",
            "For Step: 270 recon_loss: 0.27830591797828674 \tdiscriminator_loss: 1.4391875267028809 \tgenerator_loss: 0.7166257500648499\n",
            "For Step: 280 recon_loss: 0.26019856333732605 \tdiscriminator_loss: 1.4316363334655762 \tgenerator_loss: 0.7125463485717773\n",
            "For Step: 290 recon_loss: 0.28686362504959106 \tdiscriminator_loss: 1.4791033267974854 \tgenerator_loss: 0.6921700239181519\n",
            "For Step: 300 recon_loss: 0.2777688205242157 \tdiscriminator_loss: 1.3981564044952393 \tgenerator_loss: 0.7193318009376526\n",
            "For Step: 310 recon_loss: 0.26417067646980286 \tdiscriminator_loss: 1.4208691120147705 \tgenerator_loss: 0.7245776057243347\n",
            "For Step: 320 recon_loss: 0.2644692361354828 \tdiscriminator_loss: 1.3893206119537354 \tgenerator_loss: 0.7279098033905029\n",
            "For Step: 330 recon_loss: 0.2783707082271576 \tdiscriminator_loss: 1.3527321815490723 \tgenerator_loss: 0.7291513681411743\n",
            "For Step: 340 recon_loss: 0.269659161567688 \tdiscriminator_loss: 1.3225867748260498 \tgenerator_loss: 0.7213485240936279\n",
            "For Step: 350 recon_loss: 0.28128066658973694 \tdiscriminator_loss: 1.3573360443115234 \tgenerator_loss: 0.7710843086242676\n",
            "For Step: 360 recon_loss: 0.2718443274497986 \tdiscriminator_loss: 1.3812538385391235 \tgenerator_loss: 0.7129737734794617\n",
            "For Step: 370 recon_loss: 0.26288753747940063 \tdiscriminator_loss: 1.3506108522415161 \tgenerator_loss: 0.7312192916870117\n",
            "For Step: 380 recon_loss: 0.27251026034355164 \tdiscriminator_loss: 1.3550934791564941 \tgenerator_loss: 0.7379723787307739\n",
            "For Step: 390 recon_loss: 0.2594340145587921 \tdiscriminator_loss: 1.304458737373352 \tgenerator_loss: 0.7887051701545715\n",
            "For Step: 400 recon_loss: 0.2630448341369629 \tdiscriminator_loss: 1.2987637519836426 \tgenerator_loss: 0.7983688116073608\n",
            "For Step: 410 recon_loss: 0.25909245014190674 \tdiscriminator_loss: 1.3282790184020996 \tgenerator_loss: 0.743165135383606\n",
            "For Step: 420 recon_loss: 0.26878607273101807 \tdiscriminator_loss: 1.3169307708740234 \tgenerator_loss: 0.7885728478431702\n",
            "For Step: 430 recon_loss: 0.26028046011924744 \tdiscriminator_loss: 1.3124043941497803 \tgenerator_loss: 0.7661913633346558\n",
            "For Step: 440 recon_loss: 0.265653133392334 \tdiscriminator_loss: 1.2991770505905151 \tgenerator_loss: 0.7711037397384644\n",
            "For Step: 450 recon_loss: 0.2672506868839264 \tdiscriminator_loss: 1.303662896156311 \tgenerator_loss: 0.7851767539978027\n",
            "For Step: 460 recon_loss: 0.2652919292449951 \tdiscriminator_loss: 1.2687665224075317 \tgenerator_loss: 0.8325841426849365\n",
            "For Step: 470 recon_loss: 0.2650418281555176 \tdiscriminator_loss: 1.264897108078003 \tgenerator_loss: 0.804402232170105\n",
            "For Step: 480 recon_loss: 0.26723426580429077 \tdiscriminator_loss: 1.3299083709716797 \tgenerator_loss: 0.8322517275810242\n",
            "For Step: 490 recon_loss: 0.2736953794956207 \tdiscriminator_loss: 1.2659454345703125 \tgenerator_loss: 0.8003453016281128\n",
            "For Step: 500 recon_loss: 0.2544421851634979 \tdiscriminator_loss: 1.2517480850219727 \tgenerator_loss: 0.8403648138046265\n",
            "For Step: 510 recon_loss: 0.26901647448539734 \tdiscriminator_loss: 1.3010197877883911 \tgenerator_loss: 0.8299535512924194\n",
            "For Step: 520 recon_loss: 0.264092355966568 \tdiscriminator_loss: 1.2731609344482422 \tgenerator_loss: 0.814056932926178\n",
            "For Step: 530 recon_loss: 0.2879859507083893 \tdiscriminator_loss: 1.247295618057251 \tgenerator_loss: 0.8353995680809021\n",
            "For Step: 540 recon_loss: 0.26284971833229065 \tdiscriminator_loss: 1.2480437755584717 \tgenerator_loss: 0.8734776973724365\n",
            "For Step: 550 recon_loss: 0.2763805687427521 \tdiscriminator_loss: 1.1996092796325684 \tgenerator_loss: 0.8522411584854126\n",
            "For Step: 560 recon_loss: 0.24032273888587952 \tdiscriminator_loss: 1.2844270467758179 \tgenerator_loss: 0.8435312509536743\n",
            "For Step: 570 recon_loss: 0.27421802282333374 \tdiscriminator_loss: 1.3129417896270752 \tgenerator_loss: 0.8404700756072998\n",
            "For Step: 580 recon_loss: 0.25649476051330566 \tdiscriminator_loss: 1.2411203384399414 \tgenerator_loss: 0.84576416015625\n",
            "For Step: 590 recon_loss: 0.2528607249259949 \tdiscriminator_loss: 1.1848015785217285 \tgenerator_loss: 0.8360314965248108\n",
            "For Step: 600 recon_loss: 0.25300705432891846 \tdiscriminator_loss: 1.226151466369629 \tgenerator_loss: 0.8530409336090088\n",
            "For Step: 610 recon_loss: 0.2660136818885803 \tdiscriminator_loss: 1.2238829135894775 \tgenerator_loss: 0.8650976419448853\n",
            "For Step: 620 recon_loss: 0.2500245273113251 \tdiscriminator_loss: 1.2123208045959473 \tgenerator_loss: 0.8930624723434448\n",
            "For Step: 630 recon_loss: 0.2597413659095764 \tdiscriminator_loss: 1.1917657852172852 \tgenerator_loss: 0.807753324508667\n",
            "For Step: 640 recon_loss: 0.2589655816555023 \tdiscriminator_loss: 1.1921510696411133 \tgenerator_loss: 0.8751422166824341\n",
            "For Step: 650 recon_loss: 0.2643289268016815 \tdiscriminator_loss: 1.2069380283355713 \tgenerator_loss: 0.8498755693435669\n",
            "For Step: 660 recon_loss: 0.25578412413597107 \tdiscriminator_loss: 1.1723756790161133 \tgenerator_loss: 0.869403600692749\n",
            "For Step: 670 recon_loss: 0.2442639023065567 \tdiscriminator_loss: 1.237917423248291 \tgenerator_loss: 0.8944461345672607\n",
            "For Step: 680 recon_loss: 0.26055973768234253 \tdiscriminator_loss: 1.232743263244629 \tgenerator_loss: 0.870476245880127\n",
            "For Step: 690 recon_loss: 0.2550206482410431 \tdiscriminator_loss: 1.2206047773361206 \tgenerator_loss: 0.8735649585723877\n",
            "For Step: 700 recon_loss: 0.2614343464374542 \tdiscriminator_loss: 1.1508032083511353 \tgenerator_loss: 0.8710997104644775\n",
            "For Step: 710 recon_loss: 0.24169909954071045 \tdiscriminator_loss: 1.177843451499939 \tgenerator_loss: 0.8731062412261963\n",
            "For Step: 720 recon_loss: 0.24288655817508698 \tdiscriminator_loss: 1.2332360744476318 \tgenerator_loss: 0.8630785942077637\n",
            "For Step: 730 recon_loss: 0.2601028382778168 \tdiscriminator_loss: 1.2251943349838257 \tgenerator_loss: 0.8653712272644043\n",
            "For Step: 740 recon_loss: 0.23463256657123566 \tdiscriminator_loss: 1.1928192377090454 \tgenerator_loss: 0.8752351999282837\n",
            "For Step: 750 recon_loss: 0.26134806871414185 \tdiscriminator_loss: 1.198072910308838 \tgenerator_loss: 0.8753770589828491\n",
            "For Step: 760 recon_loss: 0.24576525390148163 \tdiscriminator_loss: 1.184239149093628 \tgenerator_loss: 0.8856298327445984\n",
            "For Step: 770 recon_loss: 0.2558056712150574 \tdiscriminator_loss: 1.1879611015319824 \tgenerator_loss: 0.876888632774353\n",
            "For Step: 780 recon_loss: 0.25041109323501587 \tdiscriminator_loss: 1.182527780532837 \tgenerator_loss: 0.8876796960830688\n",
            "For Step: 790 recon_loss: 0.2547793984413147 \tdiscriminator_loss: 1.2434537410736084 \tgenerator_loss: 0.9131977558135986\n",
            "For Step: 800 recon_loss: 0.24635054171085358 \tdiscriminator_loss: 1.1926928758621216 \tgenerator_loss: 0.8779026865959167\n",
            "For Step: 810 recon_loss: 0.25375673174858093 \tdiscriminator_loss: 1.1465049982070923 \tgenerator_loss: 0.8802617788314819\n",
            "For Step: 820 recon_loss: 0.2567770183086395 \tdiscriminator_loss: 1.1996451616287231 \tgenerator_loss: 0.8709999918937683\n",
            "For Step: 830 recon_loss: 0.24205990135669708 \tdiscriminator_loss: 1.1945579051971436 \tgenerator_loss: 0.8818132877349854\n",
            "For Step: 840 recon_loss: 0.25118207931518555 \tdiscriminator_loss: 1.1913883686065674 \tgenerator_loss: 0.8531864881515503\n",
            "For Step: 850 recon_loss: 0.23410536348819733 \tdiscriminator_loss: 1.1550979614257812 \tgenerator_loss: 0.892021119594574\n",
            "For Step: 860 recon_loss: 0.25966525077819824 \tdiscriminator_loss: 1.1682980060577393 \tgenerator_loss: 0.8605625629425049\n",
            "For Step: 870 recon_loss: 0.23873212933540344 \tdiscriminator_loss: 1.2127537727355957 \tgenerator_loss: 0.8584645986557007\n",
            "For Step: 880 recon_loss: 0.2516411244869232 \tdiscriminator_loss: 1.223373293876648 \tgenerator_loss: 0.869368851184845\n",
            "For Step: 890 recon_loss: 0.2420576810836792 \tdiscriminator_loss: 1.2148277759552002 \tgenerator_loss: 0.85323166847229\n",
            "For Step: 900 recon_loss: 0.24609947204589844 \tdiscriminator_loss: 1.2295117378234863 \tgenerator_loss: 0.8394589424133301\n",
            "For Step: 910 recon_loss: 0.2385610044002533 \tdiscriminator_loss: 1.2083511352539062 \tgenerator_loss: 0.8659958243370056\n",
            "For Step: 920 recon_loss: 0.2434108704328537 \tdiscriminator_loss: 1.1906059980392456 \tgenerator_loss: 0.8798763155937195\n",
            "For Step: 930 recon_loss: 0.25345832109451294 \tdiscriminator_loss: 1.1577248573303223 \tgenerator_loss: 0.8474959135055542\n",
            "For Step: 940 recon_loss: 0.23336520791053772 \tdiscriminator_loss: 1.2415834665298462 \tgenerator_loss: 0.8821481466293335\n",
            "For Step: 950 recon_loss: 0.2545260488986969 \tdiscriminator_loss: 1.1254534721374512 \tgenerator_loss: 0.8645665645599365\n",
            "For Step: 960 recon_loss: 0.22429293394088745 \tdiscriminator_loss: 1.1769671440124512 \tgenerator_loss: 0.8633684515953064\n",
            "For Step: 970 recon_loss: 0.2403339147567749 \tdiscriminator_loss: 1.1595642566680908 \tgenerator_loss: 0.9069685339927673\n",
            "For Step: 980 recon_loss: 0.24857309460639954 \tdiscriminator_loss: 1.1610562801361084 \tgenerator_loss: 0.8841240406036377\n",
            "For Step: 990 recon_loss: 0.23969022929668427 \tdiscriminator_loss: 1.1862151622772217 \tgenerator_loss: 0.8495539426803589\n",
            "For Step: 1000 recon_loss: 0.2302539050579071 \tdiscriminator_loss: 1.1815067529678345 \tgenerator_loss: 0.9149394035339355\n",
            "For Step: 1010 recon_loss: 0.23600025475025177 \tdiscriminator_loss: 1.1602740287780762 \tgenerator_loss: 0.890699028968811\n",
            "For Step: 1020 recon_loss: 0.23613490164279938 \tdiscriminator_loss: 1.1675225496292114 \tgenerator_loss: 0.8763068914413452\n",
            "For Step: 1030 recon_loss: 0.23907676339149475 \tdiscriminator_loss: 1.1683473587036133 \tgenerator_loss: 0.8488985300064087\n",
            "For Step: 1040 recon_loss: 0.2431880235671997 \tdiscriminator_loss: 1.2175226211547852 \tgenerator_loss: 0.8950862288475037\n",
            "For Step: 1050 recon_loss: 0.23116715252399445 \tdiscriminator_loss: 1.2386399507522583 \tgenerator_loss: 0.8711467981338501\n",
            "For Step: 1060 recon_loss: 0.23851153254508972 \tdiscriminator_loss: 1.138877272605896 \tgenerator_loss: 0.8655228614807129\n",
            "For Step: 1070 recon_loss: 0.24595984816551208 \tdiscriminator_loss: 1.1619329452514648 \tgenerator_loss: 0.8662576079368591\n",
            "For Step: 1080 recon_loss: 0.2521715760231018 \tdiscriminator_loss: 1.1828669309616089 \tgenerator_loss: 0.8637850284576416\n",
            "For Step: 1090 recon_loss: 0.2406100630760193 \tdiscriminator_loss: 1.144301176071167 \tgenerator_loss: 0.850810706615448\n",
            "For Step: 1100 recon_loss: 0.23474065959453583 \tdiscriminator_loss: 1.1931383609771729 \tgenerator_loss: 0.8657518625259399\n",
            "For Step: 1110 recon_loss: 0.23081181943416595 \tdiscriminator_loss: 1.1547365188598633 \tgenerator_loss: 0.8633582592010498\n",
            "For Step: 1120 recon_loss: 0.23657098412513733 \tdiscriminator_loss: 1.1890592575073242 \tgenerator_loss: 0.9002821445465088\n",
            "For Step: 1130 recon_loss: 0.22226068377494812 \tdiscriminator_loss: 1.1538680791854858 \tgenerator_loss: 0.8514172434806824\n",
            "For Step: 1140 recon_loss: 0.22701987624168396 \tdiscriminator_loss: 1.1536380052566528 \tgenerator_loss: 0.8589987754821777\n",
            "For Step: 1150 recon_loss: 0.23877964913845062 \tdiscriminator_loss: 1.1889581680297852 \tgenerator_loss: 0.8526463508605957\n",
            "For Step: 1160 recon_loss: 0.23380102217197418 \tdiscriminator_loss: 1.1165204048156738 \tgenerator_loss: 0.8599917888641357\n",
            "For Step: 1170 recon_loss: 0.2336534708738327 \tdiscriminator_loss: 1.1947975158691406 \tgenerator_loss: 0.8458885550498962\n",
            "For Step: 1180 recon_loss: 0.24047277867794037 \tdiscriminator_loss: 1.2316460609436035 \tgenerator_loss: 0.8576151132583618\n",
            "For Step: 1190 recon_loss: 0.2293722927570343 \tdiscriminator_loss: 1.248424768447876 \tgenerator_loss: 0.8848134279251099\n",
            "For Step: 1200 recon_loss: 0.2338292896747589 \tdiscriminator_loss: 1.1296526193618774 \tgenerator_loss: 0.855133056640625\n",
            "For Step: 1210 recon_loss: 0.22830933332443237 \tdiscriminator_loss: 1.2242062091827393 \tgenerator_loss: 0.8665372729301453\n",
            "For Step: 1220 recon_loss: 0.24374158680438995 \tdiscriminator_loss: 1.1351971626281738 \tgenerator_loss: 0.8836846351623535\n",
            "For Step: 1230 recon_loss: 0.23329804837703705 \tdiscriminator_loss: 1.1651945114135742 \tgenerator_loss: 0.8837480545043945\n",
            "For Step: 1240 recon_loss: 0.22320891916751862 \tdiscriminator_loss: 1.091827392578125 \tgenerator_loss: 0.8631101250648499\n",
            "For Step: 1250 recon_loss: 0.22735443711280823 \tdiscriminator_loss: 1.1646625995635986 \tgenerator_loss: 0.9010406732559204\n",
            "For Step: 1260 recon_loss: 0.21624767780303955 \tdiscriminator_loss: 1.1742554903030396 \tgenerator_loss: 0.8729997277259827\n",
            "For Step: 1270 recon_loss: 0.2359100580215454 \tdiscriminator_loss: 1.2171056270599365 \tgenerator_loss: 0.8392707705497742\n",
            "For Step: 1280 recon_loss: 0.22622141242027283 \tdiscriminator_loss: 1.1507750749588013 \tgenerator_loss: 0.8755173683166504\n",
            "For Step: 1290 recon_loss: 0.2322079837322235 \tdiscriminator_loss: 1.1455764770507812 \tgenerator_loss: 0.838793158531189\n",
            "For Step: 1300 recon_loss: 0.22807297110557556 \tdiscriminator_loss: 1.1230344772338867 \tgenerator_loss: 0.8258217573165894\n",
            "For Step: 1310 recon_loss: 0.22236435115337372 \tdiscriminator_loss: 1.0910786390304565 \tgenerator_loss: 0.8599119782447815\n",
            "For Step: 1320 recon_loss: 0.2274797558784485 \tdiscriminator_loss: 1.1103756427764893 \tgenerator_loss: 0.8545092344284058\n",
            "For Step: 1330 recon_loss: 0.22921538352966309 \tdiscriminator_loss: 1.1481772661209106 \tgenerator_loss: 0.8489606380462646\n",
            "For Step: 1340 recon_loss: 0.22099490463733673 \tdiscriminator_loss: 1.1482216119766235 \tgenerator_loss: 0.8700705766677856\n",
            "For Step: 1350 recon_loss: 0.21949730813503265 \tdiscriminator_loss: 1.2029519081115723 \tgenerator_loss: 0.8467917442321777\n",
            "For Step: 1360 recon_loss: 0.21253110468387604 \tdiscriminator_loss: 1.2221240997314453 \tgenerator_loss: 0.8378283977508545\n",
            "For Step: 1370 recon_loss: 0.2337704449892044 \tdiscriminator_loss: 1.1663131713867188 \tgenerator_loss: 0.8504200577735901\n",
            "For Step: 1380 recon_loss: 0.223267063498497 \tdiscriminator_loss: 1.1663978099822998 \tgenerator_loss: 0.8730964064598083\n",
            "For Step: 1390 recon_loss: 0.21835221350193024 \tdiscriminator_loss: 1.1809477806091309 \tgenerator_loss: 0.8594809174537659\n",
            "For Step: 1400 recon_loss: 0.2149304300546646 \tdiscriminator_loss: 1.1554313898086548 \tgenerator_loss: 0.8185003399848938\n",
            "For Step: 1410 recon_loss: 0.2175455540418625 \tdiscriminator_loss: 1.1230772733688354 \tgenerator_loss: 0.8307331204414368\n",
            "For Step: 1420 recon_loss: 0.2258613556623459 \tdiscriminator_loss: 1.1609243154525757 \tgenerator_loss: 0.8183404207229614\n",
            "For Step: 1430 recon_loss: 0.21247079968452454 \tdiscriminator_loss: 1.1567845344543457 \tgenerator_loss: 0.8214166164398193\n",
            "For Step: 1440 recon_loss: 0.2214643955230713 \tdiscriminator_loss: 1.1174228191375732 \tgenerator_loss: 0.8609848022460938\n",
            "For Step: 1450 recon_loss: 0.23144429922103882 \tdiscriminator_loss: 1.2097655534744263 \tgenerator_loss: 0.8454425930976868\n",
            "For Step: 1460 recon_loss: 0.22435927391052246 \tdiscriminator_loss: 1.1811745166778564 \tgenerator_loss: 0.8551134467124939\n",
            "For Step: 1470 recon_loss: 0.21586674451828003 \tdiscriminator_loss: 1.0587573051452637 \tgenerator_loss: 0.860019326210022\n",
            "For Step: 1480 recon_loss: 0.2204124927520752 \tdiscriminator_loss: 1.1558935642242432 \tgenerator_loss: 0.8417907953262329\n",
            "For Step: 1490 recon_loss: 0.22802944481372833 \tdiscriminator_loss: 1.1336020231246948 \tgenerator_loss: 0.8705204129219055\n",
            "For Step: 1500 recon_loss: 0.20704054832458496 \tdiscriminator_loss: 1.1957215070724487 \tgenerator_loss: 0.8241572976112366\n",
            "For Step: 1510 recon_loss: 0.2236402928829193 \tdiscriminator_loss: 1.1299858093261719 \tgenerator_loss: 0.8459887504577637\n",
            "For Step: 1520 recon_loss: 0.23769281804561615 \tdiscriminator_loss: 1.157071590423584 \tgenerator_loss: 0.8417699933052063\n",
            "For Step: 1530 recon_loss: 0.2179493010044098 \tdiscriminator_loss: 1.1036227941513062 \tgenerator_loss: 0.8186882138252258\n",
            "For Step: 1540 recon_loss: 0.23227547109127045 \tdiscriminator_loss: 1.2361551523208618 \tgenerator_loss: 0.849748432636261\n",
            "For Step: 1550 recon_loss: 0.2202656865119934 \tdiscriminator_loss: 1.2064210176467896 \tgenerator_loss: 0.8055052757263184\n",
            "For Step: 1560 recon_loss: 0.22399213910102844 \tdiscriminator_loss: 1.2435741424560547 \tgenerator_loss: 0.8063310384750366\n",
            "For Step: 1570 recon_loss: 0.22651317715644836 \tdiscriminator_loss: 1.1772842407226562 \tgenerator_loss: 0.8474442958831787\n",
            "For Step: 1580 recon_loss: 0.22398580610752106 \tdiscriminator_loss: 1.2023131847381592 \tgenerator_loss: 0.8431399464607239\n",
            "For Step: 1590 recon_loss: 0.22395052015781403 \tdiscriminator_loss: 1.1705284118652344 \tgenerator_loss: 0.843022882938385\n",
            "For Step: 1600 recon_loss: 0.2135564088821411 \tdiscriminator_loss: 1.1439554691314697 \tgenerator_loss: 0.7900837659835815\n",
            "For Step: 1610 recon_loss: 0.21870353817939758 \tdiscriminator_loss: 1.1218101978302002 \tgenerator_loss: 0.8592191934585571\n",
            "For Step: 1620 recon_loss: 0.21367497742176056 \tdiscriminator_loss: 1.197084903717041 \tgenerator_loss: 0.8379338979721069\n",
            "For Step: 1630 recon_loss: 0.23211140930652618 \tdiscriminator_loss: 1.2453382015228271 \tgenerator_loss: 0.8070589303970337\n",
            "For Step: 1640 recon_loss: 0.2117094099521637 \tdiscriminator_loss: 1.1246910095214844 \tgenerator_loss: 0.8098254203796387\n",
            "For Step: 1650 recon_loss: 0.21471449732780457 \tdiscriminator_loss: 1.1539688110351562 \tgenerator_loss: 0.8567362427711487\n",
            "For Step: 1660 recon_loss: 0.20959575474262238 \tdiscriminator_loss: 1.105104923248291 \tgenerator_loss: 0.8658211827278137\n",
            "For Step: 1670 recon_loss: 0.2141166627407074 \tdiscriminator_loss: 1.0957469940185547 \tgenerator_loss: 0.8270279169082642\n",
            "For Step: 1680 recon_loss: 0.22228889167308807 \tdiscriminator_loss: 1.2303125858306885 \tgenerator_loss: 0.854598343372345\n",
            "For Step: 1690 recon_loss: 0.22159889340400696 \tdiscriminator_loss: 1.1238231658935547 \tgenerator_loss: 0.8385658264160156\n",
            "For Step: 1700 recon_loss: 0.21591855585575104 \tdiscriminator_loss: 1.1778616905212402 \tgenerator_loss: 0.8449053764343262\n",
            "For Step: 1710 recon_loss: 0.2233107089996338 \tdiscriminator_loss: 1.1393871307373047 \tgenerator_loss: 0.8846376538276672\n",
            "For Step: 1720 recon_loss: 0.23324787616729736 \tdiscriminator_loss: 1.1175274848937988 \tgenerator_loss: 0.8526521325111389\n",
            "For Step: 1730 recon_loss: 0.22267785668373108 \tdiscriminator_loss: 1.1451337337493896 \tgenerator_loss: 0.8020329475402832\n",
            "For Step: 1740 recon_loss: 0.20957569777965546 \tdiscriminator_loss: 1.156323790550232 \tgenerator_loss: 0.8584845662117004\n",
            "For Step: 1750 recon_loss: 0.21698804199695587 \tdiscriminator_loss: 1.1337634325027466 \tgenerator_loss: 0.872844398021698\n",
            "For Step: 1760 recon_loss: 0.2087518572807312 \tdiscriminator_loss: 1.1281746625900269 \tgenerator_loss: 0.7985459566116333\n",
            "For Step: 1770 recon_loss: 0.21701419353485107 \tdiscriminator_loss: 1.1056156158447266 \tgenerator_loss: 0.8053088784217834\n",
            "For Step: 1780 recon_loss: 0.22700569033622742 \tdiscriminator_loss: 1.0770304203033447 \tgenerator_loss: 0.8497667908668518\n",
            "For Step: 1790 recon_loss: 0.21694518625736237 \tdiscriminator_loss: 1.1696703433990479 \tgenerator_loss: 0.8494716286659241\n",
            "For Step: 1800 recon_loss: 0.22785106301307678 \tdiscriminator_loss: 1.1690759658813477 \tgenerator_loss: 0.8396705389022827\n",
            "For Step: 1810 recon_loss: 0.20765450596809387 \tdiscriminator_loss: 1.167921543121338 \tgenerator_loss: 0.8013238906860352\n",
            "For Step: 1820 recon_loss: 0.20224349200725555 \tdiscriminator_loss: 1.0917835235595703 \tgenerator_loss: 0.8476719260215759\n",
            "For Step: 1830 recon_loss: 0.21550635993480682 \tdiscriminator_loss: 1.1006619930267334 \tgenerator_loss: 0.8407970070838928\n",
            "For Step: 1840 recon_loss: 0.21812507510185242 \tdiscriminator_loss: 1.1083393096923828 \tgenerator_loss: 0.807819664478302\n",
            "For Step: 1850 recon_loss: 0.21888954937458038 \tdiscriminator_loss: 1.2862720489501953 \tgenerator_loss: 0.8263951539993286\n",
            "For Step: 1860 recon_loss: 0.22245147824287415 \tdiscriminator_loss: 1.1228325366973877 \tgenerator_loss: 0.8353406190872192\n",
            "For Step: 1870 recon_loss: 0.21253833174705505 \tdiscriminator_loss: 1.2125608921051025 \tgenerator_loss: 0.7967366576194763\n",
            "For Step: 1880 recon_loss: 0.21626468002796173 \tdiscriminator_loss: 1.2246983051300049 \tgenerator_loss: 0.7961668968200684\n",
            "For Step: 1890 recon_loss: 0.21584346890449524 \tdiscriminator_loss: 1.161348581314087 \tgenerator_loss: 0.8369430899620056\n",
            "For Step: 1900 recon_loss: 0.21436883509159088 \tdiscriminator_loss: 1.209670066833496 \tgenerator_loss: 0.8358139991760254\n",
            "For Step: 1910 recon_loss: 0.21449902653694153 \tdiscriminator_loss: 1.1447434425354004 \tgenerator_loss: 0.8547808527946472\n",
            "For Step: 1920 recon_loss: 0.21764695644378662 \tdiscriminator_loss: 1.1592663526535034 \tgenerator_loss: 0.8017674088478088\n",
            "For Step: 1930 recon_loss: 0.21979135274887085 \tdiscriminator_loss: 1.1655269861221313 \tgenerator_loss: 0.7985245585441589\n",
            "For Step: 1940 recon_loss: 0.2056218385696411 \tdiscriminator_loss: 1.193407654762268 \tgenerator_loss: 0.7934865951538086\n",
            "For Step: 1950 recon_loss: 0.21378225088119507 \tdiscriminator_loss: 1.176328182220459 \tgenerator_loss: 0.8034048080444336\n",
            "For Step: 1960 recon_loss: 0.21492496132850647 \tdiscriminator_loss: 1.2495348453521729 \tgenerator_loss: 0.818483293056488\n",
            "For Step: 1970 recon_loss: 0.21860376000404358 \tdiscriminator_loss: 1.1702990531921387 \tgenerator_loss: 0.7926369905471802\n",
            "For Step: 1980 recon_loss: 0.22299937903881073 \tdiscriminator_loss: 1.2211766242980957 \tgenerator_loss: 0.7998557090759277\n",
            "For Step: 1990 recon_loss: 0.2244230955839157 \tdiscriminator_loss: 1.2220227718353271 \tgenerator_loss: 0.8115118741989136\n",
            "For Step: 2000 recon_loss: 0.20458590984344482 \tdiscriminator_loss: 1.1753804683685303 \tgenerator_loss: 0.8116124272346497\n",
            "For Step: 2010 recon_loss: 0.208918496966362 \tdiscriminator_loss: 1.1692259311676025 \tgenerator_loss: 0.8091986775398254\n",
            "For Step: 2020 recon_loss: 0.2101127654314041 \tdiscriminator_loss: 1.1747069358825684 \tgenerator_loss: 0.831674337387085\n",
            "For Step: 2030 recon_loss: 0.20902644097805023 \tdiscriminator_loss: 1.1619682312011719 \tgenerator_loss: 0.8238956928253174\n",
            "For Step: 2040 recon_loss: 0.2082212269306183 \tdiscriminator_loss: 1.2113922834396362 \tgenerator_loss: 0.8229259252548218\n",
            "For Step: 2050 recon_loss: 0.2072705179452896 \tdiscriminator_loss: 1.239285945892334 \tgenerator_loss: 0.8086420297622681\n",
            "For Step: 2060 recon_loss: 0.2095799297094345 \tdiscriminator_loss: 1.2798104286193848 \tgenerator_loss: 0.7993727326393127\n",
            "For Step: 2070 recon_loss: 0.2066860795021057 \tdiscriminator_loss: 1.1706271171569824 \tgenerator_loss: 0.8191539645195007\n",
            "For Step: 2080 recon_loss: 0.20796094834804535 \tdiscriminator_loss: 1.178525686264038 \tgenerator_loss: 0.8044366836547852\n",
            "For Step: 2090 recon_loss: 0.21358034014701843 \tdiscriminator_loss: 1.088545799255371 \tgenerator_loss: 0.790542721748352\n",
            "For Step: 2100 recon_loss: 0.2003713846206665 \tdiscriminator_loss: 1.0560307502746582 \tgenerator_loss: 0.8151336908340454\n",
            "For Step: 2110 recon_loss: 0.21895121037960052 \tdiscriminator_loss: 1.1555120944976807 \tgenerator_loss: 0.822431743144989\n",
            "For Step: 2120 recon_loss: 0.20180733501911163 \tdiscriminator_loss: 1.1818532943725586 \tgenerator_loss: 0.821428656578064\n",
            "For Step: 2130 recon_loss: 0.21193599700927734 \tdiscriminator_loss: 1.1430277824401855 \tgenerator_loss: 0.8379088044166565\n",
            "For Step: 2140 recon_loss: 0.20518600940704346 \tdiscriminator_loss: 1.2164337635040283 \tgenerator_loss: 0.8140342235565186\n",
            "For Step: 2150 recon_loss: 0.2084278017282486 \tdiscriminator_loss: 1.2631484270095825 \tgenerator_loss: 0.8186545372009277\n",
            "For Step: 2160 recon_loss: 0.22244344651699066 \tdiscriminator_loss: 1.2070910930633545 \tgenerator_loss: 0.806480348110199\n",
            "For Step: 2170 recon_loss: 0.20170943439006805 \tdiscriminator_loss: 1.215782880783081 \tgenerator_loss: 0.7584456205368042\n",
            "For Step: 2180 recon_loss: 0.21154937148094177 \tdiscriminator_loss: 1.2014343738555908 \tgenerator_loss: 0.8290175199508667\n",
            "For Step: 2190 recon_loss: 0.22646991908550262 \tdiscriminator_loss: 1.1810420751571655 \tgenerator_loss: 0.8097975254058838\n",
            "For Step: 2200 recon_loss: 0.2088644653558731 \tdiscriminator_loss: 1.1178972721099854 \tgenerator_loss: 0.8247475028038025\n",
            "For Step: 2210 recon_loss: 0.21394051611423492 \tdiscriminator_loss: 1.2574632167816162 \tgenerator_loss: 0.7842854261398315\n",
            "For Step: 2220 recon_loss: 0.21872775256633759 \tdiscriminator_loss: 1.132521629333496 \tgenerator_loss: 0.7887798547744751\n",
            "For Step: 2230 recon_loss: 0.21125127375125885 \tdiscriminator_loss: 1.266658067703247 \tgenerator_loss: 0.812061071395874\n",
            "For Step: 2240 recon_loss: 0.2148973047733307 \tdiscriminator_loss: 1.2437946796417236 \tgenerator_loss: 0.806976318359375\n",
            "For Step: 2250 recon_loss: 0.2061464488506317 \tdiscriminator_loss: 1.1931514739990234 \tgenerator_loss: 0.8316017389297485\n",
            "For Step: 2260 recon_loss: 0.21425870060920715 \tdiscriminator_loss: 1.1700339317321777 \tgenerator_loss: 0.8009817600250244\n",
            "For Step: 2270 recon_loss: 0.19824860990047455 \tdiscriminator_loss: 1.2101593017578125 \tgenerator_loss: 0.8232786655426025\n",
            "For Step: 2280 recon_loss: 0.21395963430404663 \tdiscriminator_loss: 1.13704514503479 \tgenerator_loss: 0.7892194986343384\n",
            "For Step: 2290 recon_loss: 0.20971153676509857 \tdiscriminator_loss: 1.1796221733093262 \tgenerator_loss: 0.8129216432571411\n",
            "For Step: 2300 recon_loss: 0.2118656039237976 \tdiscriminator_loss: 1.2896616458892822 \tgenerator_loss: 0.8002738356590271\n",
            "For Step: 2310 recon_loss: 0.20463445782661438 \tdiscriminator_loss: 1.2317821979522705 \tgenerator_loss: 0.7792510986328125\n",
            "For Step: 2320 recon_loss: 0.19062042236328125 \tdiscriminator_loss: 1.1625831127166748 \tgenerator_loss: 0.8287144899368286\n",
            "For Step: 2330 recon_loss: 0.20946399867534637 \tdiscriminator_loss: 1.2179436683654785 \tgenerator_loss: 0.7881983518600464\n",
            "For Step: 2340 recon_loss: 0.20477576553821564 \tdiscriminator_loss: 1.208570957183838 \tgenerator_loss: 0.7614074349403381\n",
            "For Step: 2350 recon_loss: 0.2003917545080185 \tdiscriminator_loss: 1.131274938583374 \tgenerator_loss: 0.7945706844329834\n",
            "For Step: 2360 recon_loss: 0.21448731422424316 \tdiscriminator_loss: 1.1884351968765259 \tgenerator_loss: 0.7677639722824097\n",
            "For Step: 2370 recon_loss: 0.21079374849796295 \tdiscriminator_loss: 1.2459938526153564 \tgenerator_loss: 0.7934863567352295\n",
            "For Step: 2380 recon_loss: 0.19355428218841553 \tdiscriminator_loss: 1.1254703998565674 \tgenerator_loss: 0.8203825950622559\n",
            "For Step: 2390 recon_loss: 0.20631489157676697 \tdiscriminator_loss: 1.2391043901443481 \tgenerator_loss: 0.795988917350769\n",
            "For Step: 2400 recon_loss: 0.20235009491443634 \tdiscriminator_loss: 1.146655559539795 \tgenerator_loss: 0.7585040330886841\n",
            "For Step: 2410 recon_loss: 0.20648439228534698 \tdiscriminator_loss: 1.1204668283462524 \tgenerator_loss: 0.8302006721496582\n",
            "For Step: 2420 recon_loss: 0.19410628080368042 \tdiscriminator_loss: 1.2079613208770752 \tgenerator_loss: 0.797013521194458\n",
            "For Step: 2430 recon_loss: 0.2074274867773056 \tdiscriminator_loss: 1.2172715663909912 \tgenerator_loss: 0.8168561458587646\n",
            "For Step: 2440 recon_loss: 0.20135009288787842 \tdiscriminator_loss: 1.087937831878662 \tgenerator_loss: 0.7780665159225464\n",
            "For Step: 2450 recon_loss: 0.20455360412597656 \tdiscriminator_loss: 1.1314330101013184 \tgenerator_loss: 0.7843074798583984\n",
            "For Step: 2460 recon_loss: 0.1998409479856491 \tdiscriminator_loss: 1.1280794143676758 \tgenerator_loss: 0.8104375600814819\n",
            "For Step: 2470 recon_loss: 0.19674544036388397 \tdiscriminator_loss: 1.182208776473999 \tgenerator_loss: 0.8118115067481995\n",
            "For Step: 2480 recon_loss: 0.20180414617061615 \tdiscriminator_loss: 1.2534425258636475 \tgenerator_loss: 0.824529767036438\n",
            "For Step: 2490 recon_loss: 0.20614995062351227 \tdiscriminator_loss: 1.205155849456787 \tgenerator_loss: 0.8381969928741455\n",
            "For Step: 2500 recon_loss: 0.20631015300750732 \tdiscriminator_loss: 1.2561757564544678 \tgenerator_loss: 0.7984451651573181\n",
            "For Step: 2510 recon_loss: 0.2128235548734665 \tdiscriminator_loss: 1.1434681415557861 \tgenerator_loss: 0.7889846563339233\n",
            "For Step: 2520 recon_loss: 0.20717059075832367 \tdiscriminator_loss: 1.0897643566131592 \tgenerator_loss: 0.789932131767273\n",
            "For Step: 2530 recon_loss: 0.20037992298603058 \tdiscriminator_loss: 1.1692665815353394 \tgenerator_loss: 0.8071610927581787\n",
            "For Step: 2540 recon_loss: 0.2031104862689972 \tdiscriminator_loss: 1.102116346359253 \tgenerator_loss: 0.8049376010894775\n",
            "For Step: 2550 recon_loss: 0.20085161924362183 \tdiscriminator_loss: 1.2458746433258057 \tgenerator_loss: 0.7959224581718445\n",
            "For Step: 2560 recon_loss: 0.19459885358810425 \tdiscriminator_loss: 1.0946979522705078 \tgenerator_loss: 0.8113241791725159\n",
            "For Step: 2570 recon_loss: 0.20375443994998932 \tdiscriminator_loss: 1.1698315143585205 \tgenerator_loss: 0.7948898673057556\n",
            "For Step: 2580 recon_loss: 0.20799852907657623 \tdiscriminator_loss: 1.2207739353179932 \tgenerator_loss: 0.7854577302932739\n",
            "For Step: 2590 recon_loss: 0.21637272834777832 \tdiscriminator_loss: 1.1446292400360107 \tgenerator_loss: 0.7531277537345886\n",
            "For Step: 2600 recon_loss: 0.21235695481300354 \tdiscriminator_loss: 1.2703335285186768 \tgenerator_loss: 0.7383719682693481\n",
            "For Step: 2610 recon_loss: 0.20695820450782776 \tdiscriminator_loss: 1.1618626117706299 \tgenerator_loss: 0.7969757318496704\n",
            "For Step: 2620 recon_loss: 0.2089046984910965 \tdiscriminator_loss: 1.262213110923767 \tgenerator_loss: 0.782933235168457\n",
            "For Step: 2630 recon_loss: 0.20380868017673492 \tdiscriminator_loss: 1.1352027654647827 \tgenerator_loss: 0.7470554113388062\n",
            "For Step: 2640 recon_loss: 0.21406936645507812 \tdiscriminator_loss: 1.2408583164215088 \tgenerator_loss: 0.7349420785903931\n",
            "For Step: 2650 recon_loss: 0.20812757313251495 \tdiscriminator_loss: 1.1748716831207275 \tgenerator_loss: 0.800565242767334\n",
            "For Step: 2660 recon_loss: 0.2027948945760727 \tdiscriminator_loss: 1.1102699041366577 \tgenerator_loss: 0.8017738461494446\n",
            "For Step: 2670 recon_loss: 0.20434877276420593 \tdiscriminator_loss: 1.304307222366333 \tgenerator_loss: 0.7546916604042053\n",
            "For Step: 2680 recon_loss: 0.188780277967453 \tdiscriminator_loss: 1.1307423114776611 \tgenerator_loss: 0.793221116065979\n",
            "For Step: 2690 recon_loss: 0.20864704251289368 \tdiscriminator_loss: 1.1112353801727295 \tgenerator_loss: 0.8292148113250732\n",
            "For Step: 2700 recon_loss: 0.19831521809101105 \tdiscriminator_loss: 1.169785976409912 \tgenerator_loss: 0.774869441986084\n",
            "For Step: 2710 recon_loss: 0.2023487538099289 \tdiscriminator_loss: 1.195568561553955 \tgenerator_loss: 0.7816784977912903\n",
            "For Step: 2720 recon_loss: 0.2028808444738388 \tdiscriminator_loss: 1.1035740375518799 \tgenerator_loss: 0.8046904802322388\n",
            "For Step: 2730 recon_loss: 0.1967993825674057 \tdiscriminator_loss: 1.105115294456482 \tgenerator_loss: 0.7980287075042725\n",
            "For Step: 2740 recon_loss: 0.21069514751434326 \tdiscriminator_loss: 1.1687572002410889 \tgenerator_loss: 0.7128649353981018\n",
            "For Step: 2750 recon_loss: 0.19701756536960602 \tdiscriminator_loss: 1.0982029438018799 \tgenerator_loss: 0.8020486831665039\n",
            "For Step: 2760 recon_loss: 0.18875445425510406 \tdiscriminator_loss: 1.27108895778656 \tgenerator_loss: 0.7885020971298218\n",
            "For Step: 2770 recon_loss: 0.19586308300495148 \tdiscriminator_loss: 1.1315228939056396 \tgenerator_loss: 0.7378949522972107\n",
            "For Step: 2780 recon_loss: 0.20018425583839417 \tdiscriminator_loss: 1.129513144493103 \tgenerator_loss: 0.8007326722145081\n",
            "For Step: 2790 recon_loss: 0.19759590923786163 \tdiscriminator_loss: 1.2154473066329956 \tgenerator_loss: 0.7434432506561279\n",
            "For Step: 2800 recon_loss: 0.2088986486196518 \tdiscriminator_loss: 1.1304556131362915 \tgenerator_loss: 0.8015221953392029\n",
            "For Step: 2810 recon_loss: 0.19326801598072052 \tdiscriminator_loss: 1.2073068618774414 \tgenerator_loss: 0.7685755491256714\n",
            "For Step: 2820 recon_loss: 0.19192379713058472 \tdiscriminator_loss: 1.1397440433502197 \tgenerator_loss: 0.7806656360626221\n",
            "For Step: 2830 recon_loss: 0.18937569856643677 \tdiscriminator_loss: 1.199586272239685 \tgenerator_loss: 0.7505883574485779\n",
            "For Step: 2840 recon_loss: 0.20500311255455017 \tdiscriminator_loss: 1.2093262672424316 \tgenerator_loss: 0.7337212562561035\n",
            "For Step: 2850 recon_loss: 0.2003016322851181 \tdiscriminator_loss: 1.1853231191635132 \tgenerator_loss: 0.8131916522979736\n",
            "For Step: 2860 recon_loss: 0.19019508361816406 \tdiscriminator_loss: 1.1478182077407837 \tgenerator_loss: 0.766098141670227\n",
            "For Step: 2870 recon_loss: 0.19324328005313873 \tdiscriminator_loss: 1.1181292533874512 \tgenerator_loss: 0.808634877204895\n",
            "For Step: 2880 recon_loss: 0.1911851018667221 \tdiscriminator_loss: 1.136447787284851 \tgenerator_loss: 0.7953418493270874\n",
            "For Step: 2890 recon_loss: 0.2112753689289093 \tdiscriminator_loss: 1.1757469177246094 \tgenerator_loss: 0.77617347240448\n",
            "For Step: 2900 recon_loss: 0.1844278872013092 \tdiscriminator_loss: 1.1462681293487549 \tgenerator_loss: 0.7708044052124023\n",
            "For Step: 2910 recon_loss: 0.1919420212507248 \tdiscriminator_loss: 1.1403326988220215 \tgenerator_loss: 0.7847322225570679\n",
            "For Step: 2920 recon_loss: 0.2076704502105713 \tdiscriminator_loss: 1.1632003784179688 \tgenerator_loss: 0.7221895456314087\n",
            "For Step: 2930 recon_loss: 0.19441555440425873 \tdiscriminator_loss: 1.1525530815124512 \tgenerator_loss: 0.7966495156288147\n",
            "For Step: 2940 recon_loss: 0.20041105151176453 \tdiscriminator_loss: 1.1502468585968018 \tgenerator_loss: 0.7558375597000122\n",
            "For Step: 2950 recon_loss: 0.19346511363983154 \tdiscriminator_loss: 1.139053225517273 \tgenerator_loss: 0.7654677629470825\n",
            "For Step: 2960 recon_loss: 0.20231211185455322 \tdiscriminator_loss: 1.1940698623657227 \tgenerator_loss: 0.7734619379043579\n",
            "For Step: 2970 recon_loss: 0.20430874824523926 \tdiscriminator_loss: 1.1271874904632568 \tgenerator_loss: 0.7616050243377686\n",
            "For Step: 2980 recon_loss: 0.19538798928260803 \tdiscriminator_loss: 1.1866424083709717 \tgenerator_loss: 0.7839475870132446\n",
            "For Step: 2990 recon_loss: 0.20143726468086243 \tdiscriminator_loss: 1.1448249816894531 \tgenerator_loss: 0.7627725601196289\n",
            "For Step: 3000 recon_loss: 0.19793078303337097 \tdiscriminator_loss: 1.1992970705032349 \tgenerator_loss: 0.778640627861023\n",
            "For Step: 3010 recon_loss: 0.20122972130775452 \tdiscriminator_loss: 1.1024413108825684 \tgenerator_loss: 0.7739888429641724\n",
            "For Step: 3020 recon_loss: 0.20021112263202667 \tdiscriminator_loss: 1.2556147575378418 \tgenerator_loss: 0.7787317633628845\n",
            "For Step: 3030 recon_loss: 0.1987328678369522 \tdiscriminator_loss: 1.0555636882781982 \tgenerator_loss: 0.7824630737304688\n",
            "For Step: 3040 recon_loss: 0.18722662329673767 \tdiscriminator_loss: 1.0940258502960205 \tgenerator_loss: 0.8061857223510742\n",
            "For Step: 3050 recon_loss: 0.20101255178451538 \tdiscriminator_loss: 1.2268497943878174 \tgenerator_loss: 0.7644002437591553\n",
            "For Step: 3060 recon_loss: 0.1963144838809967 \tdiscriminator_loss: 1.15268874168396 \tgenerator_loss: 0.7835646867752075\n",
            "For Step: 3070 recon_loss: 0.200749009847641 \tdiscriminator_loss: 1.156171202659607 \tgenerator_loss: 0.7785577178001404\n",
            "For Step: 3080 recon_loss: 0.19644150137901306 \tdiscriminator_loss: 1.1551181077957153 \tgenerator_loss: 0.7809772491455078\n",
            "For Step: 3090 recon_loss: 0.20559869706630707 \tdiscriminator_loss: 1.1571485996246338 \tgenerator_loss: 0.7737389802932739\n",
            "For Step: 3100 recon_loss: 0.20181229710578918 \tdiscriminator_loss: 1.1286730766296387 \tgenerator_loss: 0.782032310962677\n",
            "For Step: 3110 recon_loss: 0.20588938891887665 \tdiscriminator_loss: 1.263047695159912 \tgenerator_loss: 0.7641081809997559\n",
            "For Step: 3120 recon_loss: 0.19785834848880768 \tdiscriminator_loss: 1.24040949344635 \tgenerator_loss: 0.7696343660354614\n",
            "For Step: 3130 recon_loss: 0.1958429366350174 \tdiscriminator_loss: 1.17585027217865 \tgenerator_loss: 0.8054810166358948\n",
            "For Step: 3140 recon_loss: 0.19782844185829163 \tdiscriminator_loss: 1.2225050926208496 \tgenerator_loss: 0.7784234285354614\n",
            "For Step: 3150 recon_loss: 0.2017272263765335 \tdiscriminator_loss: 1.1970927715301514 \tgenerator_loss: 0.7586863040924072\n",
            "For Step: 3160 recon_loss: 0.19573159515857697 \tdiscriminator_loss: 1.163744330406189 \tgenerator_loss: 0.7808554172515869\n",
            "For Step: 3170 recon_loss: 0.19294673204421997 \tdiscriminator_loss: 1.2571702003479004 \tgenerator_loss: 0.7775092720985413\n",
            "For Step: 3180 recon_loss: 0.20210851728916168 \tdiscriminator_loss: 1.171536922454834 \tgenerator_loss: 0.7660192251205444\n",
            "For Step: 3190 recon_loss: 0.1796102374792099 \tdiscriminator_loss: 1.2245261669158936 \tgenerator_loss: 0.773617148399353\n",
            "For Step: 3200 recon_loss: 0.1885860562324524 \tdiscriminator_loss: 1.1472384929656982 \tgenerator_loss: 0.7557235956192017\n",
            "For Step: 3210 recon_loss: 0.20102767646312714 \tdiscriminator_loss: 1.2944211959838867 \tgenerator_loss: 0.7777361869812012\n",
            "For Step: 3220 recon_loss: 0.18832461535930634 \tdiscriminator_loss: 1.1447594165802002 \tgenerator_loss: 0.7708625793457031\n",
            "For Step: 3230 recon_loss: 0.20776984095573425 \tdiscriminator_loss: 1.146834373474121 \tgenerator_loss: 0.7475367188453674\n",
            "For Step: 3240 recon_loss: 0.20675335824489594 \tdiscriminator_loss: 1.1794183254241943 \tgenerator_loss: 0.7529418468475342\n",
            "For Step: 3250 recon_loss: 0.1911170780658722 \tdiscriminator_loss: 1.2461884021759033 \tgenerator_loss: 0.7706001996994019\n",
            "For Step: 3260 recon_loss: 0.20454393327236176 \tdiscriminator_loss: 1.1931992769241333 \tgenerator_loss: 0.7856873273849487\n",
            "For Step: 3270 recon_loss: 0.20161780714988708 \tdiscriminator_loss: 1.3611538410186768 \tgenerator_loss: 0.7089006900787354\n",
            "For Step: 3280 recon_loss: 0.1983230710029602 \tdiscriminator_loss: 1.1527098417282104 \tgenerator_loss: 0.7834001779556274\n",
            "For Step: 3290 recon_loss: 0.19682510197162628 \tdiscriminator_loss: 1.1560767889022827 \tgenerator_loss: 0.7706108689308167\n",
            "For Step: 3300 recon_loss: 0.19572684168815613 \tdiscriminator_loss: 1.1259657144546509 \tgenerator_loss: 0.7499762773513794\n",
            "For Step: 3310 recon_loss: 0.21102990210056305 \tdiscriminator_loss: 1.2108185291290283 \tgenerator_loss: 0.7293094396591187\n",
            "For Step: 3320 recon_loss: 0.2101094126701355 \tdiscriminator_loss: 1.2006168365478516 \tgenerator_loss: 0.7577973008155823\n",
            "For Step: 3330 recon_loss: 0.20263150334358215 \tdiscriminator_loss: 1.181920051574707 \tgenerator_loss: 0.7208569049835205\n",
            "For Step: 3340 recon_loss: 0.19227181375026703 \tdiscriminator_loss: 1.158307671546936 \tgenerator_loss: 0.742002010345459\n",
            "For Step: 3350 recon_loss: 0.18909195065498352 \tdiscriminator_loss: 1.1977072954177856 \tgenerator_loss: 0.753993570804596\n",
            "For Step: 3360 recon_loss: 0.19549737870693207 \tdiscriminator_loss: 1.078202486038208 \tgenerator_loss: 0.7553684115409851\n",
            "For Step: 3370 recon_loss: 0.19398503005504608 \tdiscriminator_loss: 1.2775086164474487 \tgenerator_loss: 0.7285052537918091\n",
            "For Step: 3380 recon_loss: 0.20901061594486237 \tdiscriminator_loss: 1.2402926683425903 \tgenerator_loss: 0.7272586822509766\n",
            "For Step: 3390 recon_loss: 0.20383398234844208 \tdiscriminator_loss: 1.2044389247894287 \tgenerator_loss: 0.7317793965339661\n",
            "For Step: 3400 recon_loss: 0.19986923038959503 \tdiscriminator_loss: 1.1826471090316772 \tgenerator_loss: 0.7656265497207642\n",
            "For Step: 3410 recon_loss: 0.20424631237983704 \tdiscriminator_loss: 1.1417473554611206 \tgenerator_loss: 0.7485066652297974\n",
            "For Step: 3420 recon_loss: 0.18495306372642517 \tdiscriminator_loss: 1.1496078968048096 \tgenerator_loss: 0.7888382077217102\n",
            "For Step: 3430 recon_loss: 0.2000792920589447 \tdiscriminator_loss: 1.1709963083267212 \tgenerator_loss: 0.7573779821395874\n",
            "For Step: 3440 recon_loss: 0.19614717364311218 \tdiscriminator_loss: 1.1627569198608398 \tgenerator_loss: 0.7406682968139648\n",
            "For Step: 3450 recon_loss: 0.21096821129322052 \tdiscriminator_loss: 1.1794791221618652 \tgenerator_loss: 0.7554949522018433\n",
            "For Step: 3460 recon_loss: 0.19109514355659485 \tdiscriminator_loss: 1.2129724025726318 \tgenerator_loss: 0.7774590253829956\n",
            "For Step: 3470 recon_loss: 0.19535109400749207 \tdiscriminator_loss: 1.2018961906433105 \tgenerator_loss: 0.7791707515716553\n",
            "For Step: 3480 recon_loss: 0.20027561485767365 \tdiscriminator_loss: 1.1013193130493164 \tgenerator_loss: 0.7824519872665405\n",
            "For Step: 3490 recon_loss: 0.19139856100082397 \tdiscriminator_loss: 1.194840431213379 \tgenerator_loss: 0.7798846960067749\n",
            "For Step: 3500 recon_loss: 0.19284123182296753 \tdiscriminator_loss: 1.1926215887069702 \tgenerator_loss: 0.7784409523010254\n",
            "For Step: 3510 recon_loss: 0.19223237037658691 \tdiscriminator_loss: 1.1010222434997559 \tgenerator_loss: 0.7900264263153076\n",
            "For Step: 3520 recon_loss: 0.20588859915733337 \tdiscriminator_loss: 1.0947333574295044 \tgenerator_loss: 0.7701296210289001\n",
            "For Step: 3530 recon_loss: 0.19077274203300476 \tdiscriminator_loss: 1.0972392559051514 \tgenerator_loss: 0.7753287553787231\n",
            "For Step: 3540 recon_loss: 0.18701694905757904 \tdiscriminator_loss: 1.149641990661621 \tgenerator_loss: 0.7985659241676331\n",
            "For Step: 3550 recon_loss: 0.1903650313615799 \tdiscriminator_loss: 1.1988555192947388 \tgenerator_loss: 0.81318199634552\n",
            "For Step: 3560 recon_loss: 0.19137679040431976 \tdiscriminator_loss: 1.0952608585357666 \tgenerator_loss: 0.7721622586250305\n",
            "For Step: 3570 recon_loss: 0.19114533066749573 \tdiscriminator_loss: 1.187575101852417 \tgenerator_loss: 0.7499158382415771\n",
            "For Step: 3580 recon_loss: 0.18727785348892212 \tdiscriminator_loss: 1.1879291534423828 \tgenerator_loss: 0.791438102722168\n",
            "For Step: 3590 recon_loss: 0.21188685297966003 \tdiscriminator_loss: 1.1411213874816895 \tgenerator_loss: 0.740243673324585\n",
            "For Step: 3600 recon_loss: 0.188506081700325 \tdiscriminator_loss: 1.1447184085845947 \tgenerator_loss: 0.773432731628418\n",
            "For Step: 3610 recon_loss: 0.18887504935264587 \tdiscriminator_loss: 1.1227812767028809 \tgenerator_loss: 0.7887897491455078\n",
            "For Step: 3620 recon_loss: 0.19709455966949463 \tdiscriminator_loss: 1.1902012825012207 \tgenerator_loss: 0.7634004354476929\n",
            "For Step: 3630 recon_loss: 0.1866907775402069 \tdiscriminator_loss: 1.1423113346099854 \tgenerator_loss: 0.7769917845726013\n",
            "For Step: 3640 recon_loss: 0.17542105913162231 \tdiscriminator_loss: 1.1042523384094238 \tgenerator_loss: 0.8051328659057617\n",
            "For Step: 3650 recon_loss: 0.1934763491153717 \tdiscriminator_loss: 1.14009690284729 \tgenerator_loss: 0.7900800704956055\n",
            "For Step: 3660 recon_loss: 0.19915741682052612 \tdiscriminator_loss: 1.162083387374878 \tgenerator_loss: 0.7458515167236328\n",
            "For Step: 3670 recon_loss: 0.19592779874801636 \tdiscriminator_loss: 1.1718997955322266 \tgenerator_loss: 0.7611498236656189\n",
            "For Step: 3680 recon_loss: 0.19055825471878052 \tdiscriminator_loss: 1.1741087436676025 \tgenerator_loss: 0.8226585984230042\n",
            "For Step: 3690 recon_loss: 0.19983531534671783 \tdiscriminator_loss: 1.07859206199646 \tgenerator_loss: 0.7950258255004883\n",
            "For Step: 3700 recon_loss: 0.18140877783298492 \tdiscriminator_loss: 1.1012276411056519 \tgenerator_loss: 0.7787505388259888\n",
            "For Step: 3710 recon_loss: 0.1887027621269226 \tdiscriminator_loss: 1.2039521932601929 \tgenerator_loss: 0.7813986539840698\n",
            "For Step: 3720 recon_loss: 0.18641427159309387 \tdiscriminator_loss: 1.0857547521591187 \tgenerator_loss: 0.7761750817298889\n",
            "For Step: 3730 recon_loss: 0.19692783057689667 \tdiscriminator_loss: 1.1939595937728882 \tgenerator_loss: 0.7358326315879822\n",
            "For Step: 3740 recon_loss: 0.19461023807525635 \tdiscriminator_loss: 1.2780430316925049 \tgenerator_loss: 0.7011450529098511\n",
            "For Step: 3750 recon_loss: 0.17970949411392212 \tdiscriminator_loss: 1.1043205261230469 \tgenerator_loss: 0.7943363785743713\n",
            "For Step: 3760 recon_loss: 0.20272845029830933 \tdiscriminator_loss: 1.1902008056640625 \tgenerator_loss: 0.8171908259391785\n",
            "For Step: 3770 recon_loss: 0.19376562535762787 \tdiscriminator_loss: 1.1439714431762695 \tgenerator_loss: 0.7875248193740845\n",
            "For Step: 3780 recon_loss: 0.1850210428237915 \tdiscriminator_loss: 1.1774389743804932 \tgenerator_loss: 0.7796885967254639\n",
            "For Step: 3790 recon_loss: 0.1732875257730484 \tdiscriminator_loss: 1.1580899953842163 \tgenerator_loss: 0.79127436876297\n",
            "For Step: 3800 recon_loss: 0.1962050199508667 \tdiscriminator_loss: 1.1731419563293457 \tgenerator_loss: 0.7820552587509155\n",
            "For Step: 3810 recon_loss: 0.19904188811779022 \tdiscriminator_loss: 1.1635620594024658 \tgenerator_loss: 0.744945228099823\n",
            "For Step: 3820 recon_loss: 0.1970365047454834 \tdiscriminator_loss: 1.1031506061553955 \tgenerator_loss: 0.789341926574707\n",
            "For Step: 3830 recon_loss: 0.18616962432861328 \tdiscriminator_loss: 1.1505628824234009 \tgenerator_loss: 0.7902606725692749\n",
            "For Step: 3840 recon_loss: 0.19144190847873688 \tdiscriminator_loss: 1.1582839488983154 \tgenerator_loss: 0.8214238882064819\n",
            "For Step: 3850 recon_loss: 0.18754184246063232 \tdiscriminator_loss: 1.2278525829315186 \tgenerator_loss: 0.8088452816009521\n",
            "For Step: 3860 recon_loss: 0.20360450446605682 \tdiscriminator_loss: 1.1707433462142944 \tgenerator_loss: 0.801640510559082\n",
            "For Step: 3870 recon_loss: 0.19191975891590118 \tdiscriminator_loss: 1.138291597366333 \tgenerator_loss: 0.7637312412261963\n",
            "For Step: 3880 recon_loss: 0.19808153808116913 \tdiscriminator_loss: 1.1329913139343262 \tgenerator_loss: 0.7689021825790405\n",
            "For Step: 3890 recon_loss: 0.21210706233978271 \tdiscriminator_loss: 1.09344482421875 \tgenerator_loss: 0.7771133184432983\n",
            "For Step: 3900 recon_loss: 0.20553071796894073 \tdiscriminator_loss: 1.0835151672363281 \tgenerator_loss: 0.754252016544342\n",
            "For Step: 3910 recon_loss: 0.20379652082920074 \tdiscriminator_loss: 1.2120029926300049 \tgenerator_loss: 0.7708837389945984\n",
            "For Step: 3920 recon_loss: 0.19548282027244568 \tdiscriminator_loss: 1.2707650661468506 \tgenerator_loss: 0.7213051915168762\n",
            "For Step: 3930 recon_loss: 0.19223302602767944 \tdiscriminator_loss: 1.1778850555419922 \tgenerator_loss: 0.7872467637062073\n",
            "For Step: 3940 recon_loss: 0.1801503300666809 \tdiscriminator_loss: 1.1252684593200684 \tgenerator_loss: 0.7523898482322693\n",
            "For Step: 3950 recon_loss: 0.19879160821437836 \tdiscriminator_loss: 1.1905851364135742 \tgenerator_loss: 0.7449964284896851\n",
            "For Step: 3960 recon_loss: 0.19599655270576477 \tdiscriminator_loss: 1.1552122831344604 \tgenerator_loss: 0.7400047779083252\n",
            "For Step: 3970 recon_loss: 0.18759022653102875 \tdiscriminator_loss: 1.1698627471923828 \tgenerator_loss: 0.7821062207221985\n",
            "For Step: 3980 recon_loss: 0.18888653814792633 \tdiscriminator_loss: 1.1329305171966553 \tgenerator_loss: 0.752339780330658\n",
            "For Step: 3990 recon_loss: 0.19832301139831543 \tdiscriminator_loss: 1.1767663955688477 \tgenerator_loss: 0.7691694498062134\n",
            "For Step: 4000 recon_loss: 0.20744818449020386 \tdiscriminator_loss: 1.1088664531707764 \tgenerator_loss: 0.8188420534133911\n",
            "For Step: 4010 recon_loss: 0.1846349835395813 \tdiscriminator_loss: 1.1480786800384521 \tgenerator_loss: 0.8390445709228516\n",
            "For Step: 4020 recon_loss: 0.19394218921661377 \tdiscriminator_loss: 1.2125229835510254 \tgenerator_loss: 0.721565842628479\n",
            "For Step: 4030 recon_loss: 0.17998816072940826 \tdiscriminator_loss: 1.0828077793121338 \tgenerator_loss: 0.8121072053909302\n",
            "For Step: 4040 recon_loss: 0.18805839121341705 \tdiscriminator_loss: 1.170681118965149 \tgenerator_loss: 0.7491274476051331\n",
            "For Step: 4050 recon_loss: 0.19220392405986786 \tdiscriminator_loss: 1.2595044374465942 \tgenerator_loss: 0.7823927402496338\n",
            "For Step: 4060 recon_loss: 0.19816935062408447 \tdiscriminator_loss: 1.198244571685791 \tgenerator_loss: 0.7752487659454346\n",
            "For Step: 4070 recon_loss: 0.19487828016281128 \tdiscriminator_loss: 1.1628546714782715 \tgenerator_loss: 0.7666242122650146\n",
            "For Step: 4080 recon_loss: 0.19024249911308289 \tdiscriminator_loss: 1.1624197959899902 \tgenerator_loss: 0.7988036870956421\n",
            "For Step: 4090 recon_loss: 0.19051621854305267 \tdiscriminator_loss: 1.200037956237793 \tgenerator_loss: 0.7678973078727722\n",
            "For Step: 4100 recon_loss: 0.19446241855621338 \tdiscriminator_loss: 1.1361172199249268 \tgenerator_loss: 0.7925471067428589\n",
            "For Step: 4110 recon_loss: 0.19707711040973663 \tdiscriminator_loss: 1.107055425643921 \tgenerator_loss: 0.7794456481933594\n",
            "For Step: 4120 recon_loss: 0.19152319431304932 \tdiscriminator_loss: 1.1911540031433105 \tgenerator_loss: 0.777717649936676\n",
            "For Step: 4130 recon_loss: 0.17929252982139587 \tdiscriminator_loss: 1.1958357095718384 \tgenerator_loss: 0.7899108529090881\n",
            "For Step: 4140 recon_loss: 0.19619840383529663 \tdiscriminator_loss: 1.204844355583191 \tgenerator_loss: 0.7677009105682373\n",
            "For Step: 4150 recon_loss: 0.1961415410041809 \tdiscriminator_loss: 1.1546680927276611 \tgenerator_loss: 0.7892667651176453\n",
            "For Step: 4160 recon_loss: 0.20490863919258118 \tdiscriminator_loss: 1.2050745487213135 \tgenerator_loss: 0.7818757891654968\n",
            "For Step: 4170 recon_loss: 0.18773719668388367 \tdiscriminator_loss: 1.161871075630188 \tgenerator_loss: 0.7723903656005859\n",
            "For Step: 4180 recon_loss: 0.18624788522720337 \tdiscriminator_loss: 1.266219139099121 \tgenerator_loss: 0.7405225038528442\n",
            "For Step: 4190 recon_loss: 0.17993584275245667 \tdiscriminator_loss: 1.185330867767334 \tgenerator_loss: 0.7975298166275024\n",
            "For Step: 4200 recon_loss: 0.18958058953285217 \tdiscriminator_loss: 1.1744552850723267 \tgenerator_loss: 0.833176851272583\n",
            "For Step: 4210 recon_loss: 0.19635595381259918 \tdiscriminator_loss: 1.1574374437332153 \tgenerator_loss: 0.7843865156173706\n",
            "For Step: 4220 recon_loss: 0.19239597022533417 \tdiscriminator_loss: 1.1179733276367188 \tgenerator_loss: 0.7698535919189453\n",
            "For Step: 4230 recon_loss: 0.19242477416992188 \tdiscriminator_loss: 1.2240924835205078 \tgenerator_loss: 0.7741611003875732\n",
            "For Step: 4240 recon_loss: 0.18786309659481049 \tdiscriminator_loss: 1.2337095737457275 \tgenerator_loss: 0.7783634066581726\n",
            "For Step: 4250 recon_loss: 0.18764834105968475 \tdiscriminator_loss: 1.1317826509475708 \tgenerator_loss: 0.7970118522644043\n",
            "For Step: 4260 recon_loss: 0.2035118043422699 \tdiscriminator_loss: 1.2892436981201172 \tgenerator_loss: 0.7412062287330627\n",
            "For Step: 4270 recon_loss: 0.18660543859004974 \tdiscriminator_loss: 1.1479284763336182 \tgenerator_loss: 0.7572970390319824\n",
            "For Step: 4280 recon_loss: 0.18020358681678772 \tdiscriminator_loss: 1.1109338998794556 \tgenerator_loss: 0.7986727356910706\n",
            "For Step: 4290 recon_loss: 0.1854192167520523 \tdiscriminator_loss: 1.153156042098999 \tgenerator_loss: 0.8043583035469055\n",
            "For Step: 4300 recon_loss: 0.2042015939950943 \tdiscriminator_loss: 1.1821458339691162 \tgenerator_loss: 0.814106822013855\n",
            "For Step: 4310 recon_loss: 0.18880155682563782 \tdiscriminator_loss: 1.1470646858215332 \tgenerator_loss: 0.7919145226478577\n",
            "For Step: 4320 recon_loss: 0.18420358002185822 \tdiscriminator_loss: 1.161711573600769 \tgenerator_loss: 0.747563362121582\n",
            "For Step: 4330 recon_loss: 0.17916527390480042 \tdiscriminator_loss: 1.1931500434875488 \tgenerator_loss: 0.7650322914123535\n",
            "For Step: 4340 recon_loss: 0.1786588877439499 \tdiscriminator_loss: 1.0533217191696167 \tgenerator_loss: 0.8357544541358948\n",
            "For Step: 4350 recon_loss: 0.197427436709404 \tdiscriminator_loss: 1.0839190483093262 \tgenerator_loss: 0.801318883895874\n",
            "For Step: 4360 recon_loss: 0.1923155039548874 \tdiscriminator_loss: 1.21665620803833 \tgenerator_loss: 0.7389452457427979\n",
            "For Step: 4370 recon_loss: 0.20569999516010284 \tdiscriminator_loss: 1.1483609676361084 \tgenerator_loss: 0.8387753963470459\n",
            "For Step: 4380 recon_loss: 0.19605565071105957 \tdiscriminator_loss: 1.140674352645874 \tgenerator_loss: 0.8046169877052307\n",
            "For Step: 4390 recon_loss: 0.19079862534999847 \tdiscriminator_loss: 1.201560378074646 \tgenerator_loss: 0.7627968788146973\n",
            "For Step: 4400 recon_loss: 0.19262051582336426 \tdiscriminator_loss: 1.1492302417755127 \tgenerator_loss: 0.7688198089599609\n",
            "For Step: 4410 recon_loss: 0.19134634733200073 \tdiscriminator_loss: 1.0956077575683594 \tgenerator_loss: 0.7995792627334595\n",
            "For Step: 4420 recon_loss: 0.1867874264717102 \tdiscriminator_loss: 1.187309980392456 \tgenerator_loss: 0.8429347276687622\n",
            "For Step: 4430 recon_loss: 0.19179730117321014 \tdiscriminator_loss: 1.0864992141723633 \tgenerator_loss: 0.7632745504379272\n",
            "For Step: 4440 recon_loss: 0.19577227532863617 \tdiscriminator_loss: 1.146791696548462 \tgenerator_loss: 0.7589498162269592\n",
            "For Step: 4450 recon_loss: 0.18463459610939026 \tdiscriminator_loss: 1.1577117443084717 \tgenerator_loss: 0.8146950006484985\n",
            "For Step: 4460 recon_loss: 0.19809578359127045 \tdiscriminator_loss: 1.0877259969711304 \tgenerator_loss: 0.7673476934432983\n",
            "For Step: 4470 recon_loss: 0.1821894347667694 \tdiscriminator_loss: 1.134324550628662 \tgenerator_loss: 0.8088425993919373\n",
            "For Step: 4480 recon_loss: 0.18148460984230042 \tdiscriminator_loss: 1.169278621673584 \tgenerator_loss: 0.7871825695037842\n",
            "For Step: 4490 recon_loss: 0.1815880984067917 \tdiscriminator_loss: 1.295149326324463 \tgenerator_loss: 0.7149994969367981\n",
            "For Step: 4500 recon_loss: 0.1883038431406021 \tdiscriminator_loss: 1.1359062194824219 \tgenerator_loss: 0.7749850749969482\n",
            "For Step: 4510 recon_loss: 0.18475009500980377 \tdiscriminator_loss: 1.1012113094329834 \tgenerator_loss: 0.82474684715271\n",
            "For Step: 4520 recon_loss: 0.1887659877538681 \tdiscriminator_loss: 1.0721771717071533 \tgenerator_loss: 0.8203327059745789\n",
            "For Step: 4530 recon_loss: 0.20663946866989136 \tdiscriminator_loss: 1.1646336317062378 \tgenerator_loss: 0.831367552280426\n",
            "For Step: 4540 recon_loss: 0.18575553596019745 \tdiscriminator_loss: 1.1158971786499023 \tgenerator_loss: 0.7956457734107971\n",
            "For Step: 4550 recon_loss: 0.19664515554904938 \tdiscriminator_loss: 1.16986083984375 \tgenerator_loss: 0.7816622257232666\n",
            "For Step: 4560 recon_loss: 0.19795463979244232 \tdiscriminator_loss: 1.2045178413391113 \tgenerator_loss: 0.8072218894958496\n",
            "For Step: 4570 recon_loss: 0.1894274353981018 \tdiscriminator_loss: 1.078825831413269 \tgenerator_loss: 0.823876142501831\n",
            "For Step: 4580 recon_loss: 0.18954570591449738 \tdiscriminator_loss: 1.1052217483520508 \tgenerator_loss: 0.8047176003456116\n",
            "For Step: 4590 recon_loss: 0.19513709843158722 \tdiscriminator_loss: 1.1873832941055298 \tgenerator_loss: 0.7780481576919556\n",
            "For Step: 4600 recon_loss: 0.19517064094543457 \tdiscriminator_loss: 1.1379649639129639 \tgenerator_loss: 0.7763117551803589\n",
            "For Step: 4610 recon_loss: 0.17730750143527985 \tdiscriminator_loss: 1.1324412822723389 \tgenerator_loss: 0.7985281944274902\n",
            "For Step: 4620 recon_loss: 0.18222230672836304 \tdiscriminator_loss: 1.1487268209457397 \tgenerator_loss: 0.830125093460083\n",
            "For Step: 4630 recon_loss: 0.18964648246765137 \tdiscriminator_loss: 1.1751348972320557 \tgenerator_loss: 0.7975329160690308\n",
            "For Step: 4640 recon_loss: 0.1896325647830963 \tdiscriminator_loss: 1.1250933408737183 \tgenerator_loss: 0.7775402069091797\n",
            "For Step: 4650 recon_loss: 0.18672356009483337 \tdiscriminator_loss: 1.1607332229614258 \tgenerator_loss: 0.8114382028579712\n",
            "For Step: 4660 recon_loss: 0.1948292851448059 \tdiscriminator_loss: 1.157947063446045 \tgenerator_loss: 0.7801268696784973\n",
            "For Step: 4670 recon_loss: 0.18935824930667877 \tdiscriminator_loss: 1.0959243774414062 \tgenerator_loss: 0.8195091485977173\n",
            "For Step: 4680 recon_loss: 0.19520294666290283 \tdiscriminator_loss: 1.145214319229126 \tgenerator_loss: 0.762529194355011\n",
            "For Step: 4690 recon_loss: 0.1835595667362213 \tdiscriminator_loss: 1.173103928565979 \tgenerator_loss: 0.768214225769043\n",
            "For Step: 4700 recon_loss: 0.1866346150636673 \tdiscriminator_loss: 1.244157314300537 \tgenerator_loss: 0.8058711290359497\n",
            "For Step: 4710 recon_loss: 0.1808321177959442 \tdiscriminator_loss: 1.061077356338501 \tgenerator_loss: 0.7740070819854736\n",
            "For Step: 4720 recon_loss: 0.18304108083248138 \tdiscriminator_loss: 1.1117804050445557 \tgenerator_loss: 0.7901870012283325\n",
            "For Step: 4730 recon_loss: 0.20453931391239166 \tdiscriminator_loss: 1.068772315979004 \tgenerator_loss: 0.8693581819534302\n",
            "For Step: 4740 recon_loss: 0.18821097910404205 \tdiscriminator_loss: 1.1241779327392578 \tgenerator_loss: 0.8083065152168274\n",
            "For Step: 4750 recon_loss: 0.19605128467082977 \tdiscriminator_loss: 1.2053213119506836 \tgenerator_loss: 0.7772442102432251\n",
            "For Step: 4760 recon_loss: 0.1871565282344818 \tdiscriminator_loss: 1.0717936754226685 \tgenerator_loss: 0.8557827472686768\n",
            "For Step: 4770 recon_loss: 0.18536712229251862 \tdiscriminator_loss: 1.0485817193984985 \tgenerator_loss: 0.808259129524231\n",
            "For Step: 4780 recon_loss: 0.18386180698871613 \tdiscriminator_loss: 1.1114262342453003 \tgenerator_loss: 0.818013072013855\n",
            "For Step: 4790 recon_loss: 0.1829221546649933 \tdiscriminator_loss: 1.2710249423980713 \tgenerator_loss: 0.8050486445426941\n",
            "For Step: 4800 recon_loss: 0.18987801671028137 \tdiscriminator_loss: 1.1734672784805298 \tgenerator_loss: 0.844605565071106\n",
            "For Step: 4810 recon_loss: 0.1942393034696579 \tdiscriminator_loss: 1.2388114929199219 \tgenerator_loss: 0.7164394855499268\n",
            "For Step: 4820 recon_loss: 0.19210374355316162 \tdiscriminator_loss: 1.1295872926712036 \tgenerator_loss: 0.8277110457420349\n",
            "For Step: 4830 recon_loss: 0.18853379786014557 \tdiscriminator_loss: 1.121055245399475 \tgenerator_loss: 0.8086510300636292\n",
            "For Step: 4840 recon_loss: 0.18648454546928406 \tdiscriminator_loss: 1.0990827083587646 \tgenerator_loss: 0.8351643085479736\n",
            "For Step: 4850 recon_loss: 0.1907438039779663 \tdiscriminator_loss: 1.0695858001708984 \tgenerator_loss: 0.7951216101646423\n",
            "For Step: 4860 recon_loss: 0.17778074741363525 \tdiscriminator_loss: 1.2157061100006104 \tgenerator_loss: 0.8148345947265625\n",
            "For Step: 4870 recon_loss: 0.18455727398395538 \tdiscriminator_loss: 1.2515232563018799 \tgenerator_loss: 0.8076567053794861\n",
            "For Step: 4880 recon_loss: 0.1963028758764267 \tdiscriminator_loss: 1.0617220401763916 \tgenerator_loss: 0.8132227659225464\n",
            "For Step: 4890 recon_loss: 0.20242087543010712 \tdiscriminator_loss: 1.1318906545639038 \tgenerator_loss: 0.7932711839675903\n",
            "For Step: 4900 recon_loss: 0.18758216500282288 \tdiscriminator_loss: 1.159724235534668 \tgenerator_loss: 0.8063019514083862\n",
            "For Step: 4910 recon_loss: 0.1914389729499817 \tdiscriminator_loss: 1.205091953277588 \tgenerator_loss: 0.788520872592926\n",
            "For Step: 4920 recon_loss: 0.1919533908367157 \tdiscriminator_loss: 1.0801492929458618 \tgenerator_loss: 0.802632749080658\n",
            "For Step: 4930 recon_loss: 0.18682821094989777 \tdiscriminator_loss: 1.0962495803833008 \tgenerator_loss: 0.8185204267501831\n",
            "For Step: 4940 recon_loss: 0.19048669934272766 \tdiscriminator_loss: 1.1511380672454834 \tgenerator_loss: 0.7915223836898804\n",
            "For Step: 4950 recon_loss: 0.18847745656967163 \tdiscriminator_loss: 1.2082456350326538 \tgenerator_loss: 0.811812162399292\n",
            "For Step: 4960 recon_loss: 0.19772788882255554 \tdiscriminator_loss: 1.1751511096954346 \tgenerator_loss: 0.7823992967605591\n",
            "For Step: 4970 recon_loss: 0.18528124690055847 \tdiscriminator_loss: 1.1276934146881104 \tgenerator_loss: 0.8126097917556763\n",
            "For Step: 4980 recon_loss: 0.19328199326992035 \tdiscriminator_loss: 1.1390395164489746 \tgenerator_loss: 0.860887885093689\n",
            "For Step: 4990 recon_loss: 0.1780736893415451 \tdiscriminator_loss: 1.1649317741394043 \tgenerator_loss: 0.8342207670211792\n",
            "For Step: 5000 recon_loss: 0.18232859671115875 \tdiscriminator_loss: 1.2614116668701172 \tgenerator_loss: 0.7915777564048767\n",
            "For Step: 5010 recon_loss: 0.1960015445947647 \tdiscriminator_loss: 1.1892321109771729 \tgenerator_loss: 0.777254581451416\n",
            "For Step: 5020 recon_loss: 0.20440229773521423 \tdiscriminator_loss: 1.187241792678833 \tgenerator_loss: 0.821051836013794\n",
            "For Step: 5030 recon_loss: 0.18361961841583252 \tdiscriminator_loss: 1.2049956321716309 \tgenerator_loss: 0.7712597846984863\n",
            "For Step: 5040 recon_loss: 0.1936293989419937 \tdiscriminator_loss: 1.2046000957489014 \tgenerator_loss: 0.7944327592849731\n",
            "For Step: 5050 recon_loss: 0.18437224626541138 \tdiscriminator_loss: 1.0061668157577515 \tgenerator_loss: 0.8303824663162231\n",
            "For Step: 5060 recon_loss: 0.18256670236587524 \tdiscriminator_loss: 1.2089544534683228 \tgenerator_loss: 0.8332087993621826\n",
            "For Step: 5070 recon_loss: 0.18903572857379913 \tdiscriminator_loss: 1.1771260499954224 \tgenerator_loss: 0.8192875385284424\n",
            "For Step: 5080 recon_loss: 0.1909671276807785 \tdiscriminator_loss: 1.2315138578414917 \tgenerator_loss: 0.8157986402511597\n",
            "For Step: 5090 recon_loss: 0.1999952346086502 \tdiscriminator_loss: 1.1251530647277832 \tgenerator_loss: 0.8103368878364563\n",
            "For Step: 5100 recon_loss: 0.17385856807231903 \tdiscriminator_loss: 1.2971985340118408 \tgenerator_loss: 0.7657183408737183\n",
            "For Step: 5110 recon_loss: 0.19299250841140747 \tdiscriminator_loss: 1.189460039138794 \tgenerator_loss: 0.7986704707145691\n",
            "For Step: 5120 recon_loss: 0.1899745911359787 \tdiscriminator_loss: 1.046828269958496 \tgenerator_loss: 0.8368123769760132\n",
            "For Step: 5130 recon_loss: 0.19383792579174042 \tdiscriminator_loss: 1.0776983499526978 \tgenerator_loss: 0.8232778310775757\n",
            "For Step: 5140 recon_loss: 0.17243024706840515 \tdiscriminator_loss: 1.1809061765670776 \tgenerator_loss: 0.8226475715637207\n",
            "For Step: 5150 recon_loss: 0.1830766648054123 \tdiscriminator_loss: 1.1560648679733276 \tgenerator_loss: 0.7906734943389893\n",
            "For Step: 5160 recon_loss: 0.19414524734020233 \tdiscriminator_loss: 1.0191638469696045 \tgenerator_loss: 0.8233528733253479\n",
            "For Step: 5170 recon_loss: 0.18606719374656677 \tdiscriminator_loss: 1.2675046920776367 \tgenerator_loss: 0.7785469889640808\n",
            "For Step: 5180 recon_loss: 0.17866696417331696 \tdiscriminator_loss: 1.150315284729004 \tgenerator_loss: 0.849198579788208\n",
            "For Step: 5190 recon_loss: 0.18317808210849762 \tdiscriminator_loss: 1.151093602180481 \tgenerator_loss: 0.8149698376655579\n",
            "For Step: 5200 recon_loss: 0.18307656049728394 \tdiscriminator_loss: 1.2072091102600098 \tgenerator_loss: 0.7717946767807007\n",
            "For Step: 5210 recon_loss: 0.1975620836019516 \tdiscriminator_loss: 1.0812699794769287 \tgenerator_loss: 0.8517776131629944\n",
            "For Step: 5220 recon_loss: 0.18411199748516083 \tdiscriminator_loss: 1.1122639179229736 \tgenerator_loss: 0.8573126792907715\n",
            "For Step: 5230 recon_loss: 0.1952194720506668 \tdiscriminator_loss: 1.04669988155365 \tgenerator_loss: 0.8554062247276306\n",
            "For Step: 5240 recon_loss: 0.1812635362148285 \tdiscriminator_loss: 1.194387674331665 \tgenerator_loss: 0.8787537813186646\n",
            "For Step: 5250 recon_loss: 0.18163235485553741 \tdiscriminator_loss: 1.0682200193405151 \tgenerator_loss: 0.8176074028015137\n",
            "For Step: 5260 recon_loss: 0.1930505335330963 \tdiscriminator_loss: 1.1713229417800903 \tgenerator_loss: 0.7990928888320923\n",
            "For Step: 5270 recon_loss: 0.1970330774784088 \tdiscriminator_loss: 1.2305678129196167 \tgenerator_loss: 0.841974675655365\n",
            "For Step: 5280 recon_loss: 0.18894308805465698 \tdiscriminator_loss: 1.0595831871032715 \tgenerator_loss: 0.8345199227333069\n",
            "For Step: 5290 recon_loss: 0.19062137603759766 \tdiscriminator_loss: 1.1098159551620483 \tgenerator_loss: 0.8171272277832031\n",
            "For Step: 5300 recon_loss: 0.1745101362466812 \tdiscriminator_loss: 1.1756724119186401 \tgenerator_loss: 0.8730765581130981\n",
            "For Step: 5310 recon_loss: 0.1983579397201538 \tdiscriminator_loss: 1.0961885452270508 \tgenerator_loss: 0.8302604556083679\n",
            "For Step: 5320 recon_loss: 0.18308459222316742 \tdiscriminator_loss: 1.1377171277999878 \tgenerator_loss: 0.8447279930114746\n",
            "For Step: 5330 recon_loss: 0.19139043986797333 \tdiscriminator_loss: 1.1733052730560303 \tgenerator_loss: 0.8573079705238342\n",
            "For Step: 5340 recon_loss: 0.19912774860858917 \tdiscriminator_loss: 0.9944196343421936 \tgenerator_loss: 0.8157863020896912\n",
            "For Step: 5350 recon_loss: 0.1952887326478958 \tdiscriminator_loss: 1.1206953525543213 \tgenerator_loss: 0.8702893257141113\n",
            "For Step: 5360 recon_loss: 0.19150346517562866 \tdiscriminator_loss: 1.170166254043579 \tgenerator_loss: 0.8251758813858032\n",
            "For Step: 5370 recon_loss: 0.18888582289218903 \tdiscriminator_loss: 1.1694179773330688 \tgenerator_loss: 0.822113573551178\n",
            "For Step: 5380 recon_loss: 0.1845085173845291 \tdiscriminator_loss: 1.1592744588851929 \tgenerator_loss: 0.8625863790512085\n",
            "For Step: 5390 recon_loss: 0.18162328004837036 \tdiscriminator_loss: 1.196246862411499 \tgenerator_loss: 0.8839766383171082\n",
            "For Step: 5400 recon_loss: 0.19468046724796295 \tdiscriminator_loss: 1.175804615020752 \tgenerator_loss: 0.8101944923400879\n",
            "For Step: 5410 recon_loss: 0.18482495844364166 \tdiscriminator_loss: 1.116497278213501 \tgenerator_loss: 0.8380478620529175\n",
            "For Step: 5420 recon_loss: 0.17435459792613983 \tdiscriminator_loss: 1.0268205404281616 \tgenerator_loss: 0.8743602633476257\n",
            "For Step: 5430 recon_loss: 0.18038953840732574 \tdiscriminator_loss: 1.203819751739502 \tgenerator_loss: 0.859955906867981\n",
            "For Step: 5440 recon_loss: 0.1791621297597885 \tdiscriminator_loss: 1.23331880569458 \tgenerator_loss: 0.8658266067504883\n",
            "For Step: 5450 recon_loss: 0.1841961145401001 \tdiscriminator_loss: 1.0838384628295898 \tgenerator_loss: 0.864976167678833\n",
            "For Step: 5460 recon_loss: 0.18551716208457947 \tdiscriminator_loss: 1.2116692066192627 \tgenerator_loss: 0.7886185646057129\n",
            "For Step: 5470 recon_loss: 0.18772877752780914 \tdiscriminator_loss: 1.1482528448104858 \tgenerator_loss: 0.8573164939880371\n",
            "For Step: 5480 recon_loss: 0.19112174212932587 \tdiscriminator_loss: 1.1334013938903809 \tgenerator_loss: 0.8549151420593262\n",
            "For Step: 5490 recon_loss: 0.19423823058605194 \tdiscriminator_loss: 1.2105501890182495 \tgenerator_loss: 0.8529109954833984\n",
            "For Step: 5500 recon_loss: 0.18553116917610168 \tdiscriminator_loss: 1.1771043539047241 \tgenerator_loss: 0.7996396422386169\n",
            "For Step: 5510 recon_loss: 0.19111283123493195 \tdiscriminator_loss: 1.127515196800232 \tgenerator_loss: 0.8428399562835693\n",
            "For Step: 5520 recon_loss: 0.1836511194705963 \tdiscriminator_loss: 1.0718209743499756 \tgenerator_loss: 0.8879572153091431\n",
            "For Step: 5530 recon_loss: 0.17749224603176117 \tdiscriminator_loss: 1.098536491394043 \tgenerator_loss: 0.833881676197052\n",
            "For Step: 5540 recon_loss: 0.19522365927696228 \tdiscriminator_loss: 1.0602712631225586 \tgenerator_loss: 0.8532490730285645\n",
            "For Step: 5550 recon_loss: 0.18206030130386353 \tdiscriminator_loss: 1.1760785579681396 \tgenerator_loss: 0.8973891735076904\n",
            "For Step: 5560 recon_loss: 0.18937823176383972 \tdiscriminator_loss: 1.1428375244140625 \tgenerator_loss: 0.7881268262863159\n",
            "For Step: 5570 recon_loss: 0.19745762646198273 \tdiscriminator_loss: 1.2504961490631104 \tgenerator_loss: 0.8246057033538818\n",
            "For Step: 5580 recon_loss: 0.18534986674785614 \tdiscriminator_loss: 1.1410305500030518 \tgenerator_loss: 0.8767794966697693\n",
            "For Step: 5590 recon_loss: 0.17169564962387085 \tdiscriminator_loss: 1.1161962747573853 \tgenerator_loss: 0.7983248233795166\n",
            "For Step: 5600 recon_loss: 0.17314133048057556 \tdiscriminator_loss: 1.100358009338379 \tgenerator_loss: 0.8215503096580505\n",
            "For Step: 5610 recon_loss: 0.17609617114067078 \tdiscriminator_loss: 1.1279664039611816 \tgenerator_loss: 0.8291987180709839\n",
            "For Step: 5620 recon_loss: 0.17966154217720032 \tdiscriminator_loss: 1.2329820394515991 \tgenerator_loss: 0.8146380186080933\n",
            "For Step: 5630 recon_loss: 0.18605011701583862 \tdiscriminator_loss: 1.1604663133621216 \tgenerator_loss: 0.8741926550865173\n",
            "For Step: 5640 recon_loss: 0.19011949002742767 \tdiscriminator_loss: 1.1690936088562012 \tgenerator_loss: 0.8607372045516968\n",
            "For Step: 5650 recon_loss: 0.19042161107063293 \tdiscriminator_loss: 1.2024548053741455 \tgenerator_loss: 0.8530022501945496\n",
            "For Step: 5660 recon_loss: 0.1861652284860611 \tdiscriminator_loss: 1.0559519529342651 \tgenerator_loss: 0.8693723678588867\n",
            "For Step: 5670 recon_loss: 0.1932096630334854 \tdiscriminator_loss: 1.198629379272461 \tgenerator_loss: 0.7616984844207764\n",
            "For Step: 5680 recon_loss: 0.20093651115894318 \tdiscriminator_loss: 1.1079182624816895 \tgenerator_loss: 0.8624720573425293\n",
            "For Step: 5690 recon_loss: 0.18089520931243896 \tdiscriminator_loss: 1.1455259323120117 \tgenerator_loss: 0.8681627511978149\n",
            "For Step: 5700 recon_loss: 0.1879291981458664 \tdiscriminator_loss: 1.1639549732208252 \tgenerator_loss: 0.813990592956543\n",
            "For Step: 5710 recon_loss: 0.18488986790180206 \tdiscriminator_loss: 1.099284052848816 \tgenerator_loss: 0.8813161849975586\n",
            "For Step: 5720 recon_loss: 0.1780242919921875 \tdiscriminator_loss: 1.1101343631744385 \tgenerator_loss: 0.8690230250358582\n",
            "For Step: 5730 recon_loss: 0.18364281952381134 \tdiscriminator_loss: 1.2237920761108398 \tgenerator_loss: 0.85450279712677\n",
            "For Step: 5740 recon_loss: 0.19311289489269257 \tdiscriminator_loss: 1.0542583465576172 \tgenerator_loss: 0.8958699703216553\n",
            "For Step: 5750 recon_loss: 0.18471290171146393 \tdiscriminator_loss: 1.2115046977996826 \tgenerator_loss: 0.8837741613388062\n",
            "For Step: 5760 recon_loss: 0.16682153940200806 \tdiscriminator_loss: 1.153040885925293 \tgenerator_loss: 0.872119128704071\n",
            "For Step: 5770 recon_loss: 0.18318277597427368 \tdiscriminator_loss: 1.1744111776351929 \tgenerator_loss: 0.8813149333000183\n",
            "For Step: 5780 recon_loss: 0.19632314145565033 \tdiscriminator_loss: 1.082756519317627 \tgenerator_loss: 0.8518854379653931\n",
            "For Step: 5790 recon_loss: 0.18637855350971222 \tdiscriminator_loss: 0.9894785284996033 \tgenerator_loss: 0.8940567970275879\n",
            "For Step: 5800 recon_loss: 0.17356203496456146 \tdiscriminator_loss: 1.0555018186569214 \tgenerator_loss: 0.922110915184021\n",
            "For Step: 5810 recon_loss: 0.1812967211008072 \tdiscriminator_loss: 1.1912447214126587 \tgenerator_loss: 0.7923754453659058\n",
            "For Step: 5820 recon_loss: 0.19093921780586243 \tdiscriminator_loss: 1.0117647647857666 \tgenerator_loss: 0.8672184944152832\n",
            "For Step: 5830 recon_loss: 0.17634010314941406 \tdiscriminator_loss: 1.1977649927139282 \tgenerator_loss: 0.8080742359161377\n",
            "For Step: 5840 recon_loss: 0.1824457049369812 \tdiscriminator_loss: 1.2287545204162598 \tgenerator_loss: 0.8559260964393616\n",
            "For Step: 5850 recon_loss: 0.18316146731376648 \tdiscriminator_loss: 1.0008878707885742 \tgenerator_loss: 0.9466297626495361\n",
            "For Step: 5860 recon_loss: 0.18634383380413055 \tdiscriminator_loss: 1.0560667514801025 \tgenerator_loss: 0.8716897368431091\n",
            "For Step: 5870 recon_loss: 0.18173542618751526 \tdiscriminator_loss: 1.0816025733947754 \tgenerator_loss: 0.8862787485122681\n",
            "For Step: 5880 recon_loss: 0.18768750131130219 \tdiscriminator_loss: 1.2511801719665527 \tgenerator_loss: 0.9063673615455627\n",
            "For Step: 5890 recon_loss: 0.18095728754997253 \tdiscriminator_loss: 1.1248195171356201 \tgenerator_loss: 0.9002896547317505\n",
            "For Step: 5900 recon_loss: 0.1811976134777069 \tdiscriminator_loss: 1.0128049850463867 \tgenerator_loss: 0.8908419609069824\n",
            "For Step: 5910 recon_loss: 0.19041495025157928 \tdiscriminator_loss: 1.1964309215545654 \tgenerator_loss: 0.8398070931434631\n",
            "For Step: 5920 recon_loss: 0.1887165606021881 \tdiscriminator_loss: 1.055787444114685 \tgenerator_loss: 0.8418813347816467\n",
            "For Step: 5930 recon_loss: 0.1915597766637802 \tdiscriminator_loss: 1.1613503694534302 \tgenerator_loss: 0.8314197659492493\n",
            "For Step: 5940 recon_loss: 0.19570787250995636 \tdiscriminator_loss: 1.1882479190826416 \tgenerator_loss: 0.8477087616920471\n",
            "For Step: 5950 recon_loss: 0.1919574737548828 \tdiscriminator_loss: 1.0541777610778809 \tgenerator_loss: 0.9551093578338623\n",
            "For Step: 5960 recon_loss: 0.17612579464912415 \tdiscriminator_loss: 1.09444260597229 \tgenerator_loss: 0.873237133026123\n",
            "For Step: 5970 recon_loss: 0.19367994368076324 \tdiscriminator_loss: 1.1414216756820679 \tgenerator_loss: 0.8737273812294006\n",
            "For Step: 5980 recon_loss: 0.1827203780412674 \tdiscriminator_loss: 1.1446224451065063 \tgenerator_loss: 0.849611222743988\n",
            "For Step: 5990 recon_loss: 0.18055763840675354 \tdiscriminator_loss: 1.0659503936767578 \tgenerator_loss: 0.874596118927002\n",
            "For Step: 6000 recon_loss: 0.1788116693496704 \tdiscriminator_loss: 1.250700831413269 \tgenerator_loss: 0.8549034595489502\n",
            "For Step: 6010 recon_loss: 0.19050367176532745 \tdiscriminator_loss: 1.2326281070709229 \tgenerator_loss: 0.8640453219413757\n",
            "For Step: 6020 recon_loss: 0.17639733850955963 \tdiscriminator_loss: 1.1933377981185913 \tgenerator_loss: 0.8438854217529297\n",
            "For Step: 6030 recon_loss: 0.18212904036045074 \tdiscriminator_loss: 1.1416263580322266 \tgenerator_loss: 0.8760292530059814\n",
            "For Step: 6040 recon_loss: 0.19077804684638977 \tdiscriminator_loss: 1.0859789848327637 \tgenerator_loss: 0.9363104104995728\n",
            "For Step: 6050 recon_loss: 0.18131187558174133 \tdiscriminator_loss: 1.1498329639434814 \tgenerator_loss: 0.8653519153594971\n",
            "For Step: 6060 recon_loss: 0.17486001551151276 \tdiscriminator_loss: 1.10369873046875 \tgenerator_loss: 0.8923680782318115\n",
            "For Step: 6070 recon_loss: 0.17584790289402008 \tdiscriminator_loss: 1.1223976612091064 \tgenerator_loss: 0.8399239778518677\n",
            "For Step: 6080 recon_loss: 0.18751788139343262 \tdiscriminator_loss: 1.2015440464019775 \tgenerator_loss: 0.8450403213500977\n",
            "For Step: 6090 recon_loss: 0.1852729618549347 \tdiscriminator_loss: 1.0641567707061768 \tgenerator_loss: 0.9298268556594849\n",
            "For Step: 6100 recon_loss: 0.17711713910102844 \tdiscriminator_loss: 1.15264892578125 \tgenerator_loss: 0.8527010679244995\n",
            "For Step: 6110 recon_loss: 0.17970474064350128 \tdiscriminator_loss: 1.0775113105773926 \tgenerator_loss: 0.9338392019271851\n",
            "For Step: 6120 recon_loss: 0.18321271240711212 \tdiscriminator_loss: 1.183070421218872 \tgenerator_loss: 0.848973274230957\n",
            "For Step: 6130 recon_loss: 0.18912243843078613 \tdiscriminator_loss: 1.0631141662597656 \tgenerator_loss: 0.8939268589019775\n",
            "For Step: 6140 recon_loss: 0.18427138030529022 \tdiscriminator_loss: 1.126111626625061 \tgenerator_loss: 0.9466320276260376\n",
            "For Step: 6150 recon_loss: 0.18739740550518036 \tdiscriminator_loss: 1.2588403224945068 \tgenerator_loss: 0.8628265261650085\n",
            "For Step: 6160 recon_loss: 0.1769736111164093 \tdiscriminator_loss: 1.104683756828308 \tgenerator_loss: 0.7943317294120789\n",
            "For Step: 6170 recon_loss: 0.19156323373317719 \tdiscriminator_loss: 1.089266300201416 \tgenerator_loss: 0.8786312341690063\n",
            "For Step: 6180 recon_loss: 0.1887379139661789 \tdiscriminator_loss: 1.0711610317230225 \tgenerator_loss: 0.8827440738677979\n",
            "For Step: 6190 recon_loss: 0.17178864777088165 \tdiscriminator_loss: 1.0188558101654053 \tgenerator_loss: 0.851793646812439\n",
            "For Step: 6200 recon_loss: 0.18564200401306152 \tdiscriminator_loss: 1.1400467157363892 \tgenerator_loss: 0.8619832992553711\n",
            "For Step: 6210 recon_loss: 0.18050314486026764 \tdiscriminator_loss: 1.1819450855255127 \tgenerator_loss: 0.9198325276374817\n",
            "For Step: 6220 recon_loss: 0.1858467161655426 \tdiscriminator_loss: 1.1166417598724365 \tgenerator_loss: 0.8832758069038391\n",
            "For Step: 6230 recon_loss: 0.1837901622056961 \tdiscriminator_loss: 1.0865247249603271 \tgenerator_loss: 0.968071699142456\n",
            "For Step: 6240 recon_loss: 0.19810065627098083 \tdiscriminator_loss: 1.2014520168304443 \tgenerator_loss: 0.8557162284851074\n",
            "For Step: 6250 recon_loss: 0.1750590205192566 \tdiscriminator_loss: 1.0528254508972168 \tgenerator_loss: 0.8825911283493042\n",
            "For Step: 6260 recon_loss: 0.20121724903583527 \tdiscriminator_loss: 1.1503419876098633 \tgenerator_loss: 0.8440175652503967\n",
            "For Step: 6270 recon_loss: 0.1867438107728958 \tdiscriminator_loss: 1.122891902923584 \tgenerator_loss: 0.8658167123794556\n",
            "For Step: 6280 recon_loss: 0.17633819580078125 \tdiscriminator_loss: 1.139254093170166 \tgenerator_loss: 0.8990017771720886\n",
            "For Step: 6290 recon_loss: 0.18912385404109955 \tdiscriminator_loss: 1.1363202333450317 \tgenerator_loss: 0.8596821427345276\n",
            "For Step: 6300 recon_loss: 0.1847052276134491 \tdiscriminator_loss: 1.0083390474319458 \tgenerator_loss: 0.9284238219261169\n",
            "For Step: 6310 recon_loss: 0.184814453125 \tdiscriminator_loss: 1.1778979301452637 \tgenerator_loss: 0.9030373096466064\n",
            "For Step: 6320 recon_loss: 0.17949321866035461 \tdiscriminator_loss: 1.140876054763794 \tgenerator_loss: 0.9083244204521179\n",
            "For Step: 6330 recon_loss: 0.19399870932102203 \tdiscriminator_loss: 1.0819200277328491 \tgenerator_loss: 0.892022430896759\n",
            "For Step: 6340 recon_loss: 0.19039930403232574 \tdiscriminator_loss: 1.1992557048797607 \tgenerator_loss: 0.8362808227539062\n",
            "For Step: 6350 recon_loss: 0.1779700070619583 \tdiscriminator_loss: 1.2122215032577515 \tgenerator_loss: 0.906460165977478\n",
            "For Step: 6360 recon_loss: 0.18241387605667114 \tdiscriminator_loss: 1.082244873046875 \tgenerator_loss: 0.8605990409851074\n",
            "For Step: 6370 recon_loss: 0.18247513473033905 \tdiscriminator_loss: 1.1379495859146118 \tgenerator_loss: 0.8823392987251282\n",
            "For Step: 6380 recon_loss: 0.18277738988399506 \tdiscriminator_loss: 1.0617023706436157 \tgenerator_loss: 0.9349381327629089\n",
            "For Step: 6390 recon_loss: 0.1867520660161972 \tdiscriminator_loss: 1.2279397249221802 \tgenerator_loss: 0.8764240741729736\n",
            "For Step: 6400 recon_loss: 0.1754172444343567 \tdiscriminator_loss: 1.0673701763153076 \tgenerator_loss: 0.9234523177146912\n",
            "For Step: 6410 recon_loss: 0.17876674234867096 \tdiscriminator_loss: 1.2000586986541748 \tgenerator_loss: 0.8931910991668701\n",
            "For Step: 6420 recon_loss: 0.18844285607337952 \tdiscriminator_loss: 1.0690478086471558 \tgenerator_loss: 0.9364228248596191\n",
            "For Step: 6430 recon_loss: 0.19011206924915314 \tdiscriminator_loss: 1.1462267637252808 \tgenerator_loss: 0.9725253582000732\n",
            "For Step: 6440 recon_loss: 0.18313606083393097 \tdiscriminator_loss: 1.1323206424713135 \tgenerator_loss: 0.9504906535148621\n",
            "For Step: 6450 recon_loss: 0.18910062313079834 \tdiscriminator_loss: 1.1316877603530884 \tgenerator_loss: 0.8859966993331909\n",
            "For Step: 6460 recon_loss: 0.18596488237380981 \tdiscriminator_loss: 1.159593939781189 \tgenerator_loss: 0.86406409740448\n",
            "For Step: 6470 recon_loss: 0.17799563705921173 \tdiscriminator_loss: 1.0739742517471313 \tgenerator_loss: 0.9090887904167175\n",
            "For Step: 6480 recon_loss: 0.19253025949001312 \tdiscriminator_loss: 1.1353600025177002 \tgenerator_loss: 0.8942663669586182\n",
            "For Step: 6490 recon_loss: 0.17458458244800568 \tdiscriminator_loss: 1.002443790435791 \tgenerator_loss: 0.9136254191398621\n",
            "For Step: 6500 recon_loss: 0.1844778060913086 \tdiscriminator_loss: 1.2212245464324951 \tgenerator_loss: 0.8807922601699829\n",
            "For Step: 6510 recon_loss: 0.1883063018321991 \tdiscriminator_loss: 1.07118558883667 \tgenerator_loss: 0.9348838329315186\n",
            "For Step: 6520 recon_loss: 0.17840614914894104 \tdiscriminator_loss: 1.2527141571044922 \tgenerator_loss: 0.8963499665260315\n",
            "For Step: 6530 recon_loss: 0.19454440474510193 \tdiscriminator_loss: 1.1858463287353516 \tgenerator_loss: 0.8856940269470215\n",
            "For Step: 6540 recon_loss: 0.17630022764205933 \tdiscriminator_loss: 0.9578279852867126 \tgenerator_loss: 0.8896576762199402\n",
            "For Step: 6550 recon_loss: 0.18529976904392242 \tdiscriminator_loss: 1.1739685535430908 \tgenerator_loss: 0.8800324201583862\n",
            "For Step: 6560 recon_loss: 0.1765522062778473 \tdiscriminator_loss: 1.0055391788482666 \tgenerator_loss: 0.9838331341743469\n",
            "For Step: 6570 recon_loss: 0.18034102022647858 \tdiscriminator_loss: 1.1655097007751465 \tgenerator_loss: 0.9383461475372314\n",
            "For Step: 6580 recon_loss: 0.18881914019584656 \tdiscriminator_loss: 1.122230052947998 \tgenerator_loss: 0.9154380559921265\n",
            "For Step: 6590 recon_loss: 0.1749476194381714 \tdiscriminator_loss: 1.048982858657837 \tgenerator_loss: 0.9420868754386902\n",
            "For Step: 6600 recon_loss: 0.1847892701625824 \tdiscriminator_loss: 1.0666254758834839 \tgenerator_loss: 0.8612352013587952\n",
            "For Step: 6610 recon_loss: 0.17874643206596375 \tdiscriminator_loss: 1.1355507373809814 \tgenerator_loss: 0.9196939468383789\n",
            "For Step: 6620 recon_loss: 0.1803792119026184 \tdiscriminator_loss: 1.2592601776123047 \tgenerator_loss: 0.8724378347396851\n",
            "For Step: 6630 recon_loss: 0.17666761577129364 \tdiscriminator_loss: 1.0990512371063232 \tgenerator_loss: 0.9830366969108582\n",
            "For Step: 6640 recon_loss: 0.19314567744731903 \tdiscriminator_loss: 1.009084701538086 \tgenerator_loss: 0.9127200841903687\n",
            "For Step: 6650 recon_loss: 0.19307556748390198 \tdiscriminator_loss: 0.9567209482192993 \tgenerator_loss: 0.9188629388809204\n",
            "For Step: 6660 recon_loss: 0.1700056940317154 \tdiscriminator_loss: 1.2251982688903809 \tgenerator_loss: 0.8811130523681641\n",
            "For Step: 6670 recon_loss: 0.1821693331003189 \tdiscriminator_loss: 1.1711938381195068 \tgenerator_loss: 0.9301061630249023\n",
            "For Step: 6680 recon_loss: 0.18534120917320251 \tdiscriminator_loss: 1.0500246286392212 \tgenerator_loss: 0.9358559846878052\n",
            "For Step: 6690 recon_loss: 0.1753854602575302 \tdiscriminator_loss: 1.1393028497695923 \tgenerator_loss: 0.8913983106613159\n",
            "For Step: 6700 recon_loss: 0.17223548889160156 \tdiscriminator_loss: 1.038474202156067 \tgenerator_loss: 0.9131544232368469\n",
            "For Step: 6710 recon_loss: 0.17431730031967163 \tdiscriminator_loss: 1.0725178718566895 \tgenerator_loss: 0.907547116279602\n",
            "For Step: 6720 recon_loss: 0.18323376774787903 \tdiscriminator_loss: 1.0701663494110107 \tgenerator_loss: 0.9691887497901917\n",
            "For Step: 6730 recon_loss: 0.19075341522693634 \tdiscriminator_loss: 1.0634965896606445 \tgenerator_loss: 0.9026427268981934\n",
            "For Step: 6740 recon_loss: 0.17620748281478882 \tdiscriminator_loss: 1.036605715751648 \tgenerator_loss: 0.9625235199928284\n",
            "For Step: 6750 recon_loss: 0.17543187737464905 \tdiscriminator_loss: 1.2569317817687988 \tgenerator_loss: 0.8903379440307617\n",
            "For Step: 6760 recon_loss: 0.18352818489074707 \tdiscriminator_loss: 1.1528511047363281 \tgenerator_loss: 0.9353806972503662\n",
            "For Step: 6770 recon_loss: 0.18893548846244812 \tdiscriminator_loss: 1.1155495643615723 \tgenerator_loss: 0.9273089170455933\n",
            "For Step: 6780 recon_loss: 0.19748403131961823 \tdiscriminator_loss: 1.106473445892334 \tgenerator_loss: 0.9063010215759277\n",
            "For Step: 6790 recon_loss: 0.16562345623970032 \tdiscriminator_loss: 1.2474520206451416 \tgenerator_loss: 0.8577746152877808\n",
            "For Step: 6800 recon_loss: 0.1780272126197815 \tdiscriminator_loss: 1.1391652822494507 \tgenerator_loss: 0.9522020220756531\n",
            "For Step: 6810 recon_loss: 0.1665579378604889 \tdiscriminator_loss: 1.189223051071167 \tgenerator_loss: 0.8539729118347168\n",
            "For Step: 6820 recon_loss: 0.191299170255661 \tdiscriminator_loss: 1.195115566253662 \tgenerator_loss: 0.8598446846008301\n",
            "For Step: 6830 recon_loss: 0.18520164489746094 \tdiscriminator_loss: 1.1438884735107422 \tgenerator_loss: 0.9484026432037354\n",
            "For Step: 6840 recon_loss: 0.1978532075881958 \tdiscriminator_loss: 1.1246099472045898 \tgenerator_loss: 0.8826361894607544\n",
            "For Step: 6850 recon_loss: 0.17932167649269104 \tdiscriminator_loss: 1.007206916809082 \tgenerator_loss: 0.9520368576049805\n",
            "For Step: 6860 recon_loss: 0.19705194234848022 \tdiscriminator_loss: 1.094376564025879 \tgenerator_loss: 0.9665606021881104\n",
            "For Step: 6870 recon_loss: 0.1836010217666626 \tdiscriminator_loss: 1.1366875171661377 \tgenerator_loss: 0.8747376203536987\n",
            "For Step: 6880 recon_loss: 0.17530059814453125 \tdiscriminator_loss: 1.1179912090301514 \tgenerator_loss: 0.9224188327789307\n",
            "For Step: 6890 recon_loss: 0.17396362125873566 \tdiscriminator_loss: 1.2173573970794678 \tgenerator_loss: 0.8817296624183655\n",
            "For Step: 6900 recon_loss: 0.18136316537857056 \tdiscriminator_loss: 1.1042503118515015 \tgenerator_loss: 0.8986067175865173\n",
            "For Step: 6910 recon_loss: 0.19499200582504272 \tdiscriminator_loss: 1.0866413116455078 \tgenerator_loss: 0.9696266651153564\n",
            "For Step: 6920 recon_loss: 0.18158157169818878 \tdiscriminator_loss: 1.2051126956939697 \tgenerator_loss: 0.9624377489089966\n",
            "For Step: 6930 recon_loss: 0.19447104632854462 \tdiscriminator_loss: 1.0255050659179688 \tgenerator_loss: 0.987205982208252\n",
            "For Step: 6940 recon_loss: 0.1845501810312271 \tdiscriminator_loss: 1.052871584892273 \tgenerator_loss: 0.9212506413459778\n",
            "For Step: 6950 recon_loss: 0.19132429361343384 \tdiscriminator_loss: 0.9809799194335938 \tgenerator_loss: 0.9516691565513611\n",
            "For Step: 6960 recon_loss: 0.19006645679473877 \tdiscriminator_loss: 1.1240475177764893 \tgenerator_loss: 0.897233247756958\n",
            "For Step: 6970 recon_loss: 0.19171719253063202 \tdiscriminator_loss: 1.243274450302124 \tgenerator_loss: 0.9324772357940674\n",
            "For Step: 6980 recon_loss: 0.1806938350200653 \tdiscriminator_loss: 1.147899866104126 \tgenerator_loss: 0.9558382630348206\n",
            "For Step: 6990 recon_loss: 0.1865091472864151 \tdiscriminator_loss: 1.1898157596588135 \tgenerator_loss: 0.9363830089569092\n",
            "For Step: 7000 recon_loss: 0.18386541306972504 \tdiscriminator_loss: 1.2201699018478394 \tgenerator_loss: 0.9562616348266602\n",
            "For Step: 7010 recon_loss: 0.18608024716377258 \tdiscriminator_loss: 1.2379096746444702 \tgenerator_loss: 0.9158369302749634\n",
            "For Step: 7020 recon_loss: 0.18685053288936615 \tdiscriminator_loss: 1.1297080516815186 \tgenerator_loss: 0.928392767906189\n",
            "For Step: 7030 recon_loss: 0.17594584822654724 \tdiscriminator_loss: 1.1457724571228027 \tgenerator_loss: 0.8598810434341431\n",
            "For Step: 7040 recon_loss: 0.18596094846725464 \tdiscriminator_loss: 1.051093339920044 \tgenerator_loss: 0.9841809272766113\n",
            "For Step: 7050 recon_loss: 0.18824736773967743 \tdiscriminator_loss: 1.0694115161895752 \tgenerator_loss: 0.8788204789161682\n",
            "For Step: 7060 recon_loss: 0.18648721277713776 \tdiscriminator_loss: 1.077915072441101 \tgenerator_loss: 0.9995555877685547\n",
            "For Step: 7070 recon_loss: 0.17746730148792267 \tdiscriminator_loss: 1.0624058246612549 \tgenerator_loss: 0.9064257740974426\n",
            "For Step: 7080 recon_loss: 0.17131558060646057 \tdiscriminator_loss: 1.0078105926513672 \tgenerator_loss: 0.8655916452407837\n",
            "For Step: 7090 recon_loss: 0.1685142070055008 \tdiscriminator_loss: 1.163464069366455 \tgenerator_loss: 0.8520436882972717\n",
            "For Step: 7100 recon_loss: 0.20311293005943298 \tdiscriminator_loss: 0.9683678150177002 \tgenerator_loss: 0.9624980688095093\n",
            "For Step: 7110 recon_loss: 0.18484030663967133 \tdiscriminator_loss: 1.1620807647705078 \tgenerator_loss: 0.9235132336616516\n",
            "For Step: 7120 recon_loss: 0.18864195048809052 \tdiscriminator_loss: 1.1503061056137085 \tgenerator_loss: 0.9365224242210388\n",
            "For Step: 7130 recon_loss: 0.17422111332416534 \tdiscriminator_loss: 1.182765245437622 \tgenerator_loss: 0.8756806254386902\n",
            "For Step: 7140 recon_loss: 0.18682639300823212 \tdiscriminator_loss: 1.138117790222168 \tgenerator_loss: 0.9064544439315796\n",
            "For Step: 7150 recon_loss: 0.18071506917476654 \tdiscriminator_loss: 1.146761178970337 \tgenerator_loss: 0.899491548538208\n",
            "For Step: 7160 recon_loss: 0.171449214220047 \tdiscriminator_loss: 1.0581786632537842 \tgenerator_loss: 0.949947714805603\n",
            "For Step: 7170 recon_loss: 0.19017188251018524 \tdiscriminator_loss: 1.2286770343780518 \tgenerator_loss: 0.8853952884674072\n",
            "For Step: 7180 recon_loss: 0.17826403677463531 \tdiscriminator_loss: 1.1795461177825928 \tgenerator_loss: 0.8911625742912292\n",
            "For Step: 7190 recon_loss: 0.1809002161026001 \tdiscriminator_loss: 1.113829255104065 \tgenerator_loss: 0.9276215434074402\n",
            "For Step: 7200 recon_loss: 0.17685088515281677 \tdiscriminator_loss: 1.0787129402160645 \tgenerator_loss: 0.969132661819458\n",
            "For Step: 7210 recon_loss: 0.18333013355731964 \tdiscriminator_loss: 1.0678155422210693 \tgenerator_loss: 0.9938486218452454\n",
            "For Step: 7220 recon_loss: 0.18582484126091003 \tdiscriminator_loss: 1.0753064155578613 \tgenerator_loss: 0.9364676475524902\n",
            "For Step: 7230 recon_loss: 0.1843498796224594 \tdiscriminator_loss: 1.1754344701766968 \tgenerator_loss: 0.937411904335022\n",
            "For Step: 7240 recon_loss: 0.1901339888572693 \tdiscriminator_loss: 0.974319577217102 \tgenerator_loss: 0.9660027027130127\n",
            "For Step: 7250 recon_loss: 0.17976759374141693 \tdiscriminator_loss: 1.0529900789260864 \tgenerator_loss: 0.9495210647583008\n",
            "For Step: 7260 recon_loss: 0.17186595499515533 \tdiscriminator_loss: 1.1436163187026978 \tgenerator_loss: 0.9389623999595642\n",
            "For Step: 7270 recon_loss: 0.19787853956222534 \tdiscriminator_loss: 1.1095373630523682 \tgenerator_loss: 0.918394148349762\n",
            "For Step: 7280 recon_loss: 0.18173255026340485 \tdiscriminator_loss: 1.0552589893341064 \tgenerator_loss: 0.9915234446525574\n",
            "For Step: 7290 recon_loss: 0.1912049502134323 \tdiscriminator_loss: 1.0165754556655884 \tgenerator_loss: 0.9721891283988953\n",
            "For Step: 7300 recon_loss: 0.18284717202186584 \tdiscriminator_loss: 1.0787711143493652 \tgenerator_loss: 0.9699733257293701\n",
            "For Step: 7310 recon_loss: 0.16882526874542236 \tdiscriminator_loss: 0.9679197072982788 \tgenerator_loss: 0.9973949790000916\n",
            "For Step: 7320 recon_loss: 0.17969374358654022 \tdiscriminator_loss: 1.084660530090332 \tgenerator_loss: 0.8833154439926147\n",
            "For Step: 7330 recon_loss: 0.18185384571552277 \tdiscriminator_loss: 1.1576836109161377 \tgenerator_loss: 0.8766712546348572\n",
            "For Step: 7340 recon_loss: 0.1944938749074936 \tdiscriminator_loss: 1.0836378335952759 \tgenerator_loss: 0.909986138343811\n",
            "For Step: 7350 recon_loss: 0.17177125811576843 \tdiscriminator_loss: 1.0126807689666748 \tgenerator_loss: 0.967787504196167\n",
            "For Step: 7360 recon_loss: 0.18432126939296722 \tdiscriminator_loss: 1.0240797996520996 \tgenerator_loss: 0.9811607003211975\n",
            "For Step: 7370 recon_loss: 0.18455351889133453 \tdiscriminator_loss: 0.9965171813964844 \tgenerator_loss: 1.0003893375396729\n",
            "For Step: 7380 recon_loss: 0.18774530291557312 \tdiscriminator_loss: 1.1158077716827393 \tgenerator_loss: 0.9046812653541565\n",
            "For Step: 7390 recon_loss: 0.18131712079048157 \tdiscriminator_loss: 1.08864164352417 \tgenerator_loss: 0.9318380951881409\n",
            "For Step: 7400 recon_loss: 0.18349787592887878 \tdiscriminator_loss: 1.0137600898742676 \tgenerator_loss: 0.9247416853904724\n",
            "For Step: 7410 recon_loss: 0.18533407151699066 \tdiscriminator_loss: 1.1484851837158203 \tgenerator_loss: 0.939624011516571\n",
            "For Step: 7420 recon_loss: 0.17531390488147736 \tdiscriminator_loss: 1.0986406803131104 \tgenerator_loss: 0.9755995273590088\n",
            "For Step: 7430 recon_loss: 0.1811995804309845 \tdiscriminator_loss: 1.06870698928833 \tgenerator_loss: 0.9532394409179688\n",
            "For Step: 7440 recon_loss: 0.18709582090377808 \tdiscriminator_loss: 1.0760600566864014 \tgenerator_loss: 0.9653387069702148\n",
            "For Step: 7450 recon_loss: 0.16998375952243805 \tdiscriminator_loss: 1.3016120195388794 \tgenerator_loss: 0.95672607421875\n",
            "For Step: 7460 recon_loss: 0.17452862858772278 \tdiscriminator_loss: 1.1118788719177246 \tgenerator_loss: 0.9393065571784973\n",
            "For Step: 7470 recon_loss: 0.18308314681053162 \tdiscriminator_loss: 1.1917167901992798 \tgenerator_loss: 0.9474450945854187\n",
            "For Step: 7480 recon_loss: 0.16906192898750305 \tdiscriminator_loss: 1.1364269256591797 \tgenerator_loss: 0.9839019775390625\n",
            "For Step: 7490 recon_loss: 0.18822655081748962 \tdiscriminator_loss: 1.0839135646820068 \tgenerator_loss: 0.931189775466919\n",
            "For Step: 7500 recon_loss: 0.19203388690948486 \tdiscriminator_loss: 1.2184886932373047 \tgenerator_loss: 0.9297152757644653\n",
            "For Step: 7510 recon_loss: 0.18490585684776306 \tdiscriminator_loss: 1.0098522901535034 \tgenerator_loss: 0.9825265407562256\n",
            "For Step: 7520 recon_loss: 0.18946823477745056 \tdiscriminator_loss: 1.066192865371704 \tgenerator_loss: 0.9852306842803955\n",
            "For Step: 7530 recon_loss: 0.18371446430683136 \tdiscriminator_loss: 1.1765103340148926 \tgenerator_loss: 0.9725464582443237\n",
            "For Step: 7540 recon_loss: 0.19571787118911743 \tdiscriminator_loss: 1.0519812107086182 \tgenerator_loss: 0.9469124674797058\n",
            "For Step: 7550 recon_loss: 0.1790781468153 \tdiscriminator_loss: 1.2187466621398926 \tgenerator_loss: 0.8777658343315125\n",
            "For Step: 7560 recon_loss: 0.19123825430870056 \tdiscriminator_loss: 1.069756031036377 \tgenerator_loss: 1.0696539878845215\n",
            "For Step: 7570 recon_loss: 0.18668662011623383 \tdiscriminator_loss: 1.0643731355667114 \tgenerator_loss: 0.9256973266601562\n",
            "For Step: 7580 recon_loss: 0.18916621804237366 \tdiscriminator_loss: 1.0931851863861084 \tgenerator_loss: 0.9533730149269104\n",
            "For Step: 7590 recon_loss: 0.19479770958423615 \tdiscriminator_loss: 1.126711368560791 \tgenerator_loss: 0.9704504013061523\n",
            "For Step: 7600 recon_loss: 0.17908024787902832 \tdiscriminator_loss: 1.18384850025177 \tgenerator_loss: 0.9081512093544006\n",
            "For Step: 7610 recon_loss: 0.18065479397773743 \tdiscriminator_loss: 1.2548645734786987 \tgenerator_loss: 0.9121357798576355\n",
            "For Step: 7620 recon_loss: 0.17855891585350037 \tdiscriminator_loss: 1.1402316093444824 \tgenerator_loss: 0.9733623266220093\n",
            "For Step: 7630 recon_loss: 0.1920068860054016 \tdiscriminator_loss: 1.1043140888214111 \tgenerator_loss: 0.9792630672454834\n",
            "For Step: 7640 recon_loss: 0.18770264089107513 \tdiscriminator_loss: 1.114203691482544 \tgenerator_loss: 0.9991067051887512\n",
            "For Step: 7650 recon_loss: 0.18134397268295288 \tdiscriminator_loss: 1.1286028623580933 \tgenerator_loss: 0.9306305646896362\n",
            "For Step: 7660 recon_loss: 0.1801445186138153 \tdiscriminator_loss: 1.034095048904419 \tgenerator_loss: 1.033062219619751\n",
            "For Step: 7670 recon_loss: 0.17289169132709503 \tdiscriminator_loss: 1.1464784145355225 \tgenerator_loss: 0.9232453107833862\n",
            "For Step: 7680 recon_loss: 0.198525071144104 \tdiscriminator_loss: 1.1295777559280396 \tgenerator_loss: 0.9895275831222534\n",
            "For Step: 7690 recon_loss: 0.1825147420167923 \tdiscriminator_loss: 1.0461468696594238 \tgenerator_loss: 0.9658337235450745\n",
            "For Step: 7700 recon_loss: 0.18881729245185852 \tdiscriminator_loss: 1.000868797302246 \tgenerator_loss: 1.0082988739013672\n",
            "For Step: 7710 recon_loss: 0.18363387882709503 \tdiscriminator_loss: 1.0970275402069092 \tgenerator_loss: 0.9899320602416992\n",
            "For Step: 7720 recon_loss: 0.17272892594337463 \tdiscriminator_loss: 1.0920398235321045 \tgenerator_loss: 0.96999192237854\n",
            "For Step: 7730 recon_loss: 0.18110840022563934 \tdiscriminator_loss: 1.0857607126235962 \tgenerator_loss: 0.9794604778289795\n",
            "For Step: 7740 recon_loss: 0.19209013879299164 \tdiscriminator_loss: 1.017404317855835 \tgenerator_loss: 1.0224051475524902\n",
            "For Step: 7750 recon_loss: 0.1887877881526947 \tdiscriminator_loss: 1.0136834383010864 \tgenerator_loss: 1.0450873374938965\n",
            "For Step: 7760 recon_loss: 0.1734170764684677 \tdiscriminator_loss: 1.1989511251449585 \tgenerator_loss: 0.9853225350379944\n",
            "For Step: 7770 recon_loss: 0.17077139019966125 \tdiscriminator_loss: 0.9782504439353943 \tgenerator_loss: 0.9942431449890137\n",
            "For Step: 7780 recon_loss: 0.174197718501091 \tdiscriminator_loss: 1.1170964241027832 \tgenerator_loss: 0.951110303401947\n",
            "For Step: 7790 recon_loss: 0.1941332072019577 \tdiscriminator_loss: 0.9898164868354797 \tgenerator_loss: 1.038205862045288\n",
            "For Step: 7800 recon_loss: 0.1910921335220337 \tdiscriminator_loss: 1.1839308738708496 \tgenerator_loss: 0.9320546984672546\n",
            "For Step: 7810 recon_loss: 0.1861533373594284 \tdiscriminator_loss: 1.1242787837982178 \tgenerator_loss: 0.9926571846008301\n",
            "For Step: 7820 recon_loss: 0.18537186086177826 \tdiscriminator_loss: 1.3462224006652832 \tgenerator_loss: 0.8381834626197815\n",
            "For Step: 7830 recon_loss: 0.19551192224025726 \tdiscriminator_loss: 1.121456265449524 \tgenerator_loss: 0.9939768314361572\n",
            "For Step: 7840 recon_loss: 0.1851212978363037 \tdiscriminator_loss: 1.0105341672897339 \tgenerator_loss: 1.0580685138702393\n",
            "For Step: 7850 recon_loss: 0.17576584219932556 \tdiscriminator_loss: 1.1732733249664307 \tgenerator_loss: 0.9780645370483398\n",
            "For Step: 7860 recon_loss: 0.19031944870948792 \tdiscriminator_loss: 0.9871481657028198 \tgenerator_loss: 1.0623599290847778\n",
            "For Step: 7870 recon_loss: 0.18542729318141937 \tdiscriminator_loss: 1.1726412773132324 \tgenerator_loss: 0.9710943698883057\n",
            "For Step: 7880 recon_loss: 0.18677684664726257 \tdiscriminator_loss: 1.151648998260498 \tgenerator_loss: 0.9811669588088989\n",
            "For Step: 7890 recon_loss: 0.1777849942445755 \tdiscriminator_loss: 1.2285174131393433 \tgenerator_loss: 0.9461854696273804\n",
            "For Step: 7900 recon_loss: 0.17220240831375122 \tdiscriminator_loss: 1.0502225160598755 \tgenerator_loss: 0.9914966225624084\n",
            "For Step: 7910 recon_loss: 0.17499959468841553 \tdiscriminator_loss: 1.147019863128662 \tgenerator_loss: 1.0223842859268188\n",
            "For Step: 7920 recon_loss: 0.17949894070625305 \tdiscriminator_loss: 1.0947457551956177 \tgenerator_loss: 0.9346975684165955\n",
            "For Step: 7930 recon_loss: 0.17602384090423584 \tdiscriminator_loss: 1.1639666557312012 \tgenerator_loss: 0.9770729541778564\n",
            "For Step: 7940 recon_loss: 0.18646426498889923 \tdiscriminator_loss: 1.0689396858215332 \tgenerator_loss: 1.03049898147583\n",
            "For Step: 7950 recon_loss: 0.18943294882774353 \tdiscriminator_loss: 1.1407687664031982 \tgenerator_loss: 0.9805310964584351\n",
            "For Step: 7960 recon_loss: 0.17491206526756287 \tdiscriminator_loss: 1.1138970851898193 \tgenerator_loss: 1.0128331184387207\n",
            "For Step: 7970 recon_loss: 0.1781768798828125 \tdiscriminator_loss: 1.0698459148406982 \tgenerator_loss: 1.0193560123443604\n",
            "For Step: 7980 recon_loss: 0.19700933992862701 \tdiscriminator_loss: 1.1547224521636963 \tgenerator_loss: 1.031578540802002\n",
            "For Step: 7990 recon_loss: 0.17191001772880554 \tdiscriminator_loss: 1.0525360107421875 \tgenerator_loss: 1.0477774143218994\n",
            "For Step: 8000 recon_loss: 0.18054214119911194 \tdiscriminator_loss: 1.063963770866394 \tgenerator_loss: 0.9934731721878052\n",
            "For Step: 8010 recon_loss: 0.18051186203956604 \tdiscriminator_loss: 1.0907618999481201 \tgenerator_loss: 1.0104670524597168\n",
            "For Step: 8020 recon_loss: 0.18726076185703278 \tdiscriminator_loss: 1.2520272731781006 \tgenerator_loss: 0.951994776725769\n",
            "For Step: 8030 recon_loss: 0.17750152945518494 \tdiscriminator_loss: 1.1327024698257446 \tgenerator_loss: 0.9644477367401123\n",
            "For Step: 8040 recon_loss: 0.17520111799240112 \tdiscriminator_loss: 1.0681380033493042 \tgenerator_loss: 0.9646656513214111\n",
            "For Step: 8050 recon_loss: 0.18662823736667633 \tdiscriminator_loss: 1.1058895587921143 \tgenerator_loss: 0.9448709487915039\n",
            "For Step: 8060 recon_loss: 0.18921154737472534 \tdiscriminator_loss: 1.1139395236968994 \tgenerator_loss: 0.9956202507019043\n",
            "For Step: 8070 recon_loss: 0.17600548267364502 \tdiscriminator_loss: 1.247094988822937 \tgenerator_loss: 0.9489151835441589\n",
            "For Step: 8080 recon_loss: 0.18830043077468872 \tdiscriminator_loss: 1.111905574798584 \tgenerator_loss: 0.9570285081863403\n",
            "For Step: 8090 recon_loss: 0.17487159371376038 \tdiscriminator_loss: 1.1574287414550781 \tgenerator_loss: 0.962963342666626\n",
            "For Step: 8100 recon_loss: 0.16314253211021423 \tdiscriminator_loss: 1.091018557548523 \tgenerator_loss: 0.9733222723007202\n",
            "For Step: 8110 recon_loss: 0.17795813083648682 \tdiscriminator_loss: 1.0964198112487793 \tgenerator_loss: 0.9959112405776978\n",
            "For Step: 8120 recon_loss: 0.17890490591526031 \tdiscriminator_loss: 1.2006577253341675 \tgenerator_loss: 0.9585380554199219\n",
            "For Step: 8130 recon_loss: 0.19835618138313293 \tdiscriminator_loss: 1.1694363355636597 \tgenerator_loss: 0.9796940684318542\n",
            "For Step: 8140 recon_loss: 0.17559325695037842 \tdiscriminator_loss: 1.10635507106781 \tgenerator_loss: 0.9874279499053955\n",
            "For Step: 8150 recon_loss: 0.18312981724739075 \tdiscriminator_loss: 1.0718269348144531 \tgenerator_loss: 0.9991707801818848\n",
            "For Step: 8160 recon_loss: 0.19561874866485596 \tdiscriminator_loss: 1.1390944719314575 \tgenerator_loss: 0.9758340120315552\n",
            "For Step: 8170 recon_loss: 0.18402843177318573 \tdiscriminator_loss: 1.1488442420959473 \tgenerator_loss: 0.9590365886688232\n",
            "For Step: 8180 recon_loss: 0.1909940540790558 \tdiscriminator_loss: 1.0855543613433838 \tgenerator_loss: 1.050889253616333\n",
            "For Step: 8190 recon_loss: 0.17894645035266876 \tdiscriminator_loss: 1.0397149324417114 \tgenerator_loss: 1.0475695133209229\n",
            "For Step: 8200 recon_loss: 0.2013310045003891 \tdiscriminator_loss: 1.110001564025879 \tgenerator_loss: 0.9288206100463867\n",
            "For Step: 8210 recon_loss: 0.1960884928703308 \tdiscriminator_loss: 1.1108205318450928 \tgenerator_loss: 0.9474148154258728\n",
            "For Step: 8220 recon_loss: 0.18604819476604462 \tdiscriminator_loss: 1.1178336143493652 \tgenerator_loss: 0.979290246963501\n",
            "For Step: 8230 recon_loss: 0.1714588850736618 \tdiscriminator_loss: 1.2507376670837402 \tgenerator_loss: 0.8898534178733826\n",
            "For Step: 8240 recon_loss: 0.18627141416072845 \tdiscriminator_loss: 1.1061205863952637 \tgenerator_loss: 1.077575922012329\n",
            "For Step: 8250 recon_loss: 0.17769722640514374 \tdiscriminator_loss: 1.1678318977355957 \tgenerator_loss: 0.9656835794448853\n",
            "For Step: 8260 recon_loss: 0.18105429410934448 \tdiscriminator_loss: 1.1271331310272217 \tgenerator_loss: 0.9853622317314148\n",
            "For Step: 8270 recon_loss: 0.1734355092048645 \tdiscriminator_loss: 1.0958750247955322 \tgenerator_loss: 1.028423547744751\n",
            "For Step: 8280 recon_loss: 0.17610681056976318 \tdiscriminator_loss: 1.097245216369629 \tgenerator_loss: 0.9889088273048401\n",
            "For Step: 8290 recon_loss: 0.1845327466726303 \tdiscriminator_loss: 1.0915427207946777 \tgenerator_loss: 1.0315375328063965\n",
            "For Step: 8300 recon_loss: 0.18076872825622559 \tdiscriminator_loss: 1.0044808387756348 \tgenerator_loss: 1.0078235864639282\n",
            "For Step: 8310 recon_loss: 0.17565244436264038 \tdiscriminator_loss: 1.0880285501480103 \tgenerator_loss: 0.9018731117248535\n",
            "For Step: 8320 recon_loss: 0.18304146826267242 \tdiscriminator_loss: 1.228999137878418 \tgenerator_loss: 0.8716466426849365\n",
            "For Step: 8330 recon_loss: 0.18925349414348602 \tdiscriminator_loss: 1.0545915365219116 \tgenerator_loss: 1.0441513061523438\n",
            "For Step: 8340 recon_loss: 0.18955150246620178 \tdiscriminator_loss: 1.1309106349945068 \tgenerator_loss: 0.9860290288925171\n",
            "For Step: 8350 recon_loss: 0.1814786046743393 \tdiscriminator_loss: 1.1544508934020996 \tgenerator_loss: 1.018890380859375\n",
            "For Step: 8360 recon_loss: 0.1859109103679657 \tdiscriminator_loss: 1.0999418497085571 \tgenerator_loss: 0.9838849306106567\n",
            "For Step: 8370 recon_loss: 0.18350036442279816 \tdiscriminator_loss: 1.1000598669052124 \tgenerator_loss: 0.9739974141120911\n",
            "For Step: 8380 recon_loss: 0.17770902812480927 \tdiscriminator_loss: 1.0872397422790527 \tgenerator_loss: 0.9904794692993164\n",
            "For Step: 8390 recon_loss: 0.1929503083229065 \tdiscriminator_loss: 1.1720956563949585 \tgenerator_loss: 1.0047078132629395\n",
            "For Step: 8400 recon_loss: 0.19526968896389008 \tdiscriminator_loss: 1.2324038743972778 \tgenerator_loss: 1.0374330282211304\n",
            "For Step: 8410 recon_loss: 0.17254780232906342 \tdiscriminator_loss: 1.223046064376831 \tgenerator_loss: 0.9557265639305115\n",
            "For Step: 8420 recon_loss: 0.176192045211792 \tdiscriminator_loss: 1.1665689945220947 \tgenerator_loss: 0.8440033197402954\n",
            "For Step: 8430 recon_loss: 0.18937146663665771 \tdiscriminator_loss: 1.1203266382217407 \tgenerator_loss: 0.9346306324005127\n",
            "For Step: 8440 recon_loss: 0.181082084774971 \tdiscriminator_loss: 1.2859153747558594 \tgenerator_loss: 0.8224244117736816\n",
            "For Step: 8450 recon_loss: 0.1813725084066391 \tdiscriminator_loss: 1.159124493598938 \tgenerator_loss: 0.9845075607299805\n",
            "For Step: 8460 recon_loss: 0.18083253502845764 \tdiscriminator_loss: 1.245605707168579 \tgenerator_loss: 0.9463804960250854\n",
            "For Step: 8470 recon_loss: 0.17079786956310272 \tdiscriminator_loss: 1.1403244733810425 \tgenerator_loss: 1.0014119148254395\n",
            "For Step: 8480 recon_loss: 0.169821098446846 \tdiscriminator_loss: 1.006742238998413 \tgenerator_loss: 1.007448434829712\n",
            "For Step: 8490 recon_loss: 0.18085207045078278 \tdiscriminator_loss: 1.0380674600601196 \tgenerator_loss: 0.9470178484916687\n",
            "For Step: 8500 recon_loss: 0.17962875962257385 \tdiscriminator_loss: 1.0855700969696045 \tgenerator_loss: 0.9794426560401917\n",
            "For Step: 8510 recon_loss: 0.18299439549446106 \tdiscriminator_loss: 1.1955523490905762 \tgenerator_loss: 0.9647586345672607\n",
            "For Step: 8520 recon_loss: 0.1915464997291565 \tdiscriminator_loss: 1.1313257217407227 \tgenerator_loss: 1.016781210899353\n",
            "For Step: 8530 recon_loss: 0.1839451938867569 \tdiscriminator_loss: 1.1576533317565918 \tgenerator_loss: 0.9721423387527466\n",
            "For Step: 8540 recon_loss: 0.1794925183057785 \tdiscriminator_loss: 1.1729395389556885 \tgenerator_loss: 0.9899516105651855\n",
            "For Step: 8550 recon_loss: 0.17605699598789215 \tdiscriminator_loss: 1.0816155672073364 \tgenerator_loss: 1.0215365886688232\n",
            "For Step: 8560 recon_loss: 0.18533562123775482 \tdiscriminator_loss: 1.1339387893676758 \tgenerator_loss: 0.989055871963501\n",
            "For Step: 8570 recon_loss: 0.18304228782653809 \tdiscriminator_loss: 1.1031625270843506 \tgenerator_loss: 0.9633389711380005\n",
            "For Step: 8580 recon_loss: 0.17394891381263733 \tdiscriminator_loss: 1.0018759965896606 \tgenerator_loss: 1.0387378931045532\n",
            "For Step: 8590 recon_loss: 0.1985171139240265 \tdiscriminator_loss: 0.9914060831069946 \tgenerator_loss: 1.0633432865142822\n",
            "For Step: 8600 recon_loss: 0.19407518208026886 \tdiscriminator_loss: 1.1337661743164062 \tgenerator_loss: 1.0244874954223633\n",
            "For Step: 8610 recon_loss: 0.18028557300567627 \tdiscriminator_loss: 1.0622003078460693 \tgenerator_loss: 1.0155717134475708\n",
            "For Step: 8620 recon_loss: 0.17224761843681335 \tdiscriminator_loss: 1.1623910665512085 \tgenerator_loss: 1.0363497734069824\n",
            "For Step: 8630 recon_loss: 0.19450558722019196 \tdiscriminator_loss: 1.0884109735488892 \tgenerator_loss: 0.9847701787948608\n",
            "For Step: 8640 recon_loss: 0.18121743202209473 \tdiscriminator_loss: 1.2171775102615356 \tgenerator_loss: 0.9270339012145996\n",
            "For Step: 8650 recon_loss: 0.19538332521915436 \tdiscriminator_loss: 1.0888469219207764 \tgenerator_loss: 0.9660422801971436\n",
            "For Step: 8660 recon_loss: 0.16901342570781708 \tdiscriminator_loss: 1.1025402545928955 \tgenerator_loss: 0.9370728731155396\n",
            "For Step: 8670 recon_loss: 0.18117891252040863 \tdiscriminator_loss: 0.9522528648376465 \tgenerator_loss: 1.0233378410339355\n",
            "For Step: 8680 recon_loss: 0.1847778558731079 \tdiscriminator_loss: 1.0431640148162842 \tgenerator_loss: 1.0209505558013916\n",
            "For Step: 8690 recon_loss: 0.17014609277248383 \tdiscriminator_loss: 1.1357529163360596 \tgenerator_loss: 0.9877957701683044\n",
            "For Step: 8700 recon_loss: 0.17077282071113586 \tdiscriminator_loss: 1.0861479043960571 \tgenerator_loss: 0.9478411674499512\n",
            "For Step: 8710 recon_loss: 0.18328101933002472 \tdiscriminator_loss: 1.143646240234375 \tgenerator_loss: 1.0594167709350586\n",
            "For Step: 8720 recon_loss: 0.17370304465293884 \tdiscriminator_loss: 1.2084853649139404 \tgenerator_loss: 0.9989085793495178\n",
            "For Step: 8730 recon_loss: 0.17164592444896698 \tdiscriminator_loss: 1.0721583366394043 \tgenerator_loss: 1.0717074871063232\n",
            "For Step: 8740 recon_loss: 0.1795537918806076 \tdiscriminator_loss: 1.1339319944381714 \tgenerator_loss: 0.9755287766456604\n",
            "For Step: 8750 recon_loss: 0.17656655609607697 \tdiscriminator_loss: 1.0548310279846191 \tgenerator_loss: 1.0152065753936768\n",
            "For Step: 8760 recon_loss: 0.18097920715808868 \tdiscriminator_loss: 0.9251025915145874 \tgenerator_loss: 1.1017212867736816\n",
            "For Step: 8770 recon_loss: 0.18493017554283142 \tdiscriminator_loss: 1.0795700550079346 \tgenerator_loss: 1.0501738786697388\n",
            "For Step: 8780 recon_loss: 0.19255664944648743 \tdiscriminator_loss: 1.0664162635803223 \tgenerator_loss: 0.9940064549446106\n",
            "For Step: 8790 recon_loss: 0.18308612704277039 \tdiscriminator_loss: 1.1036648750305176 \tgenerator_loss: 1.0023517608642578\n",
            "For Step: 8800 recon_loss: 0.18867017328739166 \tdiscriminator_loss: 1.0307350158691406 \tgenerator_loss: 1.0445623397827148\n",
            "For Step: 8810 recon_loss: 0.18341270089149475 \tdiscriminator_loss: 1.053241491317749 \tgenerator_loss: 0.8961664438247681\n",
            "For Step: 8820 recon_loss: 0.17799067497253418 \tdiscriminator_loss: 1.2548243999481201 \tgenerator_loss: 0.9902186989784241\n",
            "For Step: 8830 recon_loss: 0.16945554316043854 \tdiscriminator_loss: 0.9677285552024841 \tgenerator_loss: 1.057300329208374\n",
            "For Step: 8840 recon_loss: 0.1861494928598404 \tdiscriminator_loss: 1.026303768157959 \tgenerator_loss: 0.9896842241287231\n",
            "For Step: 8850 recon_loss: 0.1842672973871231 \tdiscriminator_loss: 1.0749711990356445 \tgenerator_loss: 1.0025116205215454\n",
            "For Step: 8860 recon_loss: 0.1777506023645401 \tdiscriminator_loss: 1.13700270652771 \tgenerator_loss: 0.8862630128860474\n",
            "For Step: 8870 recon_loss: 0.17797242105007172 \tdiscriminator_loss: 1.178147554397583 \tgenerator_loss: 0.9724259376525879\n",
            "For Step: 8880 recon_loss: 0.18708011507987976 \tdiscriminator_loss: 1.1204729080200195 \tgenerator_loss: 0.9984055161476135\n",
            "For Step: 8890 recon_loss: 0.18657119572162628 \tdiscriminator_loss: 1.0117064714431763 \tgenerator_loss: 1.0074999332427979\n",
            "For Step: 8900 recon_loss: 0.18519961833953857 \tdiscriminator_loss: 1.1987322568893433 \tgenerator_loss: 0.9469833374023438\n",
            "For Step: 8910 recon_loss: 0.18603649735450745 \tdiscriminator_loss: 1.0955251455307007 \tgenerator_loss: 1.0112738609313965\n",
            "For Step: 8920 recon_loss: 0.18527057766914368 \tdiscriminator_loss: 1.1656301021575928 \tgenerator_loss: 0.9908463358879089\n",
            "For Step: 8930 recon_loss: 0.17623105645179749 \tdiscriminator_loss: 1.0471110343933105 \tgenerator_loss: 0.9792487025260925\n",
            "For Step: 8940 recon_loss: 0.17787617444992065 \tdiscriminator_loss: 1.2017457485198975 \tgenerator_loss: 1.0363068580627441\n",
            "For Step: 8950 recon_loss: 0.18124891817569733 \tdiscriminator_loss: 1.053757667541504 \tgenerator_loss: 1.0581148862838745\n",
            "For Step: 8960 recon_loss: 0.18734218180179596 \tdiscriminator_loss: 1.1366407871246338 \tgenerator_loss: 0.9519050121307373\n",
            "For Step: 8970 recon_loss: 0.18775439262390137 \tdiscriminator_loss: 1.1582818031311035 \tgenerator_loss: 0.9358660578727722\n",
            "For Step: 8980 recon_loss: 0.19293726980686188 \tdiscriminator_loss: 1.069577693939209 \tgenerator_loss: 0.9989403486251831\n",
            "For Step: 8990 recon_loss: 0.18662413954734802 \tdiscriminator_loss: 1.1663949489593506 \tgenerator_loss: 1.0465377569198608\n",
            "For Step: 9000 recon_loss: 0.1779383420944214 \tdiscriminator_loss: 1.1110830307006836 \tgenerator_loss: 0.9281681776046753\n",
            "For Step: 9010 recon_loss: 0.17810764908790588 \tdiscriminator_loss: 1.2134228944778442 \tgenerator_loss: 0.9793074131011963\n",
            "For Step: 9020 recon_loss: 0.1910524219274521 \tdiscriminator_loss: 1.1097662448883057 \tgenerator_loss: 0.9864939451217651\n",
            "For Step: 9030 recon_loss: 0.19107919931411743 \tdiscriminator_loss: 1.096728801727295 \tgenerator_loss: 1.0139706134796143\n",
            "For Step: 9040 recon_loss: 0.1655515879392624 \tdiscriminator_loss: 1.1184298992156982 \tgenerator_loss: 1.0210645198822021\n",
            "For Step: 9050 recon_loss: 0.17835485935211182 \tdiscriminator_loss: 1.1919395923614502 \tgenerator_loss: 1.0010437965393066\n",
            "For Step: 9060 recon_loss: 0.18567481637001038 \tdiscriminator_loss: 1.2249689102172852 \tgenerator_loss: 0.9645708799362183\n",
            "For Step: 9070 recon_loss: 0.1857479214668274 \tdiscriminator_loss: 1.1412277221679688 \tgenerator_loss: 0.9817652702331543\n",
            "For Step: 9080 recon_loss: 0.17581221461296082 \tdiscriminator_loss: 1.1820499897003174 \tgenerator_loss: 1.0345191955566406\n",
            "For Step: 9090 recon_loss: 0.1812724471092224 \tdiscriminator_loss: 1.0806849002838135 \tgenerator_loss: 0.994438648223877\n",
            "For Step: 9100 recon_loss: 0.17803040146827698 \tdiscriminator_loss: 1.1423038244247437 \tgenerator_loss: 1.0826301574707031\n",
            "For Step: 9110 recon_loss: 0.1798839122056961 \tdiscriminator_loss: 1.1360734701156616 \tgenerator_loss: 1.054661750793457\n",
            "For Step: 9120 recon_loss: 0.18538299202919006 \tdiscriminator_loss: 1.2022886276245117 \tgenerator_loss: 0.9969504475593567\n",
            "For Step: 9130 recon_loss: 0.17440827190876007 \tdiscriminator_loss: 1.1708788871765137 \tgenerator_loss: 0.969768226146698\n",
            "For Step: 9140 recon_loss: 0.16842252016067505 \tdiscriminator_loss: 1.1462430953979492 \tgenerator_loss: 0.9515918493270874\n",
            "For Step: 9150 recon_loss: 0.1832638531923294 \tdiscriminator_loss: 1.1423166990280151 \tgenerator_loss: 0.9962238073348999\n",
            "For Step: 9160 recon_loss: 0.17144739627838135 \tdiscriminator_loss: 1.0208430290222168 \tgenerator_loss: 1.013380765914917\n",
            "For Step: 9170 recon_loss: 0.179800346493721 \tdiscriminator_loss: 1.02907133102417 \tgenerator_loss: 0.9875140190124512\n",
            "For Step: 9180 recon_loss: 0.18726445734500885 \tdiscriminator_loss: 1.1533375978469849 \tgenerator_loss: 0.9113688468933105\n",
            "For Step: 9190 recon_loss: 0.18661364912986755 \tdiscriminator_loss: 1.136169672012329 \tgenerator_loss: 0.9797253608703613\n",
            "For Step: 9200 recon_loss: 0.17859618365764618 \tdiscriminator_loss: 0.9609382152557373 \tgenerator_loss: 1.0496536493301392\n",
            "For Step: 9210 recon_loss: 0.17043554782867432 \tdiscriminator_loss: 1.0577439069747925 \tgenerator_loss: 0.9611068367958069\n",
            "For Step: 9220 recon_loss: 0.17817078530788422 \tdiscriminator_loss: 0.9491399526596069 \tgenerator_loss: 1.0871776342391968\n",
            "For Step: 9230 recon_loss: 0.17623941600322723 \tdiscriminator_loss: 1.0343416929244995 \tgenerator_loss: 0.9776186943054199\n",
            "For Step: 9240 recon_loss: 0.18701070547103882 \tdiscriminator_loss: 1.1201677322387695 \tgenerator_loss: 1.0602507591247559\n",
            "For Step: 9250 recon_loss: 0.17207491397857666 \tdiscriminator_loss: 1.155570387840271 \tgenerator_loss: 0.9297580718994141\n",
            "For Step: 9260 recon_loss: 0.18823619186878204 \tdiscriminator_loss: 1.0751888751983643 \tgenerator_loss: 1.0813984870910645\n",
            "For Step: 9270 recon_loss: 0.1853693574666977 \tdiscriminator_loss: 0.9948382377624512 \tgenerator_loss: 1.058724045753479\n",
            "For Step: 9280 recon_loss: 0.17513103783130646 \tdiscriminator_loss: 1.0674169063568115 \tgenerator_loss: 1.039811134338379\n",
            "For Step: 9290 recon_loss: 0.1725727766752243 \tdiscriminator_loss: 1.054402232170105 \tgenerator_loss: 1.0275962352752686\n",
            "For Step: 9300 recon_loss: 0.17313802242279053 \tdiscriminator_loss: 1.0494252443313599 \tgenerator_loss: 1.0319263935089111\n",
            "For Step: 9310 recon_loss: 0.1773390918970108 \tdiscriminator_loss: 1.1912957429885864 \tgenerator_loss: 1.0491523742675781\n",
            "For Step: 9320 recon_loss: 0.17872354388237 \tdiscriminator_loss: 1.0703892707824707 \tgenerator_loss: 0.9922151565551758\n",
            "For Step: 9330 recon_loss: 0.17978648841381073 \tdiscriminator_loss: 1.170104742050171 \tgenerator_loss: 0.9375255107879639\n",
            "For Step: 9340 recon_loss: 0.17907460033893585 \tdiscriminator_loss: 1.1189278364181519 \tgenerator_loss: 1.038748025894165\n",
            "For Step: 9350 recon_loss: 0.18541410565376282 \tdiscriminator_loss: 1.125929832458496 \tgenerator_loss: 0.9221447706222534\n",
            "For Step: 9360 recon_loss: 0.19755028188228607 \tdiscriminator_loss: 1.0158798694610596 \tgenerator_loss: 1.0054881572723389\n",
            "For Step: 9370 recon_loss: 0.17919176816940308 \tdiscriminator_loss: 1.1768261194229126 \tgenerator_loss: 1.068544864654541\n",
            "For Step: 9380 recon_loss: 0.17697007954120636 \tdiscriminator_loss: 1.2037345170974731 \tgenerator_loss: 1.0700503587722778\n",
            "For Step: 9390 recon_loss: 0.17952881753444672 \tdiscriminator_loss: 1.1183550357818604 \tgenerator_loss: 1.0419042110443115\n",
            "For Step: 9400 recon_loss: 0.1813686341047287 \tdiscriminator_loss: 1.2517940998077393 \tgenerator_loss: 1.0031440258026123\n",
            "For Step: 9410 recon_loss: 0.18609276413917542 \tdiscriminator_loss: 1.0294138193130493 \tgenerator_loss: 1.0290576219558716\n",
            "For Step: 9420 recon_loss: 0.17259517312049866 \tdiscriminator_loss: 1.089220404624939 \tgenerator_loss: 1.0404942035675049\n",
            "For Step: 9430 recon_loss: 0.1878477931022644 \tdiscriminator_loss: 1.2722840309143066 \tgenerator_loss: 1.0239542722702026\n",
            "For Step: 9440 recon_loss: 0.1781899333000183 \tdiscriminator_loss: 1.1531568765640259 \tgenerator_loss: 0.9834363460540771\n",
            "For Step: 9450 recon_loss: 0.1963518112897873 \tdiscriminator_loss: 1.2702629566192627 \tgenerator_loss: 1.0106617212295532\n",
            "For Step: 9460 recon_loss: 0.1833508312702179 \tdiscriminator_loss: 1.1692218780517578 \tgenerator_loss: 0.9620944261550903\n",
            "For Step: 9470 recon_loss: 0.1713920682668686 \tdiscriminator_loss: 1.07326078414917 \tgenerator_loss: 0.9609531164169312\n",
            "For Step: 9480 recon_loss: 0.17605489492416382 \tdiscriminator_loss: 1.1192641258239746 \tgenerator_loss: 1.0692665576934814\n",
            "For Step: 9490 recon_loss: 0.18311896920204163 \tdiscriminator_loss: 1.0417863130569458 \tgenerator_loss: 0.9916143417358398\n",
            "For Step: 9500 recon_loss: 0.18382127583026886 \tdiscriminator_loss: 1.2274621725082397 \tgenerator_loss: 0.9686031341552734\n",
            "For Step: 9510 recon_loss: 0.1908285766839981 \tdiscriminator_loss: 0.9876065254211426 \tgenerator_loss: 1.0639077425003052\n",
            "For Step: 9520 recon_loss: 0.1787336766719818 \tdiscriminator_loss: 1.0505039691925049 \tgenerator_loss: 0.9834774732589722\n",
            "For Step: 9530 recon_loss: 0.18469791114330292 \tdiscriminator_loss: 1.140122652053833 \tgenerator_loss: 1.0904655456542969\n",
            "For Step: 9540 recon_loss: 0.1795840561389923 \tdiscriminator_loss: 1.105929970741272 \tgenerator_loss: 1.0445380210876465\n",
            "For Step: 9550 recon_loss: 0.1953621506690979 \tdiscriminator_loss: 0.9493895173072815 \tgenerator_loss: 1.1015342473983765\n",
            "For Step: 9560 recon_loss: 0.19161683320999146 \tdiscriminator_loss: 1.2087891101837158 \tgenerator_loss: 1.0488462448120117\n",
            "For Step: 9570 recon_loss: 0.1812259703874588 \tdiscriminator_loss: 1.2025797367095947 \tgenerator_loss: 0.9913791418075562\n",
            "For Step: 9580 recon_loss: 0.1767101138830185 \tdiscriminator_loss: 1.0983328819274902 \tgenerator_loss: 1.0570545196533203\n",
            "For Step: 9590 recon_loss: 0.17427347600460052 \tdiscriminator_loss: 1.045501947402954 \tgenerator_loss: 1.0378737449645996\n",
            "For Step: 9600 recon_loss: 0.18891970813274384 \tdiscriminator_loss: 1.1816887855529785 \tgenerator_loss: 0.9944731593132019\n",
            "For Step: 9610 recon_loss: 0.18484993278980255 \tdiscriminator_loss: 1.1766716241836548 \tgenerator_loss: 0.9535391330718994\n",
            "For Step: 9620 recon_loss: 0.17694734036922455 \tdiscriminator_loss: 1.0595052242279053 \tgenerator_loss: 1.0066834688186646\n",
            "For Step: 9630 recon_loss: 0.1870889812707901 \tdiscriminator_loss: 1.0393563508987427 \tgenerator_loss: 1.0645852088928223\n",
            "For Step: 9640 recon_loss: 0.17845597863197327 \tdiscriminator_loss: 1.0783072710037231 \tgenerator_loss: 1.0391091108322144\n",
            "For Step: 9650 recon_loss: 0.18757981061935425 \tdiscriminator_loss: 1.2259552478790283 \tgenerator_loss: 0.988052248954773\n",
            "For Step: 9660 recon_loss: 0.1755792796611786 \tdiscriminator_loss: 1.2103040218353271 \tgenerator_loss: 1.0193184614181519\n",
            "For Step: 9670 recon_loss: 0.17699559032917023 \tdiscriminator_loss: 1.1107215881347656 \tgenerator_loss: 0.9713972806930542\n",
            "For Step: 9680 recon_loss: 0.18013466894626617 \tdiscriminator_loss: 1.120486855506897 \tgenerator_loss: 1.0547124147415161\n",
            "For Step: 9690 recon_loss: 0.18470655381679535 \tdiscriminator_loss: 1.0113050937652588 \tgenerator_loss: 1.0396536588668823\n",
            "For Step: 9700 recon_loss: 0.173885777592659 \tdiscriminator_loss: 1.178385853767395 \tgenerator_loss: 1.04728364944458\n",
            "For Step: 9710 recon_loss: 0.1908295750617981 \tdiscriminator_loss: 0.9772944450378418 \tgenerator_loss: 0.9408535957336426\n",
            "For Step: 9720 recon_loss: 0.17226053774356842 \tdiscriminator_loss: 1.1975173950195312 \tgenerator_loss: 0.8986791968345642\n",
            "For Step: 9730 recon_loss: 0.1892925351858139 \tdiscriminator_loss: 1.0130831003189087 \tgenerator_loss: 1.0907741785049438\n",
            "For Step: 9740 recon_loss: 0.18249262869358063 \tdiscriminator_loss: 1.0433670282363892 \tgenerator_loss: 1.0234224796295166\n",
            "For Step: 9750 recon_loss: 0.1807730346918106 \tdiscriminator_loss: 1.0198029279708862 \tgenerator_loss: 1.0787668228149414\n",
            "For Step: 9760 recon_loss: 0.17627225816249847 \tdiscriminator_loss: 1.117588758468628 \tgenerator_loss: 1.0453882217407227\n",
            "For Step: 9770 recon_loss: 0.18126170337200165 \tdiscriminator_loss: 1.0743565559387207 \tgenerator_loss: 1.0611737966537476\n",
            "For Step: 9780 recon_loss: 0.1703130304813385 \tdiscriminator_loss: 1.0714322328567505 \tgenerator_loss: 1.0516417026519775\n",
            "For Step: 9790 recon_loss: 0.17821724712848663 \tdiscriminator_loss: 1.1472035646438599 \tgenerator_loss: 1.0347461700439453\n",
            "For Step: 9800 recon_loss: 0.18289496004581451 \tdiscriminator_loss: 1.0837600231170654 \tgenerator_loss: 1.0026415586471558\n",
            "For Step: 9810 recon_loss: 0.1855406016111374 \tdiscriminator_loss: 1.1124250888824463 \tgenerator_loss: 0.9729825854301453\n",
            "For Step: 9820 recon_loss: 0.18943564593791962 \tdiscriminator_loss: 1.1994937658309937 \tgenerator_loss: 1.0693092346191406\n",
            "For Step: 9830 recon_loss: 0.18128494918346405 \tdiscriminator_loss: 1.1457674503326416 \tgenerator_loss: 1.020636796951294\n",
            "For Step: 9840 recon_loss: 0.1740271896123886 \tdiscriminator_loss: 1.2055678367614746 \tgenerator_loss: 0.9903040528297424\n",
            "For Step: 9850 recon_loss: 0.18905402719974518 \tdiscriminator_loss: 1.1404602527618408 \tgenerator_loss: 0.9957712888717651\n",
            "For Step: 9860 recon_loss: 0.16673175990581512 \tdiscriminator_loss: 1.039918065071106 \tgenerator_loss: 1.0798406600952148\n",
            "For Step: 9870 recon_loss: 0.1610611528158188 \tdiscriminator_loss: 1.0871598720550537 \tgenerator_loss: 1.0269813537597656\n",
            "For Step: 9880 recon_loss: 0.1735089123249054 \tdiscriminator_loss: 1.0520563125610352 \tgenerator_loss: 0.9647947549819946\n",
            "For Step: 9890 recon_loss: 0.1907624751329422 \tdiscriminator_loss: 1.0565966367721558 \tgenerator_loss: 1.0393471717834473\n",
            "For Step: 9900 recon_loss: 0.1737126111984253 \tdiscriminator_loss: 1.1865310668945312 \tgenerator_loss: 0.9710294008255005\n",
            "For Step: 9910 recon_loss: 0.18988622725009918 \tdiscriminator_loss: 1.0449392795562744 \tgenerator_loss: 1.0511753559112549\n",
            "For Step: 9920 recon_loss: 0.19070374965667725 \tdiscriminator_loss: 1.0284836292266846 \tgenerator_loss: 1.064069390296936\n",
            "For Step: 9930 recon_loss: 0.170230433344841 \tdiscriminator_loss: 1.0309641361236572 \tgenerator_loss: 0.9858372211456299\n",
            "For Step: 9940 recon_loss: 0.1844363957643509 \tdiscriminator_loss: 1.0925946235656738 \tgenerator_loss: 1.0285676717758179\n",
            "For Step: 9950 recon_loss: 0.1823006272315979 \tdiscriminator_loss: 1.0818337202072144 \tgenerator_loss: 1.0525368452072144\n",
            "For Step: 9960 recon_loss: 0.17266081273555756 \tdiscriminator_loss: 1.1396985054016113 \tgenerator_loss: 0.9944840669631958\n",
            "For Step: 9970 recon_loss: 0.19399605691432953 \tdiscriminator_loss: 1.0578722953796387 \tgenerator_loss: 1.0719698667526245\n",
            "For Step: 9980 recon_loss: 0.1825435757637024 \tdiscriminator_loss: 1.084261417388916 \tgenerator_loss: 0.9372175335884094\n",
            "For Step: 9990 recon_loss: 0.2000497579574585 \tdiscriminator_loss: 1.062436580657959 \tgenerator_loss: 0.9992104768753052\n",
            "For Step: 10000 recon_loss: 0.17928537726402283 \tdiscriminator_loss: 1.1234424114227295 \tgenerator_loss: 0.9976421594619751\n",
            "For Step: 10010 recon_loss: 0.18767960369586945 \tdiscriminator_loss: 1.1064953804016113 \tgenerator_loss: 1.074135422706604\n",
            "For Step: 10020 recon_loss: 0.19578175246715546 \tdiscriminator_loss: 1.076804757118225 \tgenerator_loss: 1.0300880670547485\n",
            "For Step: 10030 recon_loss: 0.1749708205461502 \tdiscriminator_loss: 1.227480173110962 \tgenerator_loss: 0.9306795597076416\n",
            "For Step: 10040 recon_loss: 0.17304587364196777 \tdiscriminator_loss: 1.1435034275054932 \tgenerator_loss: 0.993004560470581\n",
            "For Step: 10050 recon_loss: 0.1843775361776352 \tdiscriminator_loss: 1.1514205932617188 \tgenerator_loss: 0.9655408263206482\n",
            "For Step: 10060 recon_loss: 0.1813156008720398 \tdiscriminator_loss: 1.1286396980285645 \tgenerator_loss: 1.0445541143417358\n",
            "For Step: 10070 recon_loss: 0.17829202115535736 \tdiscriminator_loss: 1.1539678573608398 \tgenerator_loss: 1.0222073793411255\n",
            "For Step: 10080 recon_loss: 0.18094107508659363 \tdiscriminator_loss: 1.191502332687378 \tgenerator_loss: 1.0082792043685913\n",
            "For Step: 10090 recon_loss: 0.18783575296401978 \tdiscriminator_loss: 1.055641531944275 \tgenerator_loss: 1.019818902015686\n",
            "For Step: 10100 recon_loss: 0.17736664414405823 \tdiscriminator_loss: 1.1986809968948364 \tgenerator_loss: 0.9503782987594604\n",
            "For Step: 10110 recon_loss: 0.18524812161922455 \tdiscriminator_loss: 1.2170413732528687 \tgenerator_loss: 0.8953088521957397\n",
            "For Step: 10120 recon_loss: 0.19588100910186768 \tdiscriminator_loss: 1.110895037651062 \tgenerator_loss: 0.990869402885437\n",
            "For Step: 10130 recon_loss: 0.16654852032661438 \tdiscriminator_loss: 1.1191446781158447 \tgenerator_loss: 0.9630870223045349\n",
            "For Step: 10140 recon_loss: 0.1891152262687683 \tdiscriminator_loss: 1.1925159692764282 \tgenerator_loss: 0.9847542643547058\n",
            "For Step: 10150 recon_loss: 0.17442575097084045 \tdiscriminator_loss: 1.0637052059173584 \tgenerator_loss: 0.9710298776626587\n",
            "For Step: 10160 recon_loss: 0.16619567573070526 \tdiscriminator_loss: 1.1621334552764893 \tgenerator_loss: 0.9531391263008118\n",
            "For Step: 10170 recon_loss: 0.19343873858451843 \tdiscriminator_loss: 1.0374586582183838 \tgenerator_loss: 1.079005241394043\n",
            "For Step: 10180 recon_loss: 0.16967138648033142 \tdiscriminator_loss: 1.0558629035949707 \tgenerator_loss: 1.0302720069885254\n",
            "For Step: 10190 recon_loss: 0.17023980617523193 \tdiscriminator_loss: 1.160412073135376 \tgenerator_loss: 0.938935399055481\n",
            "For Step: 10200 recon_loss: 0.19534897804260254 \tdiscriminator_loss: 1.1898138523101807 \tgenerator_loss: 1.0858955383300781\n",
            "For Step: 10210 recon_loss: 0.17995008826255798 \tdiscriminator_loss: 1.1356295347213745 \tgenerator_loss: 1.0054782629013062\n",
            "For Step: 10220 recon_loss: 0.18021778762340546 \tdiscriminator_loss: 1.055755615234375 \tgenerator_loss: 1.03108549118042\n",
            "For Step: 10230 recon_loss: 0.18215683102607727 \tdiscriminator_loss: 1.0533661842346191 \tgenerator_loss: 1.0665621757507324\n",
            "For Step: 10240 recon_loss: 0.19375211000442505 \tdiscriminator_loss: 1.041754961013794 \tgenerator_loss: 1.0763756036758423\n",
            "For Step: 10250 recon_loss: 0.18685832619667053 \tdiscriminator_loss: 1.1460072994232178 \tgenerator_loss: 1.0831506252288818\n",
            "For Step: 10260 recon_loss: 0.1805427372455597 \tdiscriminator_loss: 1.164358139038086 \tgenerator_loss: 0.9153904914855957\n",
            "For Step: 10270 recon_loss: 0.17111672461032867 \tdiscriminator_loss: 1.1069097518920898 \tgenerator_loss: 0.9823911190032959\n",
            "For Step: 10280 recon_loss: 0.18979710340499878 \tdiscriminator_loss: 1.0405484437942505 \tgenerator_loss: 1.0386180877685547\n",
            "For Step: 10290 recon_loss: 0.1846262514591217 \tdiscriminator_loss: 1.1722711324691772 \tgenerator_loss: 1.083575963973999\n",
            "For Step: 10300 recon_loss: 0.18072085082530975 \tdiscriminator_loss: 1.1373564004898071 \tgenerator_loss: 1.034549355506897\n",
            "For Step: 10310 recon_loss: 0.16974492371082306 \tdiscriminator_loss: 1.0912147760391235 \tgenerator_loss: 0.9644436836242676\n",
            "For Step: 10320 recon_loss: 0.1952810436487198 \tdiscriminator_loss: 1.0608121156692505 \tgenerator_loss: 1.0703264474868774\n",
            "For Step: 10330 recon_loss: 0.1784016489982605 \tdiscriminator_loss: 1.1566765308380127 \tgenerator_loss: 1.0126547813415527\n",
            "For Step: 10340 recon_loss: 0.18540090322494507 \tdiscriminator_loss: 1.2372899055480957 \tgenerator_loss: 0.9424984455108643\n",
            "For Step: 10350 recon_loss: 0.17230013012886047 \tdiscriminator_loss: 1.12530517578125 \tgenerator_loss: 0.9807305335998535\n",
            "For Step: 10360 recon_loss: 0.1877981722354889 \tdiscriminator_loss: 1.1255278587341309 \tgenerator_loss: 1.0008230209350586\n",
            "For Step: 10370 recon_loss: 0.16872355341911316 \tdiscriminator_loss: 1.2023371458053589 \tgenerator_loss: 0.9727382063865662\n",
            "For Step: 10380 recon_loss: 0.17985093593597412 \tdiscriminator_loss: 1.2350822687149048 \tgenerator_loss: 1.0410680770874023\n",
            "For Step: 10390 recon_loss: 0.18164443969726562 \tdiscriminator_loss: 1.0182108879089355 \tgenerator_loss: 1.0151755809783936\n",
            "For Step: 10400 recon_loss: 0.16985103487968445 \tdiscriminator_loss: 1.2443900108337402 \tgenerator_loss: 0.9834771752357483\n",
            "For Step: 10410 recon_loss: 0.16936592757701874 \tdiscriminator_loss: 0.9774652719497681 \tgenerator_loss: 1.0674842596054077\n",
            "For Step: 10420 recon_loss: 0.17637264728546143 \tdiscriminator_loss: 1.1591835021972656 \tgenerator_loss: 1.0305817127227783\n",
            "For Step: 10430 recon_loss: 0.18056005239486694 \tdiscriminator_loss: 1.163190484046936 \tgenerator_loss: 1.034114122390747\n",
            "For Step: 10440 recon_loss: 0.17882397770881653 \tdiscriminator_loss: 1.2066469192504883 \tgenerator_loss: 1.0304577350616455\n",
            "For Step: 10450 recon_loss: 0.16864317655563354 \tdiscriminator_loss: 1.2591071128845215 \tgenerator_loss: 0.9418913125991821\n",
            "For Step: 10460 recon_loss: 0.18209588527679443 \tdiscriminator_loss: 1.2206108570098877 \tgenerator_loss: 1.0628597736358643\n",
            "For Step: 10470 recon_loss: 0.1830589473247528 \tdiscriminator_loss: 1.0962889194488525 \tgenerator_loss: 0.9656074047088623\n",
            "For Step: 10480 recon_loss: 0.17970430850982666 \tdiscriminator_loss: 1.1020936965942383 \tgenerator_loss: 0.9478143453598022\n",
            "For Step: 10490 recon_loss: 0.18451684713363647 \tdiscriminator_loss: 1.1147788763046265 \tgenerator_loss: 0.9666357040405273\n",
            "For Step: 10500 recon_loss: 0.17546330392360687 \tdiscriminator_loss: 1.0698120594024658 \tgenerator_loss: 1.011987328529358\n",
            "For Step: 10510 recon_loss: 0.18085795640945435 \tdiscriminator_loss: 1.202906847000122 \tgenerator_loss: 0.9939349293708801\n",
            "For Step: 10520 recon_loss: 0.17470873892307281 \tdiscriminator_loss: 1.1053128242492676 \tgenerator_loss: 0.9332309365272522\n",
            "For Step: 10530 recon_loss: 0.1872108280658722 \tdiscriminator_loss: 0.9474883079528809 \tgenerator_loss: 1.0748777389526367\n",
            "For Step: 10540 recon_loss: 0.17991626262664795 \tdiscriminator_loss: 1.1122406721115112 \tgenerator_loss: 1.0510344505310059\n",
            "For Step: 10550 recon_loss: 0.1811659038066864 \tdiscriminator_loss: 1.0332422256469727 \tgenerator_loss: 1.0346183776855469\n",
            "For Step: 10560 recon_loss: 0.18229439854621887 \tdiscriminator_loss: 1.1721198558807373 \tgenerator_loss: 0.9752907752990723\n",
            "For Step: 10570 recon_loss: 0.17932701110839844 \tdiscriminator_loss: 1.0231322050094604 \tgenerator_loss: 1.0329523086547852\n",
            "For Step: 10580 recon_loss: 0.176347017288208 \tdiscriminator_loss: 1.1245076656341553 \tgenerator_loss: 0.9978272914886475\n",
            "For Step: 10590 recon_loss: 0.17787772417068481 \tdiscriminator_loss: 1.2346103191375732 \tgenerator_loss: 0.8907025456428528\n",
            "For Step: 10600 recon_loss: 0.18546812236309052 \tdiscriminator_loss: 1.214084267616272 \tgenerator_loss: 1.0040302276611328\n",
            "For Step: 10610 recon_loss: 0.18360067903995514 \tdiscriminator_loss: 1.177485466003418 \tgenerator_loss: 1.040958285331726\n",
            "For Step: 10620 recon_loss: 0.175688698887825 \tdiscriminator_loss: 1.12484610080719 \tgenerator_loss: 1.034738302230835\n",
            "For Step: 10630 recon_loss: 0.1773279756307602 \tdiscriminator_loss: 0.9889479279518127 \tgenerator_loss: 1.0052748918533325\n",
            "For Step: 10640 recon_loss: 0.17349261045455933 \tdiscriminator_loss: 1.1914880275726318 \tgenerator_loss: 0.966407835483551\n",
            "For Step: 10650 recon_loss: 0.1668938547372818 \tdiscriminator_loss: 1.237304449081421 \tgenerator_loss: 0.9295191764831543\n",
            "For Step: 10660 recon_loss: 0.1865502893924713 \tdiscriminator_loss: 1.1383004188537598 \tgenerator_loss: 1.0705615282058716\n",
            "For Step: 10670 recon_loss: 0.18085455894470215 \tdiscriminator_loss: 1.1815028190612793 \tgenerator_loss: 1.04143488407135\n",
            "For Step: 10680 recon_loss: 0.17238371074199677 \tdiscriminator_loss: 1.0888437032699585 \tgenerator_loss: 0.9664620161056519\n",
            "For Step: 10690 recon_loss: 0.17695479094982147 \tdiscriminator_loss: 1.1856017112731934 \tgenerator_loss: 1.0370975732803345\n",
            "For Step: 10700 recon_loss: 0.19103997945785522 \tdiscriminator_loss: 1.0657414197921753 \tgenerator_loss: 1.0253970623016357\n",
            "For Step: 10710 recon_loss: 0.18461266160011292 \tdiscriminator_loss: 1.1148197650909424 \tgenerator_loss: 1.069342851638794\n",
            "For Step: 10720 recon_loss: 0.18374863266944885 \tdiscriminator_loss: 1.090021014213562 \tgenerator_loss: 1.0053515434265137\n",
            "For Step: 10730 recon_loss: 0.18203724920749664 \tdiscriminator_loss: 1.2295681238174438 \tgenerator_loss: 0.9257065653800964\n",
            "For Step: 10740 recon_loss: 0.1744086593389511 \tdiscriminator_loss: 1.2076971530914307 \tgenerator_loss: 1.0464935302734375\n",
            "For Step: 10750 recon_loss: 0.17389161884784698 \tdiscriminator_loss: 1.1262619495391846 \tgenerator_loss: 1.0434958934783936\n",
            "For Step: 10760 recon_loss: 0.17867431044578552 \tdiscriminator_loss: 1.1585025787353516 \tgenerator_loss: 1.0663719177246094\n",
            "For Step: 10770 recon_loss: 0.18003927171230316 \tdiscriminator_loss: 1.1989123821258545 \tgenerator_loss: 1.0135133266448975\n",
            "For Step: 10780 recon_loss: 0.17894865572452545 \tdiscriminator_loss: 1.0494154691696167 \tgenerator_loss: 1.0353516340255737\n",
            "For Step: 10790 recon_loss: 0.17350074648857117 \tdiscriminator_loss: 1.0536643266677856 \tgenerator_loss: 1.0619639158248901\n",
            "For Step: 10800 recon_loss: 0.18070240318775177 \tdiscriminator_loss: 1.1548714637756348 \tgenerator_loss: 0.9780673384666443\n",
            "For Step: 10810 recon_loss: 0.17903168499469757 \tdiscriminator_loss: 1.0802721977233887 \tgenerator_loss: 0.971030056476593\n",
            "For Step: 10820 recon_loss: 0.19171135127544403 \tdiscriminator_loss: 1.1476449966430664 \tgenerator_loss: 0.9723153114318848\n",
            "For Step: 10830 recon_loss: 0.17017675936222076 \tdiscriminator_loss: 1.2570436000823975 \tgenerator_loss: 0.972601056098938\n",
            "For Step: 10840 recon_loss: 0.18390564620494843 \tdiscriminator_loss: 1.2034833431243896 \tgenerator_loss: 1.0537676811218262\n",
            "For Step: 10850 recon_loss: 0.18880710005760193 \tdiscriminator_loss: 0.9813771843910217 \tgenerator_loss: 1.096032738685608\n",
            "For Step: 10860 recon_loss: 0.1757981777191162 \tdiscriminator_loss: 1.077139973640442 \tgenerator_loss: 1.0330860614776611\n",
            "For Step: 10870 recon_loss: 0.17438654601573944 \tdiscriminator_loss: 1.2479392290115356 \tgenerator_loss: 1.0052387714385986\n",
            "For Step: 10880 recon_loss: 0.1858215183019638 \tdiscriminator_loss: 0.9753518104553223 \tgenerator_loss: 1.0025385618209839\n",
            "For Step: 10890 recon_loss: 0.1805558055639267 \tdiscriminator_loss: 1.2078235149383545 \tgenerator_loss: 0.9724009037017822\n",
            "For Step: 10900 recon_loss: 0.17549997568130493 \tdiscriminator_loss: 0.9959734082221985 \tgenerator_loss: 1.0525821447372437\n",
            "For Step: 10910 recon_loss: 0.17659924924373627 \tdiscriminator_loss: 1.074669599533081 \tgenerator_loss: 1.014390468597412\n",
            "For Step: 10920 recon_loss: 0.18706779181957245 \tdiscriminator_loss: 1.0455151796340942 \tgenerator_loss: 0.9804273843765259\n",
            "For Step: 10930 recon_loss: 0.17376609146595 \tdiscriminator_loss: 1.4257235527038574 \tgenerator_loss: 0.9206871390342712\n",
            "For Step: 10940 recon_loss: 0.17173005640506744 \tdiscriminator_loss: 1.278153657913208 \tgenerator_loss: 1.0256803035736084\n",
            "For Step: 10950 recon_loss: 0.1867312788963318 \tdiscriminator_loss: 1.096393346786499 \tgenerator_loss: 1.0770812034606934\n",
            "For Step: 10960 recon_loss: 0.16562135517597198 \tdiscriminator_loss: 1.0697002410888672 \tgenerator_loss: 0.9883478283882141\n",
            "For Step: 10970 recon_loss: 0.18254297971725464 \tdiscriminator_loss: 1.1648881435394287 \tgenerator_loss: 0.9501172304153442\n",
            "For Step: 10980 recon_loss: 0.1855258196592331 \tdiscriminator_loss: 1.134810447692871 \tgenerator_loss: 1.0200070142745972\n",
            "For Step: 10990 recon_loss: 0.17623960971832275 \tdiscriminator_loss: 1.1953423023223877 \tgenerator_loss: 1.0272653102874756\n",
            "For Step: 11000 recon_loss: 0.18608048558235168 \tdiscriminator_loss: 1.1854177713394165 \tgenerator_loss: 1.0796964168548584\n",
            "For Step: 11010 recon_loss: 0.17469124495983124 \tdiscriminator_loss: 1.1537299156188965 \tgenerator_loss: 0.9615141749382019\n",
            "For Step: 11020 recon_loss: 0.1761351376771927 \tdiscriminator_loss: 1.165863275527954 \tgenerator_loss: 0.9836532473564148\n",
            "For Step: 11030 recon_loss: 0.18604309856891632 \tdiscriminator_loss: 1.0200947523117065 \tgenerator_loss: 1.061707615852356\n",
            "For Step: 11040 recon_loss: 0.1826143115758896 \tdiscriminator_loss: 1.1909370422363281 \tgenerator_loss: 0.9771588444709778\n",
            "For Step: 11050 recon_loss: 0.18272468447685242 \tdiscriminator_loss: 1.1282413005828857 \tgenerator_loss: 0.9993948936462402\n",
            "For Step: 11060 recon_loss: 0.17704243957996368 \tdiscriminator_loss: 1.1468851566314697 \tgenerator_loss: 0.9840344190597534\n",
            "For Step: 11070 recon_loss: 0.17137864232063293 \tdiscriminator_loss: 1.1550679206848145 \tgenerator_loss: 0.9637657403945923\n",
            "For Step: 11080 recon_loss: 0.16796009242534637 \tdiscriminator_loss: 0.9983430504798889 \tgenerator_loss: 0.9834557771682739\n",
            "For Step: 11090 recon_loss: 0.1859802007675171 \tdiscriminator_loss: 1.1840051412582397 \tgenerator_loss: 1.010174036026001\n",
            "For Step: 11100 recon_loss: 0.18057313561439514 \tdiscriminator_loss: 1.1160314083099365 \tgenerator_loss: 1.004305362701416\n",
            "For Step: 11110 recon_loss: 0.1987103372812271 \tdiscriminator_loss: 1.0857326984405518 \tgenerator_loss: 1.0333195924758911\n",
            "For Step: 11120 recon_loss: 0.18555380403995514 \tdiscriminator_loss: 1.0403859615325928 \tgenerator_loss: 1.0603995323181152\n",
            "For Step: 11130 recon_loss: 0.17399775981903076 \tdiscriminator_loss: 1.19917631149292 \tgenerator_loss: 0.9868848323822021\n",
            "For Step: 11140 recon_loss: 0.19051022827625275 \tdiscriminator_loss: 1.0378979444503784 \tgenerator_loss: 1.0992650985717773\n",
            "For Step: 11150 recon_loss: 0.1799401044845581 \tdiscriminator_loss: 1.15517258644104 \tgenerator_loss: 0.9956037998199463\n",
            "For Step: 11160 recon_loss: 0.18258017301559448 \tdiscriminator_loss: 1.1072461605072021 \tgenerator_loss: 1.010159969329834\n",
            "For Step: 11170 recon_loss: 0.1761166751384735 \tdiscriminator_loss: 1.0754878520965576 \tgenerator_loss: 1.0084927082061768\n",
            "For Step: 11180 recon_loss: 0.18714798986911774 \tdiscriminator_loss: 1.053505301475525 \tgenerator_loss: 1.0507948398590088\n",
            "For Step: 11190 recon_loss: 0.1682957261800766 \tdiscriminator_loss: 1.1285358667373657 \tgenerator_loss: 1.0643975734710693\n",
            "For Step: 11200 recon_loss: 0.18435630202293396 \tdiscriminator_loss: 1.191514015197754 \tgenerator_loss: 1.0055906772613525\n",
            "For Step: 11210 recon_loss: 0.18723183870315552 \tdiscriminator_loss: 1.0631272792816162 \tgenerator_loss: 0.968802273273468\n",
            "For Step: 11220 recon_loss: 0.17471343278884888 \tdiscriminator_loss: 1.0304597616195679 \tgenerator_loss: 0.9735627174377441\n",
            "For Step: 11230 recon_loss: 0.1783072054386139 \tdiscriminator_loss: 1.1063568592071533 \tgenerator_loss: 0.9231452345848083\n",
            "For Step: 11240 recon_loss: 0.188277930021286 \tdiscriminator_loss: 1.051414966583252 \tgenerator_loss: 1.0713961124420166\n",
            "For Step: 11250 recon_loss: 0.17011758685112 \tdiscriminator_loss: 1.102824091911316 \tgenerator_loss: 1.005849003791809\n",
            "For Step: 11260 recon_loss: 0.17685897648334503 \tdiscriminator_loss: 1.1048551797866821 \tgenerator_loss: 0.9589502811431885\n",
            "For Step: 11270 recon_loss: 0.17129018902778625 \tdiscriminator_loss: 1.064610242843628 \tgenerator_loss: 1.0227473974227905\n",
            "For Step: 11280 recon_loss: 0.17738044261932373 \tdiscriminator_loss: 1.1423954963684082 \tgenerator_loss: 1.084535837173462\n",
            "For Step: 11290 recon_loss: 0.19068694114685059 \tdiscriminator_loss: 1.0776891708374023 \tgenerator_loss: 0.9657148718833923\n",
            "For Step: 11300 recon_loss: 0.17300726473331451 \tdiscriminator_loss: 1.071507215499878 \tgenerator_loss: 1.0311163663864136\n",
            "For Step: 11310 recon_loss: 0.1935362070798874 \tdiscriminator_loss: 1.092164158821106 \tgenerator_loss: 1.050283670425415\n",
            "For Step: 11320 recon_loss: 0.18303057551383972 \tdiscriminator_loss: 1.0172456502914429 \tgenerator_loss: 1.0495588779449463\n",
            "For Step: 11330 recon_loss: 0.18762341141700745 \tdiscriminator_loss: 1.0992639064788818 \tgenerator_loss: 1.018083095550537\n",
            "For Step: 11340 recon_loss: 0.1908184289932251 \tdiscriminator_loss: 1.1378440856933594 \tgenerator_loss: 0.9972962141036987\n",
            "For Step: 11350 recon_loss: 0.19094829261302948 \tdiscriminator_loss: 1.0787054300308228 \tgenerator_loss: 1.0471022129058838\n",
            "For Step: 11360 recon_loss: 0.1679973602294922 \tdiscriminator_loss: 1.146841049194336 \tgenerator_loss: 0.9970848560333252\n",
            "For Step: 11370 recon_loss: 0.18023289740085602 \tdiscriminator_loss: 1.1777441501617432 \tgenerator_loss: 0.9927805662155151\n",
            "For Step: 11380 recon_loss: 0.18491140007972717 \tdiscriminator_loss: 1.170019507408142 \tgenerator_loss: 0.9575546979904175\n",
            "For Step: 11390 recon_loss: 0.18404917418956757 \tdiscriminator_loss: 1.206763505935669 \tgenerator_loss: 1.0211446285247803\n",
            "For Step: 11400 recon_loss: 0.16861093044281006 \tdiscriminator_loss: 1.0667506456375122 \tgenerator_loss: 0.9857732057571411\n",
            "For Step: 11410 recon_loss: 0.18078358471393585 \tdiscriminator_loss: 1.1559407711029053 \tgenerator_loss: 0.9903055429458618\n",
            "For Step: 11420 recon_loss: 0.17460773885250092 \tdiscriminator_loss: 1.2643332481384277 \tgenerator_loss: 0.9771942496299744\n",
            "For Step: 11430 recon_loss: 0.17812366783618927 \tdiscriminator_loss: 1.0139036178588867 \tgenerator_loss: 1.016087532043457\n",
            "For Step: 11440 recon_loss: 0.17387346923351288 \tdiscriminator_loss: 1.169527530670166 \tgenerator_loss: 1.0501651763916016\n",
            "For Step: 11450 recon_loss: 0.18498501181602478 \tdiscriminator_loss: 1.096653699874878 \tgenerator_loss: 1.057971715927124\n",
            "For Step: 11460 recon_loss: 0.17106138169765472 \tdiscriminator_loss: 1.0494353771209717 \tgenerator_loss: 1.031653642654419\n",
            "For Step: 11470 recon_loss: 0.17830194532871246 \tdiscriminator_loss: 1.1330221891403198 \tgenerator_loss: 0.9828716516494751\n",
            "For Step: 11480 recon_loss: 0.18124008178710938 \tdiscriminator_loss: 1.1691803932189941 \tgenerator_loss: 0.9031005501747131\n",
            "For Step: 11490 recon_loss: 0.18365126848220825 \tdiscriminator_loss: 1.1471214294433594 \tgenerator_loss: 1.080904245376587\n",
            "For Step: 11500 recon_loss: 0.18524183332920074 \tdiscriminator_loss: 1.069976568222046 \tgenerator_loss: 1.0558791160583496\n",
            "For Step: 11510 recon_loss: 0.17779730260372162 \tdiscriminator_loss: 1.1777712106704712 \tgenerator_loss: 0.9197478890419006\n",
            "For Step: 11520 recon_loss: 0.18758393824100494 \tdiscriminator_loss: 1.0724883079528809 \tgenerator_loss: 1.0224905014038086\n",
            "For Step: 11530 recon_loss: 0.18598048388957977 \tdiscriminator_loss: 1.1971185207366943 \tgenerator_loss: 0.9375016689300537\n",
            "For Step: 11540 recon_loss: 0.1812864989042282 \tdiscriminator_loss: 1.0574796199798584 \tgenerator_loss: 1.0187969207763672\n",
            "For Step: 11550 recon_loss: 0.1847487837076187 \tdiscriminator_loss: 1.150168538093567 \tgenerator_loss: 0.9483462572097778\n",
            "For Step: 11560 recon_loss: 0.19073942303657532 \tdiscriminator_loss: 1.3167791366577148 \tgenerator_loss: 0.8859468698501587\n",
            "For Step: 11570 recon_loss: 0.18678849935531616 \tdiscriminator_loss: 1.183369517326355 \tgenerator_loss: 0.9826931953430176\n",
            "For Step: 11580 recon_loss: 0.1776304692029953 \tdiscriminator_loss: 1.1798114776611328 \tgenerator_loss: 0.9678266048431396\n",
            "For Step: 11590 recon_loss: 0.17793433368206024 \tdiscriminator_loss: 1.0879623889923096 \tgenerator_loss: 1.0641930103302002\n",
            "For Step: 11600 recon_loss: 0.17753392457962036 \tdiscriminator_loss: 1.1324838399887085 \tgenerator_loss: 0.9722468852996826\n",
            "For Step: 11610 recon_loss: 0.18868249654769897 \tdiscriminator_loss: 1.075117588043213 \tgenerator_loss: 1.0112935304641724\n",
            "For Step: 11620 recon_loss: 0.17751437425613403 \tdiscriminator_loss: 1.1703133583068848 \tgenerator_loss: 0.9474849700927734\n",
            "For Step: 11630 recon_loss: 0.18743641674518585 \tdiscriminator_loss: 1.1550613641738892 \tgenerator_loss: 0.9211127161979675\n",
            "For Step: 11640 recon_loss: 0.17899635434150696 \tdiscriminator_loss: 1.1804251670837402 \tgenerator_loss: 0.9836367964744568\n",
            "For Step: 11650 recon_loss: 0.18795986473560333 \tdiscriminator_loss: 1.10978102684021 \tgenerator_loss: 0.9814176559448242\n",
            "For Step: 11660 recon_loss: 0.17291134595870972 \tdiscriminator_loss: 1.2773220539093018 \tgenerator_loss: 0.8966957330703735\n",
            "For Step: 11670 recon_loss: 0.1692642867565155 \tdiscriminator_loss: 1.0716900825500488 \tgenerator_loss: 0.9715317487716675\n",
            "For Step: 11680 recon_loss: 0.1981208324432373 \tdiscriminator_loss: 1.1236282587051392 \tgenerator_loss: 0.964268684387207\n",
            "For Step: 11690 recon_loss: 0.18165239691734314 \tdiscriminator_loss: 1.122132420539856 \tgenerator_loss: 0.9836816787719727\n",
            "For Step: 11700 recon_loss: 0.18737761676311493 \tdiscriminator_loss: 1.0385489463806152 \tgenerator_loss: 1.1111156940460205\n",
            "For Step: 11710 recon_loss: 0.17364929616451263 \tdiscriminator_loss: 1.302290439605713 \tgenerator_loss: 0.9588533639907837\n",
            "For Step: 11720 recon_loss: 0.16281208395957947 \tdiscriminator_loss: 1.0717535018920898 \tgenerator_loss: 0.9934786558151245\n",
            "For Step: 11730 recon_loss: 0.18301673233509064 \tdiscriminator_loss: 1.3565534353256226 \tgenerator_loss: 0.8690143823623657\n",
            "For Step: 11740 recon_loss: 0.1900201141834259 \tdiscriminator_loss: 1.1265498399734497 \tgenerator_loss: 0.9955309629440308\n",
            "For Step: 11750 recon_loss: 0.1645711213350296 \tdiscriminator_loss: 1.060795545578003 \tgenerator_loss: 1.0258780717849731\n",
            "For Step: 11760 recon_loss: 0.17943203449249268 \tdiscriminator_loss: 1.121992826461792 \tgenerator_loss: 0.9687869548797607\n",
            "For Step: 11770 recon_loss: 0.18530306220054626 \tdiscriminator_loss: 1.2621580362319946 \tgenerator_loss: 0.9507536292076111\n",
            "For Step: 11780 recon_loss: 0.17738744616508484 \tdiscriminator_loss: 1.1107691526412964 \tgenerator_loss: 1.0107799768447876\n",
            "For Step: 11790 recon_loss: 0.1710892915725708 \tdiscriminator_loss: 1.0632976293563843 \tgenerator_loss: 0.9743022918701172\n",
            "For Step: 11800 recon_loss: 0.17589524388313293 \tdiscriminator_loss: 1.080375075340271 \tgenerator_loss: 0.9928767681121826\n",
            "For Step: 11810 recon_loss: 0.16659244894981384 \tdiscriminator_loss: 1.0665830373764038 \tgenerator_loss: 1.0381399393081665\n",
            "For Step: 11820 recon_loss: 0.19211049377918243 \tdiscriminator_loss: 1.1482582092285156 \tgenerator_loss: 1.007138967514038\n",
            "For Step: 11830 recon_loss: 0.17993482947349548 \tdiscriminator_loss: 1.1382780075073242 \tgenerator_loss: 0.9726171493530273\n",
            "For Step: 11840 recon_loss: 0.17916153371334076 \tdiscriminator_loss: 1.1032911539077759 \tgenerator_loss: 1.0420641899108887\n",
            "For Step: 11850 recon_loss: 0.15513521432876587 \tdiscriminator_loss: 1.173398733139038 \tgenerator_loss: 0.9393563270568848\n",
            "For Step: 11860 recon_loss: 0.171325221657753 \tdiscriminator_loss: 1.150620698928833 \tgenerator_loss: 0.9916059970855713\n",
            "For Step: 11870 recon_loss: 0.18255336582660675 \tdiscriminator_loss: 1.3061861991882324 \tgenerator_loss: 1.0415239334106445\n",
            "For Step: 11880 recon_loss: 0.1801701933145523 \tdiscriminator_loss: 0.9938156604766846 \tgenerator_loss: 1.0653895139694214\n",
            "For Step: 11890 recon_loss: 0.17837268114089966 \tdiscriminator_loss: 1.1833631992340088 \tgenerator_loss: 0.9822302460670471\n",
            "For Step: 11900 recon_loss: 0.1946222335100174 \tdiscriminator_loss: 1.0376979112625122 \tgenerator_loss: 1.056626796722412\n",
            "For Step: 11910 recon_loss: 0.1727660894393921 \tdiscriminator_loss: 1.0305365324020386 \tgenerator_loss: 0.9754902124404907\n",
            "For Step: 11920 recon_loss: 0.17401570081710815 \tdiscriminator_loss: 1.105119228363037 \tgenerator_loss: 0.9690164923667908\n",
            "For Step: 11930 recon_loss: 0.1859920471906662 \tdiscriminator_loss: 1.1844035387039185 \tgenerator_loss: 0.9709732532501221\n",
            "For Step: 11940 recon_loss: 0.17170026898384094 \tdiscriminator_loss: 1.2160491943359375 \tgenerator_loss: 0.9944273829460144\n",
            "For Step: 11950 recon_loss: 0.18300296366214752 \tdiscriminator_loss: 1.051802158355713 \tgenerator_loss: 1.0440764427185059\n",
            "For Step: 11960 recon_loss: 0.18273966014385223 \tdiscriminator_loss: 1.1111102104187012 \tgenerator_loss: 1.0432300567626953\n",
            "For Step: 11970 recon_loss: 0.17523108422756195 \tdiscriminator_loss: 1.0736886262893677 \tgenerator_loss: 1.086020588874817\n",
            "For Step: 11980 recon_loss: 0.17389273643493652 \tdiscriminator_loss: 1.2336320877075195 \tgenerator_loss: 0.9919141530990601\n",
            "For Step: 11990 recon_loss: 0.18542449176311493 \tdiscriminator_loss: 1.1929328441619873 \tgenerator_loss: 0.9578312635421753\n",
            "For Step: 12000 recon_loss: 0.17336052656173706 \tdiscriminator_loss: 1.1742178201675415 \tgenerator_loss: 1.0322000980377197\n",
            "For Step: 12010 recon_loss: 0.18431314826011658 \tdiscriminator_loss: 1.1405538320541382 \tgenerator_loss: 0.9706324338912964\n",
            "For Step: 12020 recon_loss: 0.1803043782711029 \tdiscriminator_loss: 1.0074785947799683 \tgenerator_loss: 1.0766527652740479\n",
            "For Step: 12030 recon_loss: 0.1797727793455124 \tdiscriminator_loss: 1.1417264938354492 \tgenerator_loss: 0.9938008189201355\n",
            "For Step: 12040 recon_loss: 0.1694919764995575 \tdiscriminator_loss: 1.2010576725006104 \tgenerator_loss: 1.047579288482666\n",
            "For Step: 12050 recon_loss: 0.18463866412639618 \tdiscriminator_loss: 1.2266216278076172 \tgenerator_loss: 1.0351574420928955\n",
            "For Step: 12060 recon_loss: 0.17435190081596375 \tdiscriminator_loss: 1.194267749786377 \tgenerator_loss: 1.000327706336975\n",
            "For Step: 12070 recon_loss: 0.16918057203292847 \tdiscriminator_loss: 1.0977654457092285 \tgenerator_loss: 1.0260381698608398\n",
            "For Step: 12080 recon_loss: 0.1796497255563736 \tdiscriminator_loss: 1.1579118967056274 \tgenerator_loss: 1.1100077629089355\n",
            "For Step: 12090 recon_loss: 0.18247681856155396 \tdiscriminator_loss: 1.0415828227996826 \tgenerator_loss: 1.055828332901001\n",
            "For Step: 12100 recon_loss: 0.18416987359523773 \tdiscriminator_loss: 1.104958415031433 \tgenerator_loss: 1.0206937789916992\n",
            "For Step: 12110 recon_loss: 0.18927033245563507 \tdiscriminator_loss: 1.1808439493179321 \tgenerator_loss: 1.063970685005188\n",
            "For Step: 12120 recon_loss: 0.17699839174747467 \tdiscriminator_loss: 1.0981897115707397 \tgenerator_loss: 1.0553860664367676\n",
            "For Step: 12130 recon_loss: 0.1938079297542572 \tdiscriminator_loss: 1.0734949111938477 \tgenerator_loss: 1.0302914381027222\n",
            "For Step: 12140 recon_loss: 0.18781456351280212 \tdiscriminator_loss: 1.2064193487167358 \tgenerator_loss: 1.0012152194976807\n",
            "For Step: 12150 recon_loss: 0.17706356942653656 \tdiscriminator_loss: 1.1916675567626953 \tgenerator_loss: 0.9905765652656555\n",
            "For Step: 12160 recon_loss: 0.17008458077907562 \tdiscriminator_loss: 1.061362862586975 \tgenerator_loss: 1.0052189826965332\n",
            "For Step: 12170 recon_loss: 0.18310469388961792 \tdiscriminator_loss: 1.0906174182891846 \tgenerator_loss: 1.0556223392486572\n",
            "For Step: 12180 recon_loss: 0.1775755137205124 \tdiscriminator_loss: 1.0999072790145874 \tgenerator_loss: 1.0201740264892578\n",
            "For Step: 12190 recon_loss: 0.20002155005931854 \tdiscriminator_loss: 1.2923316955566406 \tgenerator_loss: 0.944355845451355\n",
            "For Step: 12200 recon_loss: 0.16949236392974854 \tdiscriminator_loss: 1.165750503540039 \tgenerator_loss: 1.01948881149292\n",
            "For Step: 12210 recon_loss: 0.16168738901615143 \tdiscriminator_loss: 1.0570331811904907 \tgenerator_loss: 1.0110732316970825\n",
            "For Step: 12220 recon_loss: 0.18735496699810028 \tdiscriminator_loss: 1.1309294700622559 \tgenerator_loss: 1.0827832221984863\n",
            "For Step: 12230 recon_loss: 0.17891018092632294 \tdiscriminator_loss: 1.1660929918289185 \tgenerator_loss: 1.00540292263031\n",
            "For Step: 12240 recon_loss: 0.1846989244222641 \tdiscriminator_loss: 1.07023024559021 \tgenerator_loss: 1.0076873302459717\n",
            "For Step: 12250 recon_loss: 0.1789037138223648 \tdiscriminator_loss: 1.2237775325775146 \tgenerator_loss: 0.9006679058074951\n",
            "For Step: 12260 recon_loss: 0.17942039668560028 \tdiscriminator_loss: 1.079628348350525 \tgenerator_loss: 1.0627894401550293\n",
            "For Step: 12270 recon_loss: 0.1955333799123764 \tdiscriminator_loss: 1.0605099201202393 \tgenerator_loss: 1.0424697399139404\n",
            "For Step: 12280 recon_loss: 0.17373812198638916 \tdiscriminator_loss: 1.1044707298278809 \tgenerator_loss: 1.0495712757110596\n",
            "For Step: 12290 recon_loss: 0.1751692295074463 \tdiscriminator_loss: 1.119246482849121 \tgenerator_loss: 1.0012822151184082\n",
            "For Step: 12300 recon_loss: 0.16804289817810059 \tdiscriminator_loss: 1.2032305002212524 \tgenerator_loss: 0.9666157960891724\n",
            "For Step: 12310 recon_loss: 0.1817973405122757 \tdiscriminator_loss: 1.1374261379241943 \tgenerator_loss: 1.0641016960144043\n",
            "For Step: 12320 recon_loss: 0.17642104625701904 \tdiscriminator_loss: 1.2112928628921509 \tgenerator_loss: 0.97988361120224\n",
            "For Step: 12330 recon_loss: 0.16512906551361084 \tdiscriminator_loss: 1.1545684337615967 \tgenerator_loss: 0.9627959728240967\n",
            "For Step: 12340 recon_loss: 0.17100754380226135 \tdiscriminator_loss: 1.0918545722961426 \tgenerator_loss: 1.0577735900878906\n",
            "For Step: 12350 recon_loss: 0.17005060613155365 \tdiscriminator_loss: 1.1899975538253784 \tgenerator_loss: 1.0178494453430176\n",
            "For Step: 12360 recon_loss: 0.17986918985843658 \tdiscriminator_loss: 1.071664810180664 \tgenerator_loss: 0.9957081079483032\n",
            "For Step: 12370 recon_loss: 0.1735815703868866 \tdiscriminator_loss: 1.0991621017456055 \tgenerator_loss: 0.9405558705329895\n",
            "For Step: 12380 recon_loss: 0.1911836713552475 \tdiscriminator_loss: 1.0359394550323486 \tgenerator_loss: 0.9227133989334106\n",
            "For Step: 12390 recon_loss: 0.16668179631233215 \tdiscriminator_loss: 1.1651055812835693 \tgenerator_loss: 0.9874687194824219\n",
            "For Step: 12400 recon_loss: 0.1734638214111328 \tdiscriminator_loss: 1.0935757160186768 \tgenerator_loss: 1.0915395021438599\n",
            "For Step: 12410 recon_loss: 0.18395178020000458 \tdiscriminator_loss: 1.1302685737609863 \tgenerator_loss: 0.977820873260498\n",
            "For Step: 12420 recon_loss: 0.17489942908287048 \tdiscriminator_loss: 1.1214745044708252 \tgenerator_loss: 1.0841513872146606\n",
            "For Step: 12430 recon_loss: 0.18795126676559448 \tdiscriminator_loss: 1.2357655763626099 \tgenerator_loss: 0.9921648502349854\n",
            "For Step: 12440 recon_loss: 0.17744699120521545 \tdiscriminator_loss: 1.1126806735992432 \tgenerator_loss: 1.0765643119812012\n",
            "For Step: 12450 recon_loss: 0.17992134392261505 \tdiscriminator_loss: 1.1437106132507324 \tgenerator_loss: 1.0858993530273438\n",
            "For Step: 12460 recon_loss: 0.1828356385231018 \tdiscriminator_loss: 1.025087833404541 \tgenerator_loss: 1.022621989250183\n",
            "For Step: 12470 recon_loss: 0.1911381632089615 \tdiscriminator_loss: 1.094191074371338 \tgenerator_loss: 1.041759729385376\n",
            "For Step: 12480 recon_loss: 0.17615549266338348 \tdiscriminator_loss: 1.1942616701126099 \tgenerator_loss: 0.9594742059707642\n",
            "For Step: 12490 recon_loss: 0.15226733684539795 \tdiscriminator_loss: 1.2147960662841797 \tgenerator_loss: 0.9260811805725098\n",
            "For Step: 12500 recon_loss: 0.17376649379730225 \tdiscriminator_loss: 1.19847571849823 \tgenerator_loss: 0.9615737199783325\n",
            "For Step: 12510 recon_loss: 0.17161279916763306 \tdiscriminator_loss: 1.093494176864624 \tgenerator_loss: 1.0259183645248413\n",
            "For Step: 12520 recon_loss: 0.19278603792190552 \tdiscriminator_loss: 1.1573224067687988 \tgenerator_loss: 1.0206435918807983\n",
            "For Step: 12530 recon_loss: 0.16927459836006165 \tdiscriminator_loss: 1.1284666061401367 \tgenerator_loss: 1.0916979312896729\n",
            "For Step: 12540 recon_loss: 0.19312770664691925 \tdiscriminator_loss: 1.052380084991455 \tgenerator_loss: 0.9851738810539246\n",
            "For Step: 12550 recon_loss: 0.18116246163845062 \tdiscriminator_loss: 1.1179585456848145 \tgenerator_loss: 1.021851897239685\n",
            "For Step: 12560 recon_loss: 0.1678403913974762 \tdiscriminator_loss: 1.0718294382095337 \tgenerator_loss: 0.9834791421890259\n",
            "For Step: 12570 recon_loss: 0.17658846080303192 \tdiscriminator_loss: 1.117595911026001 \tgenerator_loss: 1.0318310260772705\n",
            "For Step: 12580 recon_loss: 0.1949365735054016 \tdiscriminator_loss: 1.1493444442749023 \tgenerator_loss: 1.0283112525939941\n",
            "For Step: 12590 recon_loss: 0.1829831600189209 \tdiscriminator_loss: 1.1077802181243896 \tgenerator_loss: 1.010735034942627\n",
            "For Step: 12600 recon_loss: 0.17283108830451965 \tdiscriminator_loss: 1.1139404773712158 \tgenerator_loss: 0.9113064408302307\n",
            "For Step: 12610 recon_loss: 0.17199164628982544 \tdiscriminator_loss: 1.0411688089370728 \tgenerator_loss: 1.0528922080993652\n",
            "For Step: 12620 recon_loss: 0.17573361098766327 \tdiscriminator_loss: 1.0910155773162842 \tgenerator_loss: 1.0356990098953247\n",
            "For Step: 12630 recon_loss: 0.18460272252559662 \tdiscriminator_loss: 1.1490881443023682 \tgenerator_loss: 1.031326174736023\n",
            "For Step: 12640 recon_loss: 0.17444927990436554 \tdiscriminator_loss: 1.1092300415039062 \tgenerator_loss: 1.0367321968078613\n",
            "For Step: 12650 recon_loss: 0.1732712984085083 \tdiscriminator_loss: 1.2020783424377441 \tgenerator_loss: 1.0066009759902954\n",
            "For Step: 12660 recon_loss: 0.16512100398540497 \tdiscriminator_loss: 1.25978684425354 \tgenerator_loss: 0.9109777212142944\n",
            "For Step: 12670 recon_loss: 0.1752571314573288 \tdiscriminator_loss: 1.1739964485168457 \tgenerator_loss: 0.9219951629638672\n",
            "For Step: 12680 recon_loss: 0.17738427221775055 \tdiscriminator_loss: 1.2108514308929443 \tgenerator_loss: 1.0133875608444214\n",
            "For Step: 12690 recon_loss: 0.18062858283519745 \tdiscriminator_loss: 1.1490898132324219 \tgenerator_loss: 1.0631660223007202\n",
            "For Step: 12700 recon_loss: 0.1801019161939621 \tdiscriminator_loss: 1.0853549242019653 \tgenerator_loss: 0.9684192538261414\n",
            "For Step: 12710 recon_loss: 0.17022927105426788 \tdiscriminator_loss: 1.1809381246566772 \tgenerator_loss: 0.9187760353088379\n",
            "For Step: 12720 recon_loss: 0.16984279453754425 \tdiscriminator_loss: 1.154680848121643 \tgenerator_loss: 1.019071340560913\n",
            "For Step: 12730 recon_loss: 0.16022585332393646 \tdiscriminator_loss: 1.1867413520812988 \tgenerator_loss: 0.9717229604721069\n",
            "For Step: 12740 recon_loss: 0.18400603532791138 \tdiscriminator_loss: 1.198315143585205 \tgenerator_loss: 1.0007297992706299\n",
            "For Step: 12750 recon_loss: 0.17401014268398285 \tdiscriminator_loss: 1.056731939315796 \tgenerator_loss: 1.088710069656372\n",
            "For Step: 12760 recon_loss: 0.17603306472301483 \tdiscriminator_loss: 1.0860859155654907 \tgenerator_loss: 1.0261728763580322\n",
            "For Step: 12770 recon_loss: 0.17333538830280304 \tdiscriminator_loss: 1.1364142894744873 \tgenerator_loss: 1.0274221897125244\n",
            "For Step: 12780 recon_loss: 0.1760040819644928 \tdiscriminator_loss: 1.2068426609039307 \tgenerator_loss: 1.005313754081726\n",
            "For Step: 12790 recon_loss: 0.18474316596984863 \tdiscriminator_loss: 1.182976245880127 \tgenerator_loss: 0.9196681976318359\n",
            "For Step: 12800 recon_loss: 0.18837668001651764 \tdiscriminator_loss: 1.064319133758545 \tgenerator_loss: 1.014971375465393\n",
            "For Step: 12810 recon_loss: 0.17278200387954712 \tdiscriminator_loss: 1.165949821472168 \tgenerator_loss: 1.0105341672897339\n",
            "For Step: 12820 recon_loss: 0.1809782087802887 \tdiscriminator_loss: 0.9564396142959595 \tgenerator_loss: 1.1019740104675293\n",
            "For Step: 12830 recon_loss: 0.1694059520959854 \tdiscriminator_loss: 1.333329439163208 \tgenerator_loss: 0.9636245965957642\n",
            "For Step: 12840 recon_loss: 0.17837989330291748 \tdiscriminator_loss: 1.073273777961731 \tgenerator_loss: 0.9605961441993713\n",
            "For Step: 12850 recon_loss: 0.18702727556228638 \tdiscriminator_loss: 1.0520243644714355 \tgenerator_loss: 1.0741231441497803\n",
            "For Step: 12860 recon_loss: 0.18174941837787628 \tdiscriminator_loss: 1.1995525360107422 \tgenerator_loss: 0.9905919432640076\n",
            "For Step: 12870 recon_loss: 0.17159755527973175 \tdiscriminator_loss: 1.1394989490509033 \tgenerator_loss: 1.0005958080291748\n",
            "For Step: 12880 recon_loss: 0.17691974341869354 \tdiscriminator_loss: 1.2167701721191406 \tgenerator_loss: 0.9485712051391602\n",
            "For Step: 12890 recon_loss: 0.18453794717788696 \tdiscriminator_loss: 1.0602478981018066 \tgenerator_loss: 1.0968899726867676\n",
            "For Step: 12900 recon_loss: 0.17969226837158203 \tdiscriminator_loss: 0.9713656902313232 \tgenerator_loss: 1.1015360355377197\n",
            "For Step: 12910 recon_loss: 0.18612314760684967 \tdiscriminator_loss: 1.0885865688323975 \tgenerator_loss: 1.0191181898117065\n",
            "For Step: 12920 recon_loss: 0.18306149542331696 \tdiscriminator_loss: 1.2900699377059937 \tgenerator_loss: 0.9298668503761292\n",
            "For Step: 12930 recon_loss: 0.18130946159362793 \tdiscriminator_loss: 1.1693484783172607 \tgenerator_loss: 1.0197985172271729\n",
            "For Step: 12940 recon_loss: 0.16872718930244446 \tdiscriminator_loss: 1.1051418781280518 \tgenerator_loss: 1.0100595951080322\n",
            "For Step: 12950 recon_loss: 0.1676659733057022 \tdiscriminator_loss: 1.1304585933685303 \tgenerator_loss: 0.9598506689071655\n",
            "For Step: 12960 recon_loss: 0.17393697798252106 \tdiscriminator_loss: 1.1159111261367798 \tgenerator_loss: 1.0660946369171143\n",
            "For Step: 12970 recon_loss: 0.18436090648174286 \tdiscriminator_loss: 1.1427942514419556 \tgenerator_loss: 0.9031264185905457\n",
            "For Step: 12980 recon_loss: 0.1741182506084442 \tdiscriminator_loss: 1.255046010017395 \tgenerator_loss: 1.0139687061309814\n",
            "For Step: 12990 recon_loss: 0.16756591200828552 \tdiscriminator_loss: 1.1769074201583862 \tgenerator_loss: 0.9630900621414185\n",
            "For Step: 13000 recon_loss: 0.15866030752658844 \tdiscriminator_loss: 1.1326032876968384 \tgenerator_loss: 0.9983575344085693\n",
            "For Step: 13010 recon_loss: 0.16954797506332397 \tdiscriminator_loss: 1.1339497566223145 \tgenerator_loss: 1.0566084384918213\n",
            "For Step: 13020 recon_loss: 0.16650894284248352 \tdiscriminator_loss: 1.1846704483032227 \tgenerator_loss: 0.9797526597976685\n",
            "For Step: 13030 recon_loss: 0.17038103938102722 \tdiscriminator_loss: 1.005117654800415 \tgenerator_loss: 1.0314006805419922\n",
            "For Step: 13040 recon_loss: 0.18767273426055908 \tdiscriminator_loss: 1.0507880449295044 \tgenerator_loss: 0.9917069673538208\n",
            "For Step: 13050 recon_loss: 0.1872878521680832 \tdiscriminator_loss: 1.0862339735031128 \tgenerator_loss: 1.0635099411010742\n",
            "For Step: 13060 recon_loss: 0.1753440648317337 \tdiscriminator_loss: 0.9959262609481812 \tgenerator_loss: 1.049086332321167\n",
            "For Step: 13070 recon_loss: 0.17991763353347778 \tdiscriminator_loss: 1.0954968929290771 \tgenerator_loss: 1.0823427438735962\n",
            "For Step: 13080 recon_loss: 0.1843099743127823 \tdiscriminator_loss: 1.1882731914520264 \tgenerator_loss: 0.9598455429077148\n",
            "For Step: 13090 recon_loss: 0.1601271778345108 \tdiscriminator_loss: 1.1219277381896973 \tgenerator_loss: 1.0004279613494873\n",
            "For Step: 13100 recon_loss: 0.18920892477035522 \tdiscriminator_loss: 1.0189645290374756 \tgenerator_loss: 1.0815094709396362\n",
            "For Step: 13110 recon_loss: 0.17594468593597412 \tdiscriminator_loss: 1.2336313724517822 \tgenerator_loss: 1.0274701118469238\n",
            "For Step: 13120 recon_loss: 0.17769096791744232 \tdiscriminator_loss: 1.0892674922943115 \tgenerator_loss: 1.0853865146636963\n",
            "For Step: 13130 recon_loss: 0.18609103560447693 \tdiscriminator_loss: 1.1252087354660034 \tgenerator_loss: 0.9461016654968262\n",
            "For Step: 13140 recon_loss: 0.17571748793125153 \tdiscriminator_loss: 1.2787822484970093 \tgenerator_loss: 0.9817684888839722\n",
            "For Step: 13150 recon_loss: 0.18327511847019196 \tdiscriminator_loss: 1.1387324333190918 \tgenerator_loss: 1.0233633518218994\n",
            "For Step: 13160 recon_loss: 0.18162167072296143 \tdiscriminator_loss: 1.1214826107025146 \tgenerator_loss: 0.9910100698471069\n",
            "For Step: 13170 recon_loss: 0.18051877617835999 \tdiscriminator_loss: 1.2583446502685547 \tgenerator_loss: 0.9837958812713623\n",
            "For Step: 13180 recon_loss: 0.17886535823345184 \tdiscriminator_loss: 1.196982741355896 \tgenerator_loss: 0.9987335205078125\n",
            "For Step: 13190 recon_loss: 0.17913803458213806 \tdiscriminator_loss: 1.083670973777771 \tgenerator_loss: 0.9607737064361572\n",
            "For Step: 13200 recon_loss: 0.1739446222782135 \tdiscriminator_loss: 1.1473591327667236 \tgenerator_loss: 0.9822006225585938\n",
            "For Step: 13210 recon_loss: 0.17288069427013397 \tdiscriminator_loss: 1.2579749822616577 \tgenerator_loss: 0.9437584280967712\n",
            "For Step: 13220 recon_loss: 0.17710725963115692 \tdiscriminator_loss: 1.2105693817138672 \tgenerator_loss: 0.9401476979255676\n",
            "For Step: 13230 recon_loss: 0.1810404360294342 \tdiscriminator_loss: 1.2646903991699219 \tgenerator_loss: 1.0036320686340332\n",
            "For Step: 13240 recon_loss: 0.17192357778549194 \tdiscriminator_loss: 1.1401762962341309 \tgenerator_loss: 0.9685919284820557\n",
            "For Step: 13250 recon_loss: 0.1958165317773819 \tdiscriminator_loss: 1.2102164030075073 \tgenerator_loss: 1.0508238077163696\n",
            "For Step: 13260 recon_loss: 0.18696071207523346 \tdiscriminator_loss: 1.1519979238510132 \tgenerator_loss: 1.0562288761138916\n",
            "For Step: 13270 recon_loss: 0.18261967599391937 \tdiscriminator_loss: 1.2609939575195312 \tgenerator_loss: 0.9795247912406921\n",
            "For Step: 13280 recon_loss: 0.18189531564712524 \tdiscriminator_loss: 1.1027295589447021 \tgenerator_loss: 1.0577564239501953\n",
            "For Step: 13290 recon_loss: 0.1889714002609253 \tdiscriminator_loss: 1.172436237335205 \tgenerator_loss: 0.9933416843414307\n",
            "For Step: 13300 recon_loss: 0.17130623757839203 \tdiscriminator_loss: 1.2353744506835938 \tgenerator_loss: 1.0494792461395264\n",
            "For Step: 13310 recon_loss: 0.18967242538928986 \tdiscriminator_loss: 1.0740101337432861 \tgenerator_loss: 1.0000381469726562\n",
            "For Step: 13320 recon_loss: 0.17203012108802795 \tdiscriminator_loss: 1.108215093612671 \tgenerator_loss: 0.9852388501167297\n",
            "For Step: 13330 recon_loss: 0.19029471278190613 \tdiscriminator_loss: 1.130268931388855 \tgenerator_loss: 0.9934994578361511\n",
            "For Step: 13340 recon_loss: 0.17183765769004822 \tdiscriminator_loss: 1.1382250785827637 \tgenerator_loss: 1.0124785900115967\n",
            "For Step: 13350 recon_loss: 0.1723712831735611 \tdiscriminator_loss: 1.0192654132843018 \tgenerator_loss: 0.9935524463653564\n",
            "For Step: 13360 recon_loss: 0.18036995828151703 \tdiscriminator_loss: 1.251183032989502 \tgenerator_loss: 0.966136634349823\n",
            "For Step: 13370 recon_loss: 0.1694241762161255 \tdiscriminator_loss: 1.295217514038086 \tgenerator_loss: 0.8901943564414978\n",
            "For Step: 13380 recon_loss: 0.18079331517219543 \tdiscriminator_loss: 1.2549816370010376 \tgenerator_loss: 1.0131580829620361\n",
            "For Step: 13390 recon_loss: 0.18148553371429443 \tdiscriminator_loss: 1.283475637435913 \tgenerator_loss: 0.9072684049606323\n",
            "For Step: 13400 recon_loss: 0.16969183087348938 \tdiscriminator_loss: 1.1144572496414185 \tgenerator_loss: 1.0141860246658325\n",
            "For Step: 13410 recon_loss: 0.1658245325088501 \tdiscriminator_loss: 1.2357912063598633 \tgenerator_loss: 0.9968881011009216\n",
            "For Step: 13420 recon_loss: 0.17638693749904633 \tdiscriminator_loss: 1.1317412853240967 \tgenerator_loss: 1.087766408920288\n",
            "For Step: 13430 recon_loss: 0.1754130721092224 \tdiscriminator_loss: 1.1548404693603516 \tgenerator_loss: 0.9657949209213257\n",
            "For Step: 13440 recon_loss: 0.17962950468063354 \tdiscriminator_loss: 1.150705099105835 \tgenerator_loss: 1.041719913482666\n",
            "For Step: 13450 recon_loss: 0.17397071421146393 \tdiscriminator_loss: 1.182983160018921 \tgenerator_loss: 0.9714372754096985\n",
            "For Step: 13460 recon_loss: 0.17286047339439392 \tdiscriminator_loss: 1.192163109779358 \tgenerator_loss: 1.0187325477600098\n",
            "For Step: 13470 recon_loss: 0.18018707633018494 \tdiscriminator_loss: 1.2514705657958984 \tgenerator_loss: 0.9442963600158691\n",
            "For Step: 13480 recon_loss: 0.157570943236351 \tdiscriminator_loss: 1.125441312789917 \tgenerator_loss: 0.9811512231826782\n",
            "For Step: 13490 recon_loss: 0.17725718021392822 \tdiscriminator_loss: 1.0879311561584473 \tgenerator_loss: 0.9632956385612488\n",
            "For Step: 13500 recon_loss: 0.19208191335201263 \tdiscriminator_loss: 1.0799912214279175 \tgenerator_loss: 0.9886182546615601\n",
            "For Step: 13510 recon_loss: 0.1739199459552765 \tdiscriminator_loss: 1.1020231246948242 \tgenerator_loss: 1.0048868656158447\n",
            "For Step: 13520 recon_loss: 0.17980866134166718 \tdiscriminator_loss: 1.1397706270217896 \tgenerator_loss: 0.9885038733482361\n",
            "For Step: 13530 recon_loss: 0.1742643564939499 \tdiscriminator_loss: 1.2316131591796875 \tgenerator_loss: 0.9657906293869019\n",
            "For Step: 13540 recon_loss: 0.17630036175251007 \tdiscriminator_loss: 1.149285078048706 \tgenerator_loss: 0.9415707588195801\n",
            "For Step: 13550 recon_loss: 0.17227864265441895 \tdiscriminator_loss: 1.0596985816955566 \tgenerator_loss: 0.9666452407836914\n",
            "For Step: 13560 recon_loss: 0.17590565979480743 \tdiscriminator_loss: 1.2272741794586182 \tgenerator_loss: 1.0742114782333374\n",
            "For Step: 13570 recon_loss: 0.1865086704492569 \tdiscriminator_loss: 1.115799903869629 \tgenerator_loss: 0.970283567905426\n",
            "For Step: 13580 recon_loss: 0.16519951820373535 \tdiscriminator_loss: 1.1994884014129639 \tgenerator_loss: 0.9636818170547485\n",
            "For Step: 13590 recon_loss: 0.18279621005058289 \tdiscriminator_loss: 1.1233078241348267 \tgenerator_loss: 1.0491483211517334\n",
            "For Step: 13600 recon_loss: 0.1811068207025528 \tdiscriminator_loss: 1.2085578441619873 \tgenerator_loss: 0.9488458633422852\n",
            "For Step: 13610 recon_loss: 0.1808968186378479 \tdiscriminator_loss: 1.0491875410079956 \tgenerator_loss: 1.0145463943481445\n",
            "For Step: 13620 recon_loss: 0.16797314584255219 \tdiscriminator_loss: 1.1654828786849976 \tgenerator_loss: 0.9657598733901978\n",
            "For Step: 13630 recon_loss: 0.17805778980255127 \tdiscriminator_loss: 1.170563817024231 \tgenerator_loss: 1.0076708793640137\n",
            "For Step: 13640 recon_loss: 0.16854381561279297 \tdiscriminator_loss: 1.2265980243682861 \tgenerator_loss: 1.0148322582244873\n",
            "For Step: 13650 recon_loss: 0.18129627406597137 \tdiscriminator_loss: 1.2568728923797607 \tgenerator_loss: 1.0319063663482666\n",
            "For Step: 13660 recon_loss: 0.18713277578353882 \tdiscriminator_loss: 1.1315581798553467 \tgenerator_loss: 1.0122534036636353\n",
            "For Step: 13670 recon_loss: 0.17073124647140503 \tdiscriminator_loss: 1.2070293426513672 \tgenerator_loss: 1.0853631496429443\n",
            "For Step: 13680 recon_loss: 0.17866115272045135 \tdiscriminator_loss: 1.2060635089874268 \tgenerator_loss: 0.9503469467163086\n",
            "For Step: 13690 recon_loss: 0.16786536574363708 \tdiscriminator_loss: 1.1553924083709717 \tgenerator_loss: 0.9971936345100403\n",
            "For Step: 13700 recon_loss: 0.16376353800296783 \tdiscriminator_loss: 1.1705411672592163 \tgenerator_loss: 1.0052404403686523\n",
            "For Step: 13710 recon_loss: 0.19145692884922028 \tdiscriminator_loss: 1.1523551940917969 \tgenerator_loss: 1.0289456844329834\n",
            "For Step: 13720 recon_loss: 0.17984509468078613 \tdiscriminator_loss: 1.163207769393921 \tgenerator_loss: 0.9577204585075378\n",
            "For Step: 13730 recon_loss: 0.172898530960083 \tdiscriminator_loss: 1.0988223552703857 \tgenerator_loss: 0.9875785112380981\n",
            "For Step: 13740 recon_loss: 0.17290419340133667 \tdiscriminator_loss: 1.1628117561340332 \tgenerator_loss: 1.007080078125\n",
            "For Step: 13750 recon_loss: 0.16311901807785034 \tdiscriminator_loss: 1.347403645515442 \tgenerator_loss: 0.9968636631965637\n",
            "For Step: 13760 recon_loss: 0.17005246877670288 \tdiscriminator_loss: 1.1913776397705078 \tgenerator_loss: 0.983323872089386\n",
            "For Step: 13770 recon_loss: 0.1917443424463272 \tdiscriminator_loss: 1.1704597473144531 \tgenerator_loss: 0.976624608039856\n",
            "For Step: 13780 recon_loss: 0.17834877967834473 \tdiscriminator_loss: 1.1603646278381348 \tgenerator_loss: 0.992550253868103\n",
            "For Step: 13790 recon_loss: 0.17719298601150513 \tdiscriminator_loss: 1.1842539310455322 \tgenerator_loss: 0.9879915118217468\n",
            "For Step: 13800 recon_loss: 0.1757338047027588 \tdiscriminator_loss: 1.0558253526687622 \tgenerator_loss: 1.0624604225158691\n",
            "For Step: 13810 recon_loss: 0.18365588784217834 \tdiscriminator_loss: 1.248896598815918 \tgenerator_loss: 1.062498688697815\n",
            "For Step: 13820 recon_loss: 0.18055717647075653 \tdiscriminator_loss: 1.2603833675384521 \tgenerator_loss: 0.9790642261505127\n",
            "For Step: 13830 recon_loss: 0.18228022754192352 \tdiscriminator_loss: 1.0085368156433105 \tgenerator_loss: 1.1130707263946533\n",
            "For Step: 13840 recon_loss: 0.1734994798898697 \tdiscriminator_loss: 1.0968571901321411 \tgenerator_loss: 1.0501854419708252\n",
            "For Step: 13850 recon_loss: 0.16269145905971527 \tdiscriminator_loss: 1.199248194694519 \tgenerator_loss: 0.9654439687728882\n",
            "For Step: 13860 recon_loss: 0.17786924540996552 \tdiscriminator_loss: 1.2333309650421143 \tgenerator_loss: 0.9899497032165527\n",
            "For Step: 13870 recon_loss: 0.1772298961877823 \tdiscriminator_loss: 1.1655476093292236 \tgenerator_loss: 0.9708740711212158\n",
            "For Step: 13880 recon_loss: 0.18941299617290497 \tdiscriminator_loss: 1.1708457469940186 \tgenerator_loss: 0.995269775390625\n",
            "For Step: 13890 recon_loss: 0.1795082837343216 \tdiscriminator_loss: 1.113218069076538 \tgenerator_loss: 1.027815580368042\n",
            "For Step: 13900 recon_loss: 0.1693117618560791 \tdiscriminator_loss: 1.1573100090026855 \tgenerator_loss: 0.927331805229187\n",
            "For Step: 13910 recon_loss: 0.17091050744056702 \tdiscriminator_loss: 1.253551959991455 \tgenerator_loss: 1.0206952095031738\n",
            "For Step: 13920 recon_loss: 0.17964549362659454 \tdiscriminator_loss: 1.1235792636871338 \tgenerator_loss: 0.995737612247467\n",
            "For Step: 13930 recon_loss: 0.1742614507675171 \tdiscriminator_loss: 1.258068561553955 \tgenerator_loss: 1.0413823127746582\n",
            "For Step: 13940 recon_loss: 0.18978655338287354 \tdiscriminator_loss: 1.1778788566589355 \tgenerator_loss: 0.9389796257019043\n",
            "For Step: 13950 recon_loss: 0.16062165796756744 \tdiscriminator_loss: 1.1589884757995605 \tgenerator_loss: 0.9758145809173584\n",
            "For Step: 13960 recon_loss: 0.17856185138225555 \tdiscriminator_loss: 1.1294077634811401 \tgenerator_loss: 1.001882553100586\n",
            "For Step: 13970 recon_loss: 0.17205345630645752 \tdiscriminator_loss: 1.061021327972412 \tgenerator_loss: 1.0194154977798462\n",
            "For Step: 13980 recon_loss: 0.19136366248130798 \tdiscriminator_loss: 1.0966672897338867 \tgenerator_loss: 0.9614863991737366\n",
            "For Step: 13990 recon_loss: 0.18886438012123108 \tdiscriminator_loss: 1.2046012878417969 \tgenerator_loss: 0.9610790014266968\n",
            "For Step: 14000 recon_loss: 0.1768520474433899 \tdiscriminator_loss: 1.1561386585235596 \tgenerator_loss: 0.9776577353477478\n",
            "For Step: 14010 recon_loss: 0.18442954123020172 \tdiscriminator_loss: 1.1766278743743896 \tgenerator_loss: 1.0251411199569702\n",
            "For Step: 14020 recon_loss: 0.176241934299469 \tdiscriminator_loss: 1.1685707569122314 \tgenerator_loss: 0.9774109125137329\n",
            "For Step: 14030 recon_loss: 0.17466315627098083 \tdiscriminator_loss: 1.2375893592834473 \tgenerator_loss: 0.9311301708221436\n",
            "For Step: 14040 recon_loss: 0.19555918872356415 \tdiscriminator_loss: 1.2739182710647583 \tgenerator_loss: 0.9488878846168518\n",
            "For Step: 14050 recon_loss: 0.18446104228496552 \tdiscriminator_loss: 1.0494871139526367 \tgenerator_loss: 1.0665286779403687\n",
            "For Step: 14060 recon_loss: 0.16920825839042664 \tdiscriminator_loss: 1.0972291231155396 \tgenerator_loss: 0.9873355627059937\n",
            "For Step: 14070 recon_loss: 0.17964549362659454 \tdiscriminator_loss: 1.35064697265625 \tgenerator_loss: 0.9308356642723083\n",
            "For Step: 14080 recon_loss: 0.18443608283996582 \tdiscriminator_loss: 1.1433675289154053 \tgenerator_loss: 0.9435638189315796\n",
            "For Step: 14090 recon_loss: 0.17029693722724915 \tdiscriminator_loss: 1.107133150100708 \tgenerator_loss: 0.9013521075248718\n",
            "For Step: 14100 recon_loss: 0.18539725244045258 \tdiscriminator_loss: 1.1758434772491455 \tgenerator_loss: 1.0026825666427612\n",
            "For Step: 14110 recon_loss: 0.18770942091941833 \tdiscriminator_loss: 1.1598703861236572 \tgenerator_loss: 0.9829487800598145\n",
            "For Step: 14120 recon_loss: 0.17578816413879395 \tdiscriminator_loss: 1.1174200773239136 \tgenerator_loss: 1.0817745923995972\n",
            "For Step: 14130 recon_loss: 0.17516930401325226 \tdiscriminator_loss: 1.117933988571167 \tgenerator_loss: 1.0047045946121216\n",
            "For Step: 14140 recon_loss: 0.1804288625717163 \tdiscriminator_loss: 1.1443078517913818 \tgenerator_loss: 1.03328537940979\n",
            "For Step: 14150 recon_loss: 0.1860859990119934 \tdiscriminator_loss: 1.1818009614944458 \tgenerator_loss: 0.9981833696365356\n",
            "For Step: 14160 recon_loss: 0.1806747317314148 \tdiscriminator_loss: 1.1362676620483398 \tgenerator_loss: 0.9312629699707031\n",
            "For Step: 14170 recon_loss: 0.1734158843755722 \tdiscriminator_loss: 1.1608936786651611 \tgenerator_loss: 1.0844969749450684\n",
            "For Step: 14180 recon_loss: 0.18019631505012512 \tdiscriminator_loss: 1.144431471824646 \tgenerator_loss: 0.9977008104324341\n",
            "For Step: 14190 recon_loss: 0.17254942655563354 \tdiscriminator_loss: 1.1692228317260742 \tgenerator_loss: 0.9132513403892517\n",
            "For Step: 14200 recon_loss: 0.18054884672164917 \tdiscriminator_loss: 1.0488766431808472 \tgenerator_loss: 1.0602058172225952\n",
            "For Step: 14210 recon_loss: 0.17190609872341156 \tdiscriminator_loss: 1.0525705814361572 \tgenerator_loss: 1.0615687370300293\n",
            "For Step: 14220 recon_loss: 0.1874951720237732 \tdiscriminator_loss: 1.2747336626052856 \tgenerator_loss: 0.9767647981643677\n",
            "For Step: 14230 recon_loss: 0.18127451837062836 \tdiscriminator_loss: 1.1516876220703125 \tgenerator_loss: 1.0143389701843262\n",
            "For Step: 14240 recon_loss: 0.18025536835193634 \tdiscriminator_loss: 1.1112358570098877 \tgenerator_loss: 0.899937629699707\n",
            "For Step: 14250 recon_loss: 0.17709119617938995 \tdiscriminator_loss: 1.232520341873169 \tgenerator_loss: 0.9437592625617981\n",
            "For Step: 14260 recon_loss: 0.18097056448459625 \tdiscriminator_loss: 1.1662440299987793 \tgenerator_loss: 0.9331285953521729\n",
            "For Step: 14270 recon_loss: 0.17412972450256348 \tdiscriminator_loss: 1.1106092929840088 \tgenerator_loss: 1.001340627670288\n",
            "For Step: 14280 recon_loss: 0.17701992392539978 \tdiscriminator_loss: 1.2174015045166016 \tgenerator_loss: 1.0266252756118774\n",
            "For Step: 14290 recon_loss: 0.18472301959991455 \tdiscriminator_loss: 1.2775958776474 \tgenerator_loss: 0.8853092789649963\n",
            "For Step: 14300 recon_loss: 0.1787346601486206 \tdiscriminator_loss: 1.1202740669250488 \tgenerator_loss: 0.9432981014251709\n",
            "For Step: 14310 recon_loss: 0.1671842336654663 \tdiscriminator_loss: 1.1692508459091187 \tgenerator_loss: 0.9410074949264526\n",
            "For Step: 14320 recon_loss: 0.16762053966522217 \tdiscriminator_loss: 1.1861598491668701 \tgenerator_loss: 0.994805097579956\n",
            "For Step: 14330 recon_loss: 0.1725517362356186 \tdiscriminator_loss: 1.263169288635254 \tgenerator_loss: 1.0337656736373901\n",
            "For Step: 14340 recon_loss: 0.17576929926872253 \tdiscriminator_loss: 1.192732334136963 \tgenerator_loss: 1.0250422954559326\n",
            "For Step: 14350 recon_loss: 0.1832278072834015 \tdiscriminator_loss: 1.09934401512146 \tgenerator_loss: 1.0186899900436401\n",
            "For Step: 14360 recon_loss: 0.18613092601299286 \tdiscriminator_loss: 1.2645444869995117 \tgenerator_loss: 0.9044285416603088\n",
            "For Step: 14370 recon_loss: 0.17632994055747986 \tdiscriminator_loss: 1.1744117736816406 \tgenerator_loss: 0.9978185892105103\n",
            "For Step: 14380 recon_loss: 0.18604493141174316 \tdiscriminator_loss: 1.2983825206756592 \tgenerator_loss: 0.935150146484375\n",
            "For Step: 14390 recon_loss: 0.17441745102405548 \tdiscriminator_loss: 1.2375712394714355 \tgenerator_loss: 0.9431852102279663\n",
            "For Step: 14400 recon_loss: 0.18408825993537903 \tdiscriminator_loss: 1.2325928211212158 \tgenerator_loss: 0.9563542604446411\n",
            "For Step: 14410 recon_loss: 0.16258257627487183 \tdiscriminator_loss: 1.1251838207244873 \tgenerator_loss: 0.9920991659164429\n",
            "For Step: 14420 recon_loss: 0.178536519408226 \tdiscriminator_loss: 1.0720834732055664 \tgenerator_loss: 1.001490592956543\n",
            "For Step: 14430 recon_loss: 0.1817362904548645 \tdiscriminator_loss: 1.0486315488815308 \tgenerator_loss: 1.0450427532196045\n",
            "For Step: 14440 recon_loss: 0.1824210286140442 \tdiscriminator_loss: 1.107279896736145 \tgenerator_loss: 1.0659594535827637\n",
            "For Step: 14450 recon_loss: 0.16585229337215424 \tdiscriminator_loss: 1.1878409385681152 \tgenerator_loss: 0.9602137207984924\n",
            "For Step: 14460 recon_loss: 0.17419107258319855 \tdiscriminator_loss: 1.3202126026153564 \tgenerator_loss: 0.8849748969078064\n",
            "For Step: 14470 recon_loss: 0.17535176873207092 \tdiscriminator_loss: 1.2842841148376465 \tgenerator_loss: 0.9188123941421509\n",
            "For Step: 14480 recon_loss: 0.18110255897045135 \tdiscriminator_loss: 1.3168056011199951 \tgenerator_loss: 0.9774302244186401\n",
            "For Step: 14490 recon_loss: 0.16640013456344604 \tdiscriminator_loss: 1.028195858001709 \tgenerator_loss: 0.9875802993774414\n",
            "For Step: 14500 recon_loss: 0.1837884932756424 \tdiscriminator_loss: 1.075218677520752 \tgenerator_loss: 1.0357427597045898\n",
            "For Step: 14510 recon_loss: 0.17680083215236664 \tdiscriminator_loss: 1.2361207008361816 \tgenerator_loss: 0.8701653480529785\n",
            "For Step: 14520 recon_loss: 0.1771833449602127 \tdiscriminator_loss: 1.2371864318847656 \tgenerator_loss: 0.9642753005027771\n",
            "For Step: 14530 recon_loss: 0.18602707982063293 \tdiscriminator_loss: 1.2118825912475586 \tgenerator_loss: 0.9653685092926025\n",
            "For Step: 14540 recon_loss: 0.1751345843076706 \tdiscriminator_loss: 1.1170017719268799 \tgenerator_loss: 1.0026429891586304\n",
            "For Step: 14550 recon_loss: 0.18001481890678406 \tdiscriminator_loss: 1.2573938369750977 \tgenerator_loss: 0.9187888503074646\n",
            "For Step: 14560 recon_loss: 0.17482292652130127 \tdiscriminator_loss: 1.1462275981903076 \tgenerator_loss: 0.9863336682319641\n",
            "For Step: 14570 recon_loss: 0.17958037555217743 \tdiscriminator_loss: 1.0370612144470215 \tgenerator_loss: 0.9681696891784668\n",
            "For Step: 14580 recon_loss: 0.1698538362979889 \tdiscriminator_loss: 1.244240641593933 \tgenerator_loss: 1.0294170379638672\n",
            "For Step: 14590 recon_loss: 0.17676930129528046 \tdiscriminator_loss: 1.1579234600067139 \tgenerator_loss: 1.0356402397155762\n",
            "For Step: 14600 recon_loss: 0.1850675493478775 \tdiscriminator_loss: 1.1307284832000732 \tgenerator_loss: 1.023603916168213\n",
            "For Step: 14610 recon_loss: 0.17421232163906097 \tdiscriminator_loss: 1.2633991241455078 \tgenerator_loss: 1.0393213033676147\n",
            "For Step: 14620 recon_loss: 0.17313525080680847 \tdiscriminator_loss: 1.253885269165039 \tgenerator_loss: 0.9684934616088867\n",
            "For Step: 14630 recon_loss: 0.17944659292697906 \tdiscriminator_loss: 1.206470012664795 \tgenerator_loss: 1.0006394386291504\n",
            "For Step: 14640 recon_loss: 0.17679016292095184 \tdiscriminator_loss: 1.222265601158142 \tgenerator_loss: 0.9627838134765625\n",
            "For Step: 14650 recon_loss: 0.18405680358409882 \tdiscriminator_loss: 1.170823335647583 \tgenerator_loss: 0.9471982717514038\n",
            "For Step: 14660 recon_loss: 0.17400991916656494 \tdiscriminator_loss: 1.185248851776123 \tgenerator_loss: 0.9861457347869873\n",
            "For Step: 14670 recon_loss: 0.18090325593948364 \tdiscriminator_loss: 1.3058602809906006 \tgenerator_loss: 0.952281653881073\n",
            "For Step: 14680 recon_loss: 0.17138883471488953 \tdiscriminator_loss: 0.9955859780311584 \tgenerator_loss: 1.031485915184021\n",
            "For Step: 14690 recon_loss: 0.18755240738391876 \tdiscriminator_loss: 1.0896352529525757 \tgenerator_loss: 0.9861955046653748\n",
            "For Step: 14700 recon_loss: 0.17948508262634277 \tdiscriminator_loss: 1.151617169380188 \tgenerator_loss: 0.9360134601593018\n",
            "For Step: 14710 recon_loss: 0.18464745581150055 \tdiscriminator_loss: 1.213539958000183 \tgenerator_loss: 1.0887011289596558\n",
            "For Step: 14720 recon_loss: 0.15467123687267303 \tdiscriminator_loss: 1.2477078437805176 \tgenerator_loss: 0.9082491397857666\n",
            "For Step: 14730 recon_loss: 0.1740409880876541 \tdiscriminator_loss: 1.108597755432129 \tgenerator_loss: 1.0987944602966309\n",
            "For Step: 14740 recon_loss: 0.18136557936668396 \tdiscriminator_loss: 1.2235331535339355 \tgenerator_loss: 1.090766429901123\n",
            "For Step: 14750 recon_loss: 0.16722598671913147 \tdiscriminator_loss: 1.1875715255737305 \tgenerator_loss: 0.9178913831710815\n",
            "For Step: 14760 recon_loss: 0.1804581880569458 \tdiscriminator_loss: 1.092725157737732 \tgenerator_loss: 1.0706950426101685\n",
            "For Step: 14770 recon_loss: 0.1719464808702469 \tdiscriminator_loss: 1.1408506631851196 \tgenerator_loss: 1.0315109491348267\n",
            "For Step: 14780 recon_loss: 0.17111359536647797 \tdiscriminator_loss: 1.0919206142425537 \tgenerator_loss: 0.959575891494751\n",
            "For Step: 14790 recon_loss: 0.17624446749687195 \tdiscriminator_loss: 1.1991357803344727 \tgenerator_loss: 0.9733742475509644\n",
            "For Step: 14800 recon_loss: 0.1731407791376114 \tdiscriminator_loss: 1.256365180015564 \tgenerator_loss: 0.9186601042747498\n",
            "For Step: 14810 recon_loss: 0.1717158854007721 \tdiscriminator_loss: 1.1746610403060913 \tgenerator_loss: 0.9253441095352173\n",
            "For Step: 14820 recon_loss: 0.1837240755558014 \tdiscriminator_loss: 1.0758962631225586 \tgenerator_loss: 1.0400443077087402\n",
            "For Step: 14830 recon_loss: 0.171855166554451 \tdiscriminator_loss: 1.0375785827636719 \tgenerator_loss: 1.0095922946929932\n",
            "For Step: 14840 recon_loss: 0.17791946232318878 \tdiscriminator_loss: 1.1778950691223145 \tgenerator_loss: 0.9455549716949463\n",
            "For Step: 14850 recon_loss: 0.1848772019147873 \tdiscriminator_loss: 1.3340423107147217 \tgenerator_loss: 0.9823253154754639\n",
            "For Step: 14860 recon_loss: 0.16818998754024506 \tdiscriminator_loss: 1.1046584844589233 \tgenerator_loss: 0.9225035309791565\n",
            "For Step: 14870 recon_loss: 0.17196263372898102 \tdiscriminator_loss: 1.0501340627670288 \tgenerator_loss: 1.0356268882751465\n",
            "For Step: 14880 recon_loss: 0.18145018815994263 \tdiscriminator_loss: 1.1309252977371216 \tgenerator_loss: 1.0108449459075928\n",
            "For Step: 14890 recon_loss: 0.17646482586860657 \tdiscriminator_loss: 1.1473581790924072 \tgenerator_loss: 0.9939730167388916\n",
            "For Step: 14900 recon_loss: 0.17409662902355194 \tdiscriminator_loss: 1.2503678798675537 \tgenerator_loss: 0.9464073181152344\n",
            "For Step: 14910 recon_loss: 0.17346924543380737 \tdiscriminator_loss: 1.19350004196167 \tgenerator_loss: 0.9989252090454102\n",
            "For Step: 14920 recon_loss: 0.17582999169826508 \tdiscriminator_loss: 1.1642744541168213 \tgenerator_loss: 0.9980398416519165\n",
            "For Step: 14930 recon_loss: 0.18842244148254395 \tdiscriminator_loss: 1.146411657333374 \tgenerator_loss: 0.9960973858833313\n",
            "For Step: 14940 recon_loss: 0.18479812145233154 \tdiscriminator_loss: 1.2465901374816895 \tgenerator_loss: 1.0155729055404663\n",
            "For Step: 14950 recon_loss: 0.18153907358646393 \tdiscriminator_loss: 1.1396794319152832 \tgenerator_loss: 1.0980734825134277\n",
            "For Step: 14960 recon_loss: 0.16595859825611115 \tdiscriminator_loss: 1.2155263423919678 \tgenerator_loss: 0.863215982913971\n",
            "For Step: 14970 recon_loss: 0.16841863095760345 \tdiscriminator_loss: 1.117081642150879 \tgenerator_loss: 0.9968987703323364\n",
            "For Step: 14980 recon_loss: 0.17833946645259857 \tdiscriminator_loss: 1.2431199550628662 \tgenerator_loss: 1.0574779510498047\n",
            "For Step: 14990 recon_loss: 0.1785157024860382 \tdiscriminator_loss: 1.2731177806854248 \tgenerator_loss: 1.0298008918762207\n",
            "For Step: 15000 recon_loss: 0.17835566401481628 \tdiscriminator_loss: 1.159385323524475 \tgenerator_loss: 1.0310304164886475\n",
            "For Step: 15010 recon_loss: 0.18356595933437347 \tdiscriminator_loss: 1.2573513984680176 \tgenerator_loss: 0.9643287658691406\n",
            "For Step: 15020 recon_loss: 0.18693065643310547 \tdiscriminator_loss: 1.1270055770874023 \tgenerator_loss: 1.0532445907592773\n",
            "For Step: 15030 recon_loss: 0.17324034869670868 \tdiscriminator_loss: 1.1577370166778564 \tgenerator_loss: 1.0337162017822266\n",
            "For Step: 15040 recon_loss: 0.170265331864357 \tdiscriminator_loss: 1.22232985496521 \tgenerator_loss: 0.955712616443634\n",
            "For Step: 15050 recon_loss: 0.18481360375881195 \tdiscriminator_loss: 1.0383379459381104 \tgenerator_loss: 0.9712797403335571\n",
            "For Step: 15060 recon_loss: 0.16261407732963562 \tdiscriminator_loss: 1.1026440858840942 \tgenerator_loss: 0.933222770690918\n",
            "For Step: 15070 recon_loss: 0.1643703579902649 \tdiscriminator_loss: 1.0613017082214355 \tgenerator_loss: 1.036421298980713\n",
            "For Step: 15080 recon_loss: 0.17737282812595367 \tdiscriminator_loss: 1.0631812810897827 \tgenerator_loss: 0.9514479637145996\n",
            "For Step: 15090 recon_loss: 0.1875002384185791 \tdiscriminator_loss: 1.248244285583496 \tgenerator_loss: 0.9025822877883911\n",
            "For Step: 15100 recon_loss: 0.17357996106147766 \tdiscriminator_loss: 1.131032943725586 \tgenerator_loss: 1.0131207704544067\n",
            "For Step: 15110 recon_loss: 0.18545524775981903 \tdiscriminator_loss: 1.2551512718200684 \tgenerator_loss: 1.0299456119537354\n",
            "For Step: 15120 recon_loss: 0.18115825951099396 \tdiscriminator_loss: 1.0926079750061035 \tgenerator_loss: 1.0298447608947754\n",
            "For Step: 15130 recon_loss: 0.17602212727069855 \tdiscriminator_loss: 1.2319632768630981 \tgenerator_loss: 0.952717661857605\n",
            "For Step: 15140 recon_loss: 0.17464789748191833 \tdiscriminator_loss: 1.3172938823699951 \tgenerator_loss: 0.8791493773460388\n",
            "For Step: 15150 recon_loss: 0.17630410194396973 \tdiscriminator_loss: 1.1796797513961792 \tgenerator_loss: 0.9961640238761902\n",
            "For Step: 15160 recon_loss: 0.18784715235233307 \tdiscriminator_loss: 1.1620204448699951 \tgenerator_loss: 0.9899271130561829\n",
            "For Step: 15170 recon_loss: 0.18002471327781677 \tdiscriminator_loss: 1.2611656188964844 \tgenerator_loss: 0.9215177297592163\n",
            "For Step: 15180 recon_loss: 0.1677013337612152 \tdiscriminator_loss: 1.3072423934936523 \tgenerator_loss: 0.9924193620681763\n",
            "For Step: 15190 recon_loss: 0.17546136677265167 \tdiscriminator_loss: 1.2038862705230713 \tgenerator_loss: 1.0027753114700317\n",
            "For Step: 15200 recon_loss: 0.17868530750274658 \tdiscriminator_loss: 1.2901238203048706 \tgenerator_loss: 0.9401956796646118\n",
            "For Step: 15210 recon_loss: 0.1851307600736618 \tdiscriminator_loss: 1.1417779922485352 \tgenerator_loss: 0.9629635214805603\n",
            "For Step: 15220 recon_loss: 0.18824563920497894 \tdiscriminator_loss: 1.0741617679595947 \tgenerator_loss: 1.0440752506256104\n",
            "For Step: 15230 recon_loss: 0.17553944885730743 \tdiscriminator_loss: 1.0028655529022217 \tgenerator_loss: 0.9996633529663086\n",
            "For Step: 15240 recon_loss: 0.17906680703163147 \tdiscriminator_loss: 1.093245506286621 \tgenerator_loss: 0.9918258786201477\n",
            "For Step: 15250 recon_loss: 0.17153270542621613 \tdiscriminator_loss: 1.1931363344192505 \tgenerator_loss: 1.0158281326293945\n",
            "For Step: 15260 recon_loss: 0.18160875141620636 \tdiscriminator_loss: 1.1670825481414795 \tgenerator_loss: 1.0408475399017334\n",
            "For Step: 15270 recon_loss: 0.18124212324619293 \tdiscriminator_loss: 1.1219576597213745 \tgenerator_loss: 1.025755524635315\n",
            "For Step: 15280 recon_loss: 0.17377090454101562 \tdiscriminator_loss: 1.1519088745117188 \tgenerator_loss: 0.9879153966903687\n",
            "For Step: 15290 recon_loss: 0.18150973320007324 \tdiscriminator_loss: 1.291628360748291 \tgenerator_loss: 0.944949746131897\n",
            "For Step: 15300 recon_loss: 0.20059852302074432 \tdiscriminator_loss: 1.1313610076904297 \tgenerator_loss: 1.0383161306381226\n",
            "For Step: 15310 recon_loss: 0.17473946511745453 \tdiscriminator_loss: 1.2195653915405273 \tgenerator_loss: 1.0059430599212646\n",
            "For Step: 15320 recon_loss: 0.17548711597919464 \tdiscriminator_loss: 1.0917974710464478 \tgenerator_loss: 0.9773983955383301\n",
            "For Step: 15330 recon_loss: 0.17678789794445038 \tdiscriminator_loss: 1.2371764183044434 \tgenerator_loss: 0.9342374205589294\n",
            "For Step: 15340 recon_loss: 0.1684923619031906 \tdiscriminator_loss: 1.2096995115280151 \tgenerator_loss: 0.9526962041854858\n",
            "For Step: 15350 recon_loss: 0.17503632605075836 \tdiscriminator_loss: 1.146615982055664 \tgenerator_loss: 0.9593585729598999\n",
            "For Step: 15360 recon_loss: 0.17558293044567108 \tdiscriminator_loss: 1.1921420097351074 \tgenerator_loss: 0.9789235591888428\n",
            "For Step: 15370 recon_loss: 0.17394104599952698 \tdiscriminator_loss: 1.1902427673339844 \tgenerator_loss: 1.0149879455566406\n",
            "For Step: 15380 recon_loss: 0.19028988480567932 \tdiscriminator_loss: 1.1904847621917725 \tgenerator_loss: 0.9610357880592346\n",
            "For Step: 15390 recon_loss: 0.17707164585590363 \tdiscriminator_loss: 1.1790788173675537 \tgenerator_loss: 1.01215398311615\n",
            "For Step: 15400 recon_loss: 0.18003720045089722 \tdiscriminator_loss: 1.230318307876587 \tgenerator_loss: 0.9267603158950806\n",
            "For Step: 15410 recon_loss: 0.17941515147686005 \tdiscriminator_loss: 1.269836187362671 \tgenerator_loss: 1.0229125022888184\n",
            "For Step: 15420 recon_loss: 0.18613065779209137 \tdiscriminator_loss: 1.1089386940002441 \tgenerator_loss: 0.995956301689148\n",
            "For Step: 15430 recon_loss: 0.18858705461025238 \tdiscriminator_loss: 1.128136157989502 \tgenerator_loss: 1.011430263519287\n",
            "For Step: 15440 recon_loss: 0.1767810583114624 \tdiscriminator_loss: 1.037987232208252 \tgenerator_loss: 1.0480124950408936\n",
            "For Step: 15450 recon_loss: 0.17869460582733154 \tdiscriminator_loss: 1.1648030281066895 \tgenerator_loss: 1.0169212818145752\n",
            "For Step: 15460 recon_loss: 0.17027077078819275 \tdiscriminator_loss: 1.1435778141021729 \tgenerator_loss: 1.0267674922943115\n",
            "For Step: 15470 recon_loss: 0.16680808365345 \tdiscriminator_loss: 1.2409560680389404 \tgenerator_loss: 0.9644957184791565\n",
            "For Step: 15480 recon_loss: 0.16388873755931854 \tdiscriminator_loss: 1.1867637634277344 \tgenerator_loss: 0.9800506830215454\n",
            "For Step: 15490 recon_loss: 0.18147683143615723 \tdiscriminator_loss: 1.2059972286224365 \tgenerator_loss: 0.991642415523529\n",
            "For Step: 15500 recon_loss: 0.19091658294200897 \tdiscriminator_loss: 1.122001051902771 \tgenerator_loss: 0.9781835079193115\n",
            "For Step: 15510 recon_loss: 0.19351136684417725 \tdiscriminator_loss: 1.1270015239715576 \tgenerator_loss: 0.9648722410202026\n",
            "For Step: 15520 recon_loss: 0.177774578332901 \tdiscriminator_loss: 1.1291015148162842 \tgenerator_loss: 1.022568702697754\n",
            "For Step: 15530 recon_loss: 0.18338361382484436 \tdiscriminator_loss: 1.0960588455200195 \tgenerator_loss: 1.0135146379470825\n",
            "For Step: 15540 recon_loss: 0.16491061449050903 \tdiscriminator_loss: 1.0491397380828857 \tgenerator_loss: 0.9831351041793823\n",
            "For Step: 15550 recon_loss: 0.1777370274066925 \tdiscriminator_loss: 1.3086150884628296 \tgenerator_loss: 0.9720659255981445\n",
            "For Step: 15560 recon_loss: 0.1843504160642624 \tdiscriminator_loss: 1.1067163944244385 \tgenerator_loss: 0.95955491065979\n",
            "For Step: 15570 recon_loss: 0.16944807767868042 \tdiscriminator_loss: 1.1694550514221191 \tgenerator_loss: 0.9620630741119385\n",
            "For Step: 15580 recon_loss: 0.18171316385269165 \tdiscriminator_loss: 1.1463165283203125 \tgenerator_loss: 1.0418683290481567\n",
            "For Step: 15590 recon_loss: 0.17144306004047394 \tdiscriminator_loss: 1.170557975769043 \tgenerator_loss: 1.0013141632080078\n",
            "For Step: 15600 recon_loss: 0.17069664597511292 \tdiscriminator_loss: 1.2526822090148926 \tgenerator_loss: 0.9719560146331787\n",
            "For Step: 15610 recon_loss: 0.17386923730373383 \tdiscriminator_loss: 1.1456996202468872 \tgenerator_loss: 1.0072991847991943\n",
            "For Step: 15620 recon_loss: 0.17625349760055542 \tdiscriminator_loss: 1.190409779548645 \tgenerator_loss: 0.9615707993507385\n",
            "For Step: 15630 recon_loss: 0.1802097111940384 \tdiscriminator_loss: 1.2454111576080322 \tgenerator_loss: 0.9774013757705688\n",
            "For Step: 15640 recon_loss: 0.18279029428958893 \tdiscriminator_loss: 1.2108306884765625 \tgenerator_loss: 0.9568797945976257\n",
            "For Step: 15650 recon_loss: 0.16793689131736755 \tdiscriminator_loss: 1.2937957048416138 \tgenerator_loss: 0.9663318395614624\n",
            "For Step: 15660 recon_loss: 0.1747005432844162 \tdiscriminator_loss: 1.242048740386963 \tgenerator_loss: 0.9525407552719116\n",
            "For Step: 15670 recon_loss: 0.16267989575862885 \tdiscriminator_loss: 1.2625715732574463 \tgenerator_loss: 0.9547242522239685\n",
            "For Step: 15680 recon_loss: 0.1602110117673874 \tdiscriminator_loss: 1.2059588432312012 \tgenerator_loss: 1.0141677856445312\n",
            "For Step: 15690 recon_loss: 0.16906432807445526 \tdiscriminator_loss: 1.2121182680130005 \tgenerator_loss: 1.0135815143585205\n",
            "For Step: 15700 recon_loss: 0.1749177724123001 \tdiscriminator_loss: 1.1683883666992188 \tgenerator_loss: 0.9875094294548035\n",
            "For Step: 15710 recon_loss: 0.17962346971035004 \tdiscriminator_loss: 1.1501452922821045 \tgenerator_loss: 0.9553187489509583\n",
            "For Step: 15720 recon_loss: 0.1711413711309433 \tdiscriminator_loss: 1.119760274887085 \tgenerator_loss: 1.0031626224517822\n",
            "For Step: 15730 recon_loss: 0.16515177488327026 \tdiscriminator_loss: 1.1195330619812012 \tgenerator_loss: 0.9611644744873047\n",
            "For Step: 15740 recon_loss: 0.17810280621051788 \tdiscriminator_loss: 1.1090483665466309 \tgenerator_loss: 0.9577798843383789\n",
            "For Step: 15750 recon_loss: 0.18746618926525116 \tdiscriminator_loss: 1.1340125799179077 \tgenerator_loss: 0.9980431795120239\n",
            "For Step: 15760 recon_loss: 0.16339842975139618 \tdiscriminator_loss: 1.2269001007080078 \tgenerator_loss: 0.9267599582672119\n",
            "For Step: 15770 recon_loss: 0.17549431324005127 \tdiscriminator_loss: 1.1328996419906616 \tgenerator_loss: 0.9995495676994324\n",
            "For Step: 15780 recon_loss: 0.1691965013742447 \tdiscriminator_loss: 1.0817625522613525 \tgenerator_loss: 0.9948243498802185\n",
            "For Step: 15790 recon_loss: 0.16618861258029938 \tdiscriminator_loss: 1.2596937417984009 \tgenerator_loss: 0.9308909177780151\n",
            "For Step: 15800 recon_loss: 0.16898125410079956 \tdiscriminator_loss: 1.1438692808151245 \tgenerator_loss: 0.9725567698478699\n",
            "For Step: 15810 recon_loss: 0.17584329843521118 \tdiscriminator_loss: 1.1435737609863281 \tgenerator_loss: 0.9417717456817627\n",
            "For Step: 15820 recon_loss: 0.18029306828975677 \tdiscriminator_loss: 1.1675851345062256 \tgenerator_loss: 0.9586310982704163\n",
            "For Step: 15830 recon_loss: 0.1741819828748703 \tdiscriminator_loss: 1.261103868484497 \tgenerator_loss: 0.9199041724205017\n",
            "For Step: 15840 recon_loss: 0.18225759267807007 \tdiscriminator_loss: 1.2384581565856934 \tgenerator_loss: 0.938949465751648\n",
            "For Step: 15850 recon_loss: 0.18084362149238586 \tdiscriminator_loss: 1.201157569885254 \tgenerator_loss: 0.9642235636711121\n",
            "For Step: 15860 recon_loss: 0.16843904554843903 \tdiscriminator_loss: 1.2951339483261108 \tgenerator_loss: 0.873294472694397\n",
            "For Step: 15870 recon_loss: 0.1703151911497116 \tdiscriminator_loss: 1.0833206176757812 \tgenerator_loss: 1.0147897005081177\n",
            "For Step: 15880 recon_loss: 0.17197997868061066 \tdiscriminator_loss: 1.070127248764038 \tgenerator_loss: 1.0836472511291504\n",
            "For Step: 15890 recon_loss: 0.1758464276790619 \tdiscriminator_loss: 1.1440768241882324 \tgenerator_loss: 0.8910576701164246\n",
            "For Step: 15900 recon_loss: 0.16639003157615662 \tdiscriminator_loss: 1.3263225555419922 \tgenerator_loss: 0.9429575204849243\n",
            "For Step: 15910 recon_loss: 0.16933853924274445 \tdiscriminator_loss: 1.1451606750488281 \tgenerator_loss: 0.967080295085907\n",
            "For Step: 15920 recon_loss: 0.1684218943119049 \tdiscriminator_loss: 1.1765602827072144 \tgenerator_loss: 1.0366694927215576\n",
            "For Step: 15930 recon_loss: 0.16983067989349365 \tdiscriminator_loss: 1.1061151027679443 \tgenerator_loss: 0.9508584141731262\n",
            "For Step: 15940 recon_loss: 0.1725631207227707 \tdiscriminator_loss: 1.144903302192688 \tgenerator_loss: 0.9926086664199829\n",
            "For Step: 15950 recon_loss: 0.17503313720226288 \tdiscriminator_loss: 1.2070350646972656 \tgenerator_loss: 0.9372659921646118\n",
            "For Step: 15960 recon_loss: 0.1767800748348236 \tdiscriminator_loss: 1.1296415328979492 \tgenerator_loss: 0.9856255054473877\n",
            "For Step: 15970 recon_loss: 0.1624758094549179 \tdiscriminator_loss: 1.0903449058532715 \tgenerator_loss: 0.9378976225852966\n",
            "For Step: 15980 recon_loss: 0.19026325643062592 \tdiscriminator_loss: 1.2760692834854126 \tgenerator_loss: 0.9279977083206177\n",
            "For Step: 15990 recon_loss: 0.17827187478542328 \tdiscriminator_loss: 1.221632957458496 \tgenerator_loss: 1.0247653722763062\n",
            "For Step: 16000 recon_loss: 0.18009412288665771 \tdiscriminator_loss: 1.1329745054244995 \tgenerator_loss: 1.0054986476898193\n",
            "For Step: 16010 recon_loss: 0.17936478555202484 \tdiscriminator_loss: 1.2448958158493042 \tgenerator_loss: 0.9486441612243652\n",
            "For Step: 16020 recon_loss: 0.1812959760427475 \tdiscriminator_loss: 1.213614821434021 \tgenerator_loss: 1.011648178100586\n",
            "For Step: 16030 recon_loss: 0.16641120612621307 \tdiscriminator_loss: 1.1288316249847412 \tgenerator_loss: 1.0563602447509766\n",
            "For Step: 16040 recon_loss: 0.19580070674419403 \tdiscriminator_loss: 1.2557121515274048 \tgenerator_loss: 0.942639946937561\n",
            "For Step: 16050 recon_loss: 0.1841288059949875 \tdiscriminator_loss: 1.2414644956588745 \tgenerator_loss: 0.9598097205162048\n",
            "For Step: 16060 recon_loss: 0.1837053894996643 \tdiscriminator_loss: 1.0832176208496094 \tgenerator_loss: 1.0186078548431396\n",
            "For Step: 16070 recon_loss: 0.1722375750541687 \tdiscriminator_loss: 1.1027917861938477 \tgenerator_loss: 1.0219647884368896\n",
            "For Step: 16080 recon_loss: 0.18355076014995575 \tdiscriminator_loss: 1.1355295181274414 \tgenerator_loss: 1.0061241388320923\n",
            "For Step: 16090 recon_loss: 0.1789262592792511 \tdiscriminator_loss: 1.1604609489440918 \tgenerator_loss: 0.9397099614143372\n",
            "For Step: 16100 recon_loss: 0.1782814860343933 \tdiscriminator_loss: 1.1733839511871338 \tgenerator_loss: 0.9321216940879822\n",
            "For Step: 16110 recon_loss: 0.17284739017486572 \tdiscriminator_loss: 1.2531251907348633 \tgenerator_loss: 0.8942256569862366\n",
            "For Step: 16120 recon_loss: 0.20048527419567108 \tdiscriminator_loss: 1.211548924446106 \tgenerator_loss: 0.9525984525680542\n",
            "For Step: 16130 recon_loss: 0.17930947244167328 \tdiscriminator_loss: 1.0555171966552734 \tgenerator_loss: 1.035573959350586\n",
            "For Step: 16140 recon_loss: 0.1719270646572113 \tdiscriminator_loss: 1.2378597259521484 \tgenerator_loss: 0.9308043122291565\n",
            "For Step: 16150 recon_loss: 0.17231863737106323 \tdiscriminator_loss: 1.1137356758117676 \tgenerator_loss: 1.0748815536499023\n",
            "For Step: 16160 recon_loss: 0.15814244747161865 \tdiscriminator_loss: 1.268855094909668 \tgenerator_loss: 0.9691810607910156\n",
            "For Step: 16170 recon_loss: 0.1810571849346161 \tdiscriminator_loss: 1.2148256301879883 \tgenerator_loss: 0.9831529855728149\n",
            "For Step: 16180 recon_loss: 0.164622962474823 \tdiscriminator_loss: 1.1085171699523926 \tgenerator_loss: 0.9411402940750122\n",
            "For Step: 16190 recon_loss: 0.18068461120128632 \tdiscriminator_loss: 1.2501332759857178 \tgenerator_loss: 0.9736771583557129\n",
            "For Step: 16200 recon_loss: 0.18059372901916504 \tdiscriminator_loss: 1.2202208042144775 \tgenerator_loss: 1.031689167022705\n",
            "For Step: 16210 recon_loss: 0.16201147437095642 \tdiscriminator_loss: 1.1981815099716187 \tgenerator_loss: 1.0216395854949951\n",
            "For Step: 16220 recon_loss: 0.18804338574409485 \tdiscriminator_loss: 1.2231465578079224 \tgenerator_loss: 0.9463014006614685\n",
            "For Step: 16230 recon_loss: 0.1716645359992981 \tdiscriminator_loss: 1.272264003753662 \tgenerator_loss: 1.0005180835723877\n",
            "For Step: 16240 recon_loss: 0.1789591908454895 \tdiscriminator_loss: 1.0995920896530151 \tgenerator_loss: 1.0032718181610107\n",
            "For Step: 16250 recon_loss: 0.17516370117664337 \tdiscriminator_loss: 1.1747543811798096 \tgenerator_loss: 0.9623731970787048\n",
            "For Step: 16260 recon_loss: 0.1695515513420105 \tdiscriminator_loss: 1.124226450920105 \tgenerator_loss: 0.9531158208847046\n",
            "For Step: 16270 recon_loss: 0.17838886380195618 \tdiscriminator_loss: 1.2590250968933105 \tgenerator_loss: 0.9228935241699219\n",
            "For Step: 16280 recon_loss: 0.17855067551136017 \tdiscriminator_loss: 1.1050843000411987 \tgenerator_loss: 1.0220144987106323\n",
            "For Step: 16290 recon_loss: 0.1820918470621109 \tdiscriminator_loss: 1.1495277881622314 \tgenerator_loss: 1.0181388854980469\n",
            "For Step: 16300 recon_loss: 0.17620812356472015 \tdiscriminator_loss: 1.2926660776138306 \tgenerator_loss: 0.9773980379104614\n",
            "For Step: 16310 recon_loss: 0.16666805744171143 \tdiscriminator_loss: 1.0956084728240967 \tgenerator_loss: 1.0191726684570312\n",
            "For Step: 16320 recon_loss: 0.1827217936515808 \tdiscriminator_loss: 1.2039422988891602 \tgenerator_loss: 0.9591194987297058\n",
            "For Step: 16330 recon_loss: 0.17953947186470032 \tdiscriminator_loss: 1.0818170309066772 \tgenerator_loss: 1.0454363822937012\n",
            "For Step: 16340 recon_loss: 0.1708148866891861 \tdiscriminator_loss: 1.3371894359588623 \tgenerator_loss: 0.9131762981414795\n",
            "For Step: 16350 recon_loss: 0.18554452061653137 \tdiscriminator_loss: 1.175602912902832 \tgenerator_loss: 1.0375871658325195\n",
            "For Step: 16360 recon_loss: 0.1808316707611084 \tdiscriminator_loss: 1.0569645166397095 \tgenerator_loss: 1.049439549446106\n",
            "For Step: 16370 recon_loss: 0.16710327565670013 \tdiscriminator_loss: 1.2532262802124023 \tgenerator_loss: 0.9240283966064453\n",
            "For Step: 16380 recon_loss: 0.16883467137813568 \tdiscriminator_loss: 1.2012016773223877 \tgenerator_loss: 0.9354514479637146\n",
            "For Step: 16390 recon_loss: 0.1796267330646515 \tdiscriminator_loss: 1.1263642311096191 \tgenerator_loss: 0.9463973641395569\n",
            "For Step: 16400 recon_loss: 0.1753714233636856 \tdiscriminator_loss: 1.1573960781097412 \tgenerator_loss: 0.9700039625167847\n",
            "For Step: 16410 recon_loss: 0.17793519794940948 \tdiscriminator_loss: 0.9769733548164368 \tgenerator_loss: 1.022552490234375\n",
            "For Step: 16420 recon_loss: 0.18001289665699005 \tdiscriminator_loss: 1.1816301345825195 \tgenerator_loss: 0.9172636866569519\n",
            "For Step: 16430 recon_loss: 0.18114623427391052 \tdiscriminator_loss: 1.1384892463684082 \tgenerator_loss: 1.0579187870025635\n",
            "For Step: 16440 recon_loss: 0.18110321462154388 \tdiscriminator_loss: 1.4295225143432617 \tgenerator_loss: 0.9244219660758972\n",
            "For Step: 16450 recon_loss: 0.17480139434337616 \tdiscriminator_loss: 1.2214072942733765 \tgenerator_loss: 0.9736077785491943\n",
            "For Step: 16460 recon_loss: 0.17686417698860168 \tdiscriminator_loss: 1.1219514608383179 \tgenerator_loss: 0.9603319764137268\n",
            "For Step: 16470 recon_loss: 0.17531390488147736 \tdiscriminator_loss: 1.2631773948669434 \tgenerator_loss: 1.0180749893188477\n",
            "For Step: 16480 recon_loss: 0.18442152440547943 \tdiscriminator_loss: 1.264775276184082 \tgenerator_loss: 0.903150737285614\n",
            "For Step: 16490 recon_loss: 0.16429556906223297 \tdiscriminator_loss: 1.21849524974823 \tgenerator_loss: 0.9730894565582275\n",
            "For Step: 16500 recon_loss: 0.1744040846824646 \tdiscriminator_loss: 1.1378928422927856 \tgenerator_loss: 0.9715405106544495\n",
            "For Step: 16510 recon_loss: 0.18677175045013428 \tdiscriminator_loss: 1.2111084461212158 \tgenerator_loss: 0.9334863424301147\n",
            "For Step: 16520 recon_loss: 0.18185414373874664 \tdiscriminator_loss: 1.1116347312927246 \tgenerator_loss: 1.0048630237579346\n",
            "For Step: 16530 recon_loss: 0.1737137883901596 \tdiscriminator_loss: 1.3183870315551758 \tgenerator_loss: 0.9198262691497803\n",
            "For Step: 16540 recon_loss: 0.1694871336221695 \tdiscriminator_loss: 1.1623108386993408 \tgenerator_loss: 1.0302571058273315\n",
            "For Step: 16550 recon_loss: 0.18623989820480347 \tdiscriminator_loss: 1.1697643995285034 \tgenerator_loss: 0.9658313989639282\n",
            "For Step: 16560 recon_loss: 0.177064448595047 \tdiscriminator_loss: 1.2278400659561157 \tgenerator_loss: 0.9439713358879089\n",
            "For Step: 16570 recon_loss: 0.17495295405387878 \tdiscriminator_loss: 1.0599384307861328 \tgenerator_loss: 0.9793760776519775\n",
            "For Step: 16580 recon_loss: 0.16741377115249634 \tdiscriminator_loss: 1.1728365421295166 \tgenerator_loss: 0.9611724615097046\n",
            "For Step: 16590 recon_loss: 0.19058839976787567 \tdiscriminator_loss: 1.2426605224609375 \tgenerator_loss: 0.9733631610870361\n",
            "For Step: 16600 recon_loss: 0.18259753286838531 \tdiscriminator_loss: 1.161656141281128 \tgenerator_loss: 1.0087332725524902\n",
            "For Step: 16610 recon_loss: 0.18054747581481934 \tdiscriminator_loss: 1.2502695322036743 \tgenerator_loss: 0.9170612692832947\n",
            "For Step: 16620 recon_loss: 0.1832830309867859 \tdiscriminator_loss: 1.2011466026306152 \tgenerator_loss: 0.9451909065246582\n",
            "For Step: 16630 recon_loss: 0.1750408113002777 \tdiscriminator_loss: 1.2276806831359863 \tgenerator_loss: 0.989173173904419\n",
            "For Step: 16640 recon_loss: 0.18262244760990143 \tdiscriminator_loss: 1.0792287588119507 \tgenerator_loss: 0.9775857925415039\n",
            "For Step: 16650 recon_loss: 0.17490273714065552 \tdiscriminator_loss: 1.236485481262207 \tgenerator_loss: 0.8751803636550903\n",
            "For Step: 16660 recon_loss: 0.18512161076068878 \tdiscriminator_loss: 1.1749207973480225 \tgenerator_loss: 1.0163480043411255\n",
            "For Step: 16670 recon_loss: 0.17211970686912537 \tdiscriminator_loss: 1.173144817352295 \tgenerator_loss: 1.0014305114746094\n",
            "For Step: 16680 recon_loss: 0.16681092977523804 \tdiscriminator_loss: 1.2320573329925537 \tgenerator_loss: 0.9518128633499146\n",
            "For Step: 16690 recon_loss: 0.17603270709514618 \tdiscriminator_loss: 1.1527655124664307 \tgenerator_loss: 1.0106810331344604\n",
            "For Step: 16700 recon_loss: 0.1684161275625229 \tdiscriminator_loss: 1.306049108505249 \tgenerator_loss: 0.8525187969207764\n",
            "For Step: 16710 recon_loss: 0.18533562123775482 \tdiscriminator_loss: 1.2370660305023193 \tgenerator_loss: 0.9797810316085815\n",
            "For Step: 16720 recon_loss: 0.1755693256855011 \tdiscriminator_loss: 1.1780180931091309 \tgenerator_loss: 0.9349222183227539\n",
            "For Step: 16730 recon_loss: 0.17869161069393158 \tdiscriminator_loss: 0.998950719833374 \tgenerator_loss: 1.0209978818893433\n",
            "For Step: 16740 recon_loss: 0.17647825181484222 \tdiscriminator_loss: 1.111361026763916 \tgenerator_loss: 1.0050764083862305\n",
            "For Step: 16750 recon_loss: 0.18826617300510406 \tdiscriminator_loss: 1.2606313228607178 \tgenerator_loss: 0.978575587272644\n",
            "For Step: 16760 recon_loss: 0.1708848923444748 \tdiscriminator_loss: 1.1261272430419922 \tgenerator_loss: 1.0752296447753906\n",
            "For Step: 16770 recon_loss: 0.18322937190532684 \tdiscriminator_loss: 1.257789969444275 \tgenerator_loss: 0.9136390686035156\n",
            "For Step: 16780 recon_loss: 0.17589569091796875 \tdiscriminator_loss: 1.1915361881256104 \tgenerator_loss: 1.0340118408203125\n",
            "For Step: 16790 recon_loss: 0.19049842655658722 \tdiscriminator_loss: 1.1491074562072754 \tgenerator_loss: 0.9653099179267883\n",
            "For Step: 16800 recon_loss: 0.17489255964756012 \tdiscriminator_loss: 1.3810555934906006 \tgenerator_loss: 0.8852614760398865\n",
            "For Step: 16810 recon_loss: 0.18125483393669128 \tdiscriminator_loss: 1.2817714214324951 \tgenerator_loss: 0.8885956406593323\n",
            "For Step: 16820 recon_loss: 0.1801985204219818 \tdiscriminator_loss: 1.0944745540618896 \tgenerator_loss: 0.9734340310096741\n",
            "For Step: 16830 recon_loss: 0.1795254945755005 \tdiscriminator_loss: 1.1048731803894043 \tgenerator_loss: 1.0930777788162231\n",
            "For Step: 16840 recon_loss: 0.17347723245620728 \tdiscriminator_loss: 1.09700608253479 \tgenerator_loss: 1.0564334392547607\n",
            "For Step: 16850 recon_loss: 0.17672240734100342 \tdiscriminator_loss: 1.1813585758209229 \tgenerator_loss: 0.9492802619934082\n",
            "For Step: 16860 recon_loss: 0.18336784839630127 \tdiscriminator_loss: 1.0765211582183838 \tgenerator_loss: 1.0655081272125244\n",
            "For Step: 16870 recon_loss: 0.18108001351356506 \tdiscriminator_loss: 1.2620512247085571 \tgenerator_loss: 1.0033791065216064\n",
            "For Step: 16880 recon_loss: 0.1735956072807312 \tdiscriminator_loss: 1.082797646522522 \tgenerator_loss: 0.9797677993774414\n",
            "For Step: 16890 recon_loss: 0.17587999999523163 \tdiscriminator_loss: 1.02708101272583 \tgenerator_loss: 1.0266932249069214\n",
            "For Step: 16900 recon_loss: 0.17477785050868988 \tdiscriminator_loss: 1.1462657451629639 \tgenerator_loss: 1.0182337760925293\n",
            "For Step: 16910 recon_loss: 0.17848512530326843 \tdiscriminator_loss: 1.2218570709228516 \tgenerator_loss: 1.0251890420913696\n",
            "For Step: 16920 recon_loss: 0.1814415156841278 \tdiscriminator_loss: 1.1748104095458984 \tgenerator_loss: 1.0581969022750854\n",
            "For Step: 16930 recon_loss: 0.17991280555725098 \tdiscriminator_loss: 1.2894121408462524 \tgenerator_loss: 0.9683206081390381\n",
            "For Step: 16940 recon_loss: 0.17725808918476105 \tdiscriminator_loss: 1.1722197532653809 \tgenerator_loss: 0.9409964680671692\n",
            "For Step: 16950 recon_loss: 0.17024612426757812 \tdiscriminator_loss: 1.1640592813491821 \tgenerator_loss: 0.9792417287826538\n",
            "For Step: 16960 recon_loss: 0.18617814779281616 \tdiscriminator_loss: 1.1359868049621582 \tgenerator_loss: 1.0409469604492188\n",
            "For Step: 16970 recon_loss: 0.16282919049263 \tdiscriminator_loss: 1.2558659315109253 \tgenerator_loss: 0.9621270895004272\n",
            "For Step: 16980 recon_loss: 0.18479187786579132 \tdiscriminator_loss: 1.1581125259399414 \tgenerator_loss: 0.993354320526123\n",
            "For Step: 16990 recon_loss: 0.18210594356060028 \tdiscriminator_loss: 1.2444727420806885 \tgenerator_loss: 0.9648723602294922\n",
            "For Step: 17000 recon_loss: 0.17442575097084045 \tdiscriminator_loss: 1.1815954446792603 \tgenerator_loss: 0.9888260364532471\n",
            "For Step: 17010 recon_loss: 0.18522122502326965 \tdiscriminator_loss: 1.0870695114135742 \tgenerator_loss: 1.0033905506134033\n",
            "For Step: 17020 recon_loss: 0.1826115846633911 \tdiscriminator_loss: 1.3038569688796997 \tgenerator_loss: 0.8647382855415344\n",
            "For Step: 17030 recon_loss: 0.1731773167848587 \tdiscriminator_loss: 1.0298051834106445 \tgenerator_loss: 0.9954874515533447\n",
            "For Step: 17040 recon_loss: 0.18192240595817566 \tdiscriminator_loss: 1.1008727550506592 \tgenerator_loss: 1.012444019317627\n",
            "For Step: 17050 recon_loss: 0.18973438441753387 \tdiscriminator_loss: 1.081737756729126 \tgenerator_loss: 1.0368692874908447\n",
            "For Step: 17060 recon_loss: 0.1643403172492981 \tdiscriminator_loss: 1.1332148313522339 \tgenerator_loss: 1.002323865890503\n",
            "For Step: 17070 recon_loss: 0.1868225336074829 \tdiscriminator_loss: 1.1487715244293213 \tgenerator_loss: 1.0591375827789307\n",
            "For Step: 17080 recon_loss: 0.18376116454601288 \tdiscriminator_loss: 1.1743837594985962 \tgenerator_loss: 0.9844509363174438\n",
            "For Step: 17090 recon_loss: 0.17590269446372986 \tdiscriminator_loss: 1.2546278238296509 \tgenerator_loss: 0.9348104000091553\n",
            "For Step: 17100 recon_loss: 0.16700699925422668 \tdiscriminator_loss: 1.1764684915542603 \tgenerator_loss: 0.9733130931854248\n",
            "For Step: 17110 recon_loss: 0.1774643510580063 \tdiscriminator_loss: 1.167614459991455 \tgenerator_loss: 0.9795827865600586\n",
            "For Step: 17120 recon_loss: 0.18118202686309814 \tdiscriminator_loss: 1.288983702659607 \tgenerator_loss: 0.9108297824859619\n",
            "For Step: 17130 recon_loss: 0.17534051835536957 \tdiscriminator_loss: 1.2597242593765259 \tgenerator_loss: 0.9435679912567139\n",
            "For Step: 17140 recon_loss: 0.1767074167728424 \tdiscriminator_loss: 1.0864851474761963 \tgenerator_loss: 1.0545666217803955\n",
            "For Step: 17150 recon_loss: 0.1784622073173523 \tdiscriminator_loss: 1.193467140197754 \tgenerator_loss: 0.9857531189918518\n",
            "For Step: 17160 recon_loss: 0.18125611543655396 \tdiscriminator_loss: 1.1991143226623535 \tgenerator_loss: 0.9504349231719971\n",
            "For Step: 17170 recon_loss: 0.18502071499824524 \tdiscriminator_loss: 1.086507797241211 \tgenerator_loss: 0.9878195524215698\n",
            "For Step: 17180 recon_loss: 0.18192730844020844 \tdiscriminator_loss: 1.3004446029663086 \tgenerator_loss: 0.9294805526733398\n",
            "For Step: 17190 recon_loss: 0.17555071413516998 \tdiscriminator_loss: 1.2397119998931885 \tgenerator_loss: 0.9821125864982605\n",
            "For Step: 17200 recon_loss: 0.1729680299758911 \tdiscriminator_loss: 1.2431738376617432 \tgenerator_loss: 0.8864481449127197\n",
            "For Step: 17210 recon_loss: 0.17723721265792847 \tdiscriminator_loss: 1.158120036125183 \tgenerator_loss: 0.9281557202339172\n",
            "For Step: 17220 recon_loss: 0.1754966676235199 \tdiscriminator_loss: 1.1564887762069702 \tgenerator_loss: 0.9852766394615173\n",
            "For Step: 17230 recon_loss: 0.17647100985050201 \tdiscriminator_loss: 1.140695571899414 \tgenerator_loss: 0.9435423612594604\n",
            "For Step: 17240 recon_loss: 0.190049946308136 \tdiscriminator_loss: 1.1374523639678955 \tgenerator_loss: 0.9510645270347595\n",
            "For Step: 17250 recon_loss: 0.1631881594657898 \tdiscriminator_loss: 1.2554137706756592 \tgenerator_loss: 0.8814338445663452\n",
            "For Step: 17260 recon_loss: 0.17084896564483643 \tdiscriminator_loss: 1.1291990280151367 \tgenerator_loss: 0.9997856020927429\n",
            "For Step: 17270 recon_loss: 0.1726704090833664 \tdiscriminator_loss: 1.0922935009002686 \tgenerator_loss: 0.969692051410675\n",
            "For Step: 17280 recon_loss: 0.1784660816192627 \tdiscriminator_loss: 1.321770429611206 \tgenerator_loss: 0.9025061130523682\n",
            "For Step: 17290 recon_loss: 0.18215312063694 \tdiscriminator_loss: 1.1416645050048828 \tgenerator_loss: 0.9858068227767944\n",
            "For Step: 17300 recon_loss: 0.19690312445163727 \tdiscriminator_loss: 1.1032512187957764 \tgenerator_loss: 0.9937236309051514\n",
            "For Step: 17310 recon_loss: 0.18128740787506104 \tdiscriminator_loss: 1.172569751739502 \tgenerator_loss: 0.9595674276351929\n",
            "For Step: 17320 recon_loss: 0.17545485496520996 \tdiscriminator_loss: 1.1982988119125366 \tgenerator_loss: 1.0097098350524902\n",
            "For Step: 17330 recon_loss: 0.18807917833328247 \tdiscriminator_loss: 1.104559063911438 \tgenerator_loss: 0.9072583913803101\n",
            "For Step: 17340 recon_loss: 0.1799059957265854 \tdiscriminator_loss: 1.0980641841888428 \tgenerator_loss: 0.9513276219367981\n",
            "For Step: 17350 recon_loss: 0.17306199669837952 \tdiscriminator_loss: 1.2279963493347168 \tgenerator_loss: 0.9463515877723694\n",
            "For Step: 17360 recon_loss: 0.18039336800575256 \tdiscriminator_loss: 1.2985432147979736 \tgenerator_loss: 0.9797329902648926\n",
            "For Step: 17370 recon_loss: 0.16641592979431152 \tdiscriminator_loss: 1.0507965087890625 \tgenerator_loss: 0.9865416288375854\n",
            "For Step: 17380 recon_loss: 0.17958290874958038 \tdiscriminator_loss: 1.315221905708313 \tgenerator_loss: 0.8533033132553101\n",
            "For Step: 17390 recon_loss: 0.18001443147659302 \tdiscriminator_loss: 1.2678194046020508 \tgenerator_loss: 0.9294396042823792\n",
            "For Step: 17400 recon_loss: 0.1782781183719635 \tdiscriminator_loss: 1.184720754623413 \tgenerator_loss: 1.0653514862060547\n",
            "For Step: 17410 recon_loss: 0.17603939771652222 \tdiscriminator_loss: 1.2311164140701294 \tgenerator_loss: 0.9983248114585876\n",
            "For Step: 17420 recon_loss: 0.1684330850839615 \tdiscriminator_loss: 1.2364671230316162 \tgenerator_loss: 0.9216175079345703\n",
            "For Step: 17430 recon_loss: 0.1748918741941452 \tdiscriminator_loss: 1.238558053970337 \tgenerator_loss: 0.920885443687439\n",
            "For Step: 17440 recon_loss: 0.1756732314825058 \tdiscriminator_loss: 1.2185988426208496 \tgenerator_loss: 1.0016144514083862\n",
            "For Step: 17450 recon_loss: 0.17423634231090546 \tdiscriminator_loss: 1.1889348030090332 \tgenerator_loss: 1.0159069299697876\n",
            "For Step: 17460 recon_loss: 0.17353998124599457 \tdiscriminator_loss: 1.2306474447250366 \tgenerator_loss: 0.9609255194664001\n",
            "For Step: 17470 recon_loss: 0.1766773760318756 \tdiscriminator_loss: 1.2376041412353516 \tgenerator_loss: 0.9825693964958191\n",
            "For Step: 17480 recon_loss: 0.16301016509532928 \tdiscriminator_loss: 1.1258655786514282 \tgenerator_loss: 0.9787148237228394\n",
            "For Step: 17490 recon_loss: 0.1701892912387848 \tdiscriminator_loss: 1.1479381322860718 \tgenerator_loss: 0.956092894077301\n",
            "For Step: 17500 recon_loss: 0.16391097009181976 \tdiscriminator_loss: 1.1981070041656494 \tgenerator_loss: 0.9316272735595703\n",
            "For Step: 17510 recon_loss: 0.17674458026885986 \tdiscriminator_loss: 1.111318826675415 \tgenerator_loss: 0.9233274459838867\n",
            "For Step: 17520 recon_loss: 0.17380346357822418 \tdiscriminator_loss: 1.242539405822754 \tgenerator_loss: 0.9533295631408691\n",
            "For Step: 17530 recon_loss: 0.17051440477371216 \tdiscriminator_loss: 1.3016952276229858 \tgenerator_loss: 0.8977572321891785\n",
            "For Step: 17540 recon_loss: 0.17992077767848969 \tdiscriminator_loss: 1.1617286205291748 \tgenerator_loss: 1.0119833946228027\n",
            "For Step: 17550 recon_loss: 0.1697567105293274 \tdiscriminator_loss: 1.171026349067688 \tgenerator_loss: 0.9415189027786255\n",
            "For Step: 17560 recon_loss: 0.179135262966156 \tdiscriminator_loss: 1.3129360675811768 \tgenerator_loss: 0.9027976989746094\n",
            "For Step: 17570 recon_loss: 0.18771399557590485 \tdiscriminator_loss: 1.1353483200073242 \tgenerator_loss: 0.9352480173110962\n",
            "For Step: 17580 recon_loss: 0.17525158822536469 \tdiscriminator_loss: 1.1890454292297363 \tgenerator_loss: 0.89508056640625\n",
            "For Step: 17590 recon_loss: 0.17068997025489807 \tdiscriminator_loss: 1.2138116359710693 \tgenerator_loss: 1.0112757682800293\n",
            "For Step: 17600 recon_loss: 0.17598599195480347 \tdiscriminator_loss: 1.0788897275924683 \tgenerator_loss: 1.0105035305023193\n",
            "For Step: 17610 recon_loss: 0.17735017836093903 \tdiscriminator_loss: 1.2474005222320557 \tgenerator_loss: 0.9157588481903076\n",
            "For Step: 17620 recon_loss: 0.17093409597873688 \tdiscriminator_loss: 1.1067413091659546 \tgenerator_loss: 0.9099529385566711\n",
            "For Step: 17630 recon_loss: 0.16197718679904938 \tdiscriminator_loss: 1.3156636953353882 \tgenerator_loss: 0.922766923904419\n",
            "For Step: 17640 recon_loss: 0.16531099379062653 \tdiscriminator_loss: 1.2053251266479492 \tgenerator_loss: 0.9374783039093018\n",
            "For Step: 17650 recon_loss: 0.17791754007339478 \tdiscriminator_loss: 1.1178526878356934 \tgenerator_loss: 0.9613069891929626\n",
            "For Step: 17660 recon_loss: 0.16512945294380188 \tdiscriminator_loss: 1.132537603378296 \tgenerator_loss: 1.062846302986145\n",
            "For Step: 17670 recon_loss: 0.17122171819210052 \tdiscriminator_loss: 1.2099523544311523 \tgenerator_loss: 0.9042080640792847\n",
            "For Step: 17680 recon_loss: 0.17512322962284088 \tdiscriminator_loss: 1.2524573802947998 \tgenerator_loss: 0.9818121194839478\n",
            "For Step: 17690 recon_loss: 0.18002569675445557 \tdiscriminator_loss: 1.1085901260375977 \tgenerator_loss: 0.9599074721336365\n",
            "For Step: 17700 recon_loss: 0.17805008590221405 \tdiscriminator_loss: 1.047203779220581 \tgenerator_loss: 0.9891141653060913\n",
            "For Step: 17710 recon_loss: 0.16796326637268066 \tdiscriminator_loss: 1.1761136054992676 \tgenerator_loss: 1.0411306619644165\n",
            "For Step: 17720 recon_loss: 0.1712944209575653 \tdiscriminator_loss: 1.100032925605774 \tgenerator_loss: 1.0450987815856934\n",
            "For Step: 17730 recon_loss: 0.18029281497001648 \tdiscriminator_loss: 1.1756775379180908 \tgenerator_loss: 0.9751389622688293\n",
            "For Step: 17740 recon_loss: 0.1864023059606552 \tdiscriminator_loss: 1.2236149311065674 \tgenerator_loss: 0.996820330619812\n",
            "For Step: 17750 recon_loss: 0.17684926092624664 \tdiscriminator_loss: 1.1508235931396484 \tgenerator_loss: 0.9778642058372498\n",
            "For Step: 17760 recon_loss: 0.17290712893009186 \tdiscriminator_loss: 1.2535231113433838 \tgenerator_loss: 0.9048959016799927\n",
            "For Step: 17770 recon_loss: 0.18272240459918976 \tdiscriminator_loss: 1.0969536304473877 \tgenerator_loss: 0.9756207466125488\n",
            "For Step: 17780 recon_loss: 0.1813639998435974 \tdiscriminator_loss: 1.142730474472046 \tgenerator_loss: 0.9794021248817444\n",
            "For Step: 17790 recon_loss: 0.17574043571949005 \tdiscriminator_loss: 1.1024467945098877 \tgenerator_loss: 1.0685036182403564\n",
            "For Step: 17800 recon_loss: 0.18451142311096191 \tdiscriminator_loss: 1.1173685789108276 \tgenerator_loss: 1.02616548538208\n",
            "For Step: 17810 recon_loss: 0.17284360527992249 \tdiscriminator_loss: 1.2864619493484497 \tgenerator_loss: 1.0025098323822021\n",
            "For Step: 17820 recon_loss: 0.17967794835567474 \tdiscriminator_loss: 1.0959820747375488 \tgenerator_loss: 1.0359212160110474\n",
            "For Step: 17830 recon_loss: 0.18206483125686646 \tdiscriminator_loss: 1.2296998500823975 \tgenerator_loss: 0.9460170269012451\n",
            "For Step: 17840 recon_loss: 0.1712159514427185 \tdiscriminator_loss: 1.1926910877227783 \tgenerator_loss: 1.016033411026001\n",
            "For Step: 17850 recon_loss: 0.16228358447551727 \tdiscriminator_loss: 1.1338140964508057 \tgenerator_loss: 0.9560368657112122\n",
            "For Step: 17860 recon_loss: 0.1775256246328354 \tdiscriminator_loss: 1.2314074039459229 \tgenerator_loss: 0.9395255446434021\n",
            "For Step: 17870 recon_loss: 0.1703370362520218 \tdiscriminator_loss: 1.2570538520812988 \tgenerator_loss: 0.9737333059310913\n",
            "For Step: 17880 recon_loss: 0.17915785312652588 \tdiscriminator_loss: 1.2234623432159424 \tgenerator_loss: 1.0257595777511597\n",
            "For Step: 17890 recon_loss: 0.1706213802099228 \tdiscriminator_loss: 1.1618069410324097 \tgenerator_loss: 0.9604204893112183\n",
            "For Step: 17900 recon_loss: 0.18714332580566406 \tdiscriminator_loss: 1.1017422676086426 \tgenerator_loss: 1.0156383514404297\n",
            "For Step: 17910 recon_loss: 0.17154037952423096 \tdiscriminator_loss: 1.1852891445159912 \tgenerator_loss: 0.931915819644928\n",
            "For Step: 17920 recon_loss: 0.17331695556640625 \tdiscriminator_loss: 1.2571122646331787 \tgenerator_loss: 0.9021039009094238\n",
            "For Step: 17930 recon_loss: 0.16706949472427368 \tdiscriminator_loss: 1.1216202974319458 \tgenerator_loss: 0.9542944431304932\n",
            "For Step: 17940 recon_loss: 0.15485826134681702 \tdiscriminator_loss: 1.1989632844924927 \tgenerator_loss: 0.8992294073104858\n",
            "For Step: 17950 recon_loss: 0.175129696726799 \tdiscriminator_loss: 1.1778515577316284 \tgenerator_loss: 0.9390161633491516\n",
            "For Step: 17960 recon_loss: 0.1716098040342331 \tdiscriminator_loss: 1.1165976524353027 \tgenerator_loss: 0.9776516556739807\n",
            "For Step: 17970 recon_loss: 0.17661191523075104 \tdiscriminator_loss: 1.2447943687438965 \tgenerator_loss: 0.9329885244369507\n",
            "For Step: 17980 recon_loss: 0.17776361107826233 \tdiscriminator_loss: 1.222440242767334 \tgenerator_loss: 0.9812620878219604\n",
            "For Step: 17990 recon_loss: 0.18231244385242462 \tdiscriminator_loss: 1.0829166173934937 \tgenerator_loss: 0.9556536078453064\n",
            "For Step: 18000 recon_loss: 0.186670184135437 \tdiscriminator_loss: 1.2305653095245361 \tgenerator_loss: 0.9810991883277893\n",
            "For Step: 18010 recon_loss: 0.18054607510566711 \tdiscriminator_loss: 1.2368721961975098 \tgenerator_loss: 0.9141607284545898\n",
            "For Step: 18020 recon_loss: 0.18244105577468872 \tdiscriminator_loss: 1.1314797401428223 \tgenerator_loss: 0.9831410050392151\n",
            "For Step: 18030 recon_loss: 0.1843479573726654 \tdiscriminator_loss: 1.0866777896881104 \tgenerator_loss: 1.0520272254943848\n",
            "For Step: 18040 recon_loss: 0.16289320588111877 \tdiscriminator_loss: 1.2016685009002686 \tgenerator_loss: 0.9434627294540405\n",
            "For Step: 18050 recon_loss: 0.18170668184757233 \tdiscriminator_loss: 1.174534559249878 \tgenerator_loss: 0.9676596522331238\n",
            "For Step: 18060 recon_loss: 0.1779928058385849 \tdiscriminator_loss: 1.1956675052642822 \tgenerator_loss: 1.0013291835784912\n",
            "For Step: 18070 recon_loss: 0.18644244968891144 \tdiscriminator_loss: 1.1475112438201904 \tgenerator_loss: 0.9987638592720032\n",
            "For Step: 18080 recon_loss: 0.1690671145915985 \tdiscriminator_loss: 1.118669867515564 \tgenerator_loss: 1.056777834892273\n",
            "For Step: 18090 recon_loss: 0.17643634974956512 \tdiscriminator_loss: 1.1700286865234375 \tgenerator_loss: 0.9691630601882935\n",
            "For Step: 18100 recon_loss: 0.18444563448429108 \tdiscriminator_loss: 1.2029805183410645 \tgenerator_loss: 0.9600003957748413\n",
            "For Step: 18110 recon_loss: 0.17220084369182587 \tdiscriminator_loss: 1.1600048542022705 \tgenerator_loss: 0.9751986265182495\n",
            "For Step: 18120 recon_loss: 0.1688302457332611 \tdiscriminator_loss: 1.289989948272705 \tgenerator_loss: 0.944833517074585\n",
            "For Step: 18130 recon_loss: 0.16805218160152435 \tdiscriminator_loss: 1.2025089263916016 \tgenerator_loss: 0.9144390821456909\n",
            "For Step: 18140 recon_loss: 0.17368026077747345 \tdiscriminator_loss: 1.20327627658844 \tgenerator_loss: 0.957969605922699\n",
            "For Step: 18150 recon_loss: 0.17583654820919037 \tdiscriminator_loss: 1.1423834562301636 \tgenerator_loss: 0.9377846121788025\n",
            "For Step: 18160 recon_loss: 0.17716479301452637 \tdiscriminator_loss: 1.153660774230957 \tgenerator_loss: 1.0130465030670166\n",
            "For Step: 18170 recon_loss: 0.16930222511291504 \tdiscriminator_loss: 1.1203622817993164 \tgenerator_loss: 0.968735933303833\n",
            "For Step: 18180 recon_loss: 0.17721903324127197 \tdiscriminator_loss: 1.187242031097412 \tgenerator_loss: 0.9776521921157837\n",
            "For Step: 18190 recon_loss: 0.17137326300144196 \tdiscriminator_loss: 1.3369442224502563 \tgenerator_loss: 0.9258980751037598\n",
            "For Step: 18200 recon_loss: 0.1839056760072708 \tdiscriminator_loss: 1.1760776042938232 \tgenerator_loss: 0.9532501697540283\n",
            "For Step: 18210 recon_loss: 0.17086192965507507 \tdiscriminator_loss: 1.0742161273956299 \tgenerator_loss: 1.0133452415466309\n",
            "For Step: 18220 recon_loss: 0.17153340578079224 \tdiscriminator_loss: 1.1592825651168823 \tgenerator_loss: 0.9821509122848511\n",
            "For Step: 18230 recon_loss: 0.17360059916973114 \tdiscriminator_loss: 1.2535886764526367 \tgenerator_loss: 0.9135600328445435\n",
            "For Step: 18240 recon_loss: 0.17752976715564728 \tdiscriminator_loss: 1.1873575448989868 \tgenerator_loss: 0.9414933919906616\n",
            "For Step: 18250 recon_loss: 0.19273313879966736 \tdiscriminator_loss: 1.149837851524353 \tgenerator_loss: 1.012930989265442\n",
            "For Step: 18260 recon_loss: 0.17930850386619568 \tdiscriminator_loss: 1.220940351486206 \tgenerator_loss: 0.9766409397125244\n",
            "For Step: 18270 recon_loss: 0.18719500303268433 \tdiscriminator_loss: 1.2516039609909058 \tgenerator_loss: 0.9882614612579346\n",
            "For Step: 18280 recon_loss: 0.16948427259922028 \tdiscriminator_loss: 1.287223219871521 \tgenerator_loss: 0.9529210329055786\n",
            "For Step: 18290 recon_loss: 0.16421407461166382 \tdiscriminator_loss: 1.2924649715423584 \tgenerator_loss: 1.0020008087158203\n",
            "For Step: 18300 recon_loss: 0.1789071261882782 \tdiscriminator_loss: 1.1142863035202026 \tgenerator_loss: 0.9939243197441101\n",
            "For Step: 18310 recon_loss: 0.17165690660476685 \tdiscriminator_loss: 1.1555203199386597 \tgenerator_loss: 0.9929671287536621\n",
            "For Step: 18320 recon_loss: 0.17773500084877014 \tdiscriminator_loss: 1.202269196510315 \tgenerator_loss: 0.9615994691848755\n",
            "For Step: 18330 recon_loss: 0.17618797719478607 \tdiscriminator_loss: 1.0907676219940186 \tgenerator_loss: 0.970484733581543\n",
            "For Step: 18340 recon_loss: 0.16604717075824738 \tdiscriminator_loss: 1.18592369556427 \tgenerator_loss: 0.9843610525131226\n",
            "For Step: 18350 recon_loss: 0.17230598628520966 \tdiscriminator_loss: 1.2223148345947266 \tgenerator_loss: 0.8998661041259766\n",
            "For Step: 18360 recon_loss: 0.18084479868412018 \tdiscriminator_loss: 1.2251513004302979 \tgenerator_loss: 0.9982142448425293\n",
            "For Step: 18370 recon_loss: 0.1807752549648285 \tdiscriminator_loss: 1.1567010879516602 \tgenerator_loss: 0.9926620721817017\n",
            "For Step: 18380 recon_loss: 0.18887890875339508 \tdiscriminator_loss: 1.2032489776611328 \tgenerator_loss: 0.9846088886260986\n",
            "For Step: 18390 recon_loss: 0.17920583486557007 \tdiscriminator_loss: 1.2700161933898926 \tgenerator_loss: 0.8908828496932983\n",
            "For Step: 18400 recon_loss: 0.17647506296634674 \tdiscriminator_loss: 1.2314448356628418 \tgenerator_loss: 0.8755367994308472\n",
            "For Step: 18410 recon_loss: 0.16536350548267365 \tdiscriminator_loss: 1.3318884372711182 \tgenerator_loss: 0.9350706338882446\n",
            "For Step: 18420 recon_loss: 0.17931409180164337 \tdiscriminator_loss: 1.2153807878494263 \tgenerator_loss: 0.916191041469574\n",
            "For Step: 18430 recon_loss: 0.18456821143627167 \tdiscriminator_loss: 1.1671597957611084 \tgenerator_loss: 1.0235661268234253\n",
            "For Step: 18440 recon_loss: 0.17600880563259125 \tdiscriminator_loss: 1.147847056388855 \tgenerator_loss: 1.009403109550476\n",
            "For Step: 18450 recon_loss: 0.18143171072006226 \tdiscriminator_loss: 1.2174447774887085 \tgenerator_loss: 0.9746619462966919\n",
            "For Step: 18460 recon_loss: 0.16863970458507538 \tdiscriminator_loss: 1.2752385139465332 \tgenerator_loss: 0.911737322807312\n",
            "For Step: 18470 recon_loss: 0.17906099557876587 \tdiscriminator_loss: 1.0620509386062622 \tgenerator_loss: 0.9267008900642395\n",
            "For Step: 18480 recon_loss: 0.1799357533454895 \tdiscriminator_loss: 1.2869454622268677 \tgenerator_loss: 0.896946907043457\n",
            "For Step: 18490 recon_loss: 0.18002904951572418 \tdiscriminator_loss: 1.0714523792266846 \tgenerator_loss: 0.9627088308334351\n",
            "For Step: 18500 recon_loss: 0.1849268525838852 \tdiscriminator_loss: 1.1786367893218994 \tgenerator_loss: 0.9416052103042603\n",
            "For Step: 18510 recon_loss: 0.1664922684431076 \tdiscriminator_loss: 1.307450771331787 \tgenerator_loss: 0.8832403421401978\n",
            "For Step: 18520 recon_loss: 0.16441860795021057 \tdiscriminator_loss: 1.1644259691238403 \tgenerator_loss: 0.9481945633888245\n",
            "For Step: 18530 recon_loss: 0.17614753544330597 \tdiscriminator_loss: 1.1875596046447754 \tgenerator_loss: 0.9877957105636597\n",
            "For Step: 18540 recon_loss: 0.1841297298669815 \tdiscriminator_loss: 1.0786306858062744 \tgenerator_loss: 0.9599696397781372\n",
            "For Step: 18550 recon_loss: 0.18688932061195374 \tdiscriminator_loss: 1.2105224132537842 \tgenerator_loss: 0.9180200099945068\n",
            "For Step: 18560 recon_loss: 0.16553984582424164 \tdiscriminator_loss: 1.170093059539795 \tgenerator_loss: 0.9372195601463318\n",
            "For Step: 18570 recon_loss: 0.1774054914712906 \tdiscriminator_loss: 1.1106939315795898 \tgenerator_loss: 1.0001342296600342\n",
            "For Step: 18580 recon_loss: 0.17474402487277985 \tdiscriminator_loss: 1.0167028903961182 \tgenerator_loss: 1.0200297832489014\n",
            "For Step: 18590 recon_loss: 0.17652125656604767 \tdiscriminator_loss: 1.3139076232910156 \tgenerator_loss: 0.9836899042129517\n",
            "For Step: 18600 recon_loss: 0.17204374074935913 \tdiscriminator_loss: 1.2034662961959839 \tgenerator_loss: 0.996503472328186\n",
            "For Step: 18610 recon_loss: 0.17273253202438354 \tdiscriminator_loss: 1.150594711303711 \tgenerator_loss: 0.9528188705444336\n",
            "For Step: 18620 recon_loss: 0.18309389054775238 \tdiscriminator_loss: 1.1448018550872803 \tgenerator_loss: 1.0106903314590454\n",
            "For Step: 18630 recon_loss: 0.18558259308338165 \tdiscriminator_loss: 1.2263705730438232 \tgenerator_loss: 0.9415978193283081\n",
            "For Step: 18640 recon_loss: 0.17954976856708527 \tdiscriminator_loss: 1.1633594036102295 \tgenerator_loss: 1.0034255981445312\n",
            "For Step: 18650 recon_loss: 0.17361776530742645 \tdiscriminator_loss: 1.2477821111679077 \tgenerator_loss: 0.9301044940948486\n",
            "For Step: 18660 recon_loss: 0.17873503267765045 \tdiscriminator_loss: 1.1729068756103516 \tgenerator_loss: 0.8844887018203735\n",
            "For Step: 18670 recon_loss: 0.1680215299129486 \tdiscriminator_loss: 1.1963326930999756 \tgenerator_loss: 0.9451191425323486\n",
            "For Step: 18680 recon_loss: 0.1626473069190979 \tdiscriminator_loss: 1.182413101196289 \tgenerator_loss: 0.925628125667572\n",
            "For Step: 18690 recon_loss: 0.1721852719783783 \tdiscriminator_loss: 1.068767786026001 \tgenerator_loss: 0.9943472743034363\n",
            "For Step: 18700 recon_loss: 0.17205919325351715 \tdiscriminator_loss: 1.2136914730072021 \tgenerator_loss: 0.971027672290802\n",
            "For Step: 18710 recon_loss: 0.17505624890327454 \tdiscriminator_loss: 1.2189815044403076 \tgenerator_loss: 0.9682816863059998\n",
            "For Step: 18720 recon_loss: 0.1635063737630844 \tdiscriminator_loss: 1.2073771953582764 \tgenerator_loss: 0.9531787037849426\n",
            "For Step: 18730 recon_loss: 0.1716027408838272 \tdiscriminator_loss: 1.1287202835083008 \tgenerator_loss: 1.0025365352630615\n",
            "For Step: 18740 recon_loss: 0.1781100481748581 \tdiscriminator_loss: 1.1950504779815674 \tgenerator_loss: 1.007226586341858\n",
            "For Step: 18750 recon_loss: 0.18003584444522858 \tdiscriminator_loss: 1.0524041652679443 \tgenerator_loss: 1.0239547491073608\n",
            "For Step: 18760 recon_loss: 0.1635940670967102 \tdiscriminator_loss: 1.350921630859375 \tgenerator_loss: 0.8319884538650513\n",
            "For Step: 18770 recon_loss: 0.18828752636909485 \tdiscriminator_loss: 1.0743157863616943 \tgenerator_loss: 1.0367448329925537\n",
            "For Step: 18780 recon_loss: 0.18919332325458527 \tdiscriminator_loss: 1.3276629447937012 \tgenerator_loss: 0.9356037378311157\n",
            "For Step: 18790 recon_loss: 0.18991845846176147 \tdiscriminator_loss: 1.1382992267608643 \tgenerator_loss: 0.9572960138320923\n",
            "For Step: 18800 recon_loss: 0.1745976060628891 \tdiscriminator_loss: 1.1660161018371582 \tgenerator_loss: 0.9565319418907166\n",
            "For Step: 18810 recon_loss: 0.1841830015182495 \tdiscriminator_loss: 1.3139071464538574 \tgenerator_loss: 0.876325249671936\n",
            "For Step: 18820 recon_loss: 0.17361827194690704 \tdiscriminator_loss: 1.202622413635254 \tgenerator_loss: 0.8797987103462219\n",
            "For Step: 18830 recon_loss: 0.16847018897533417 \tdiscriminator_loss: 1.1914061307907104 \tgenerator_loss: 0.9873660802841187\n",
            "For Step: 18840 recon_loss: 0.18473662436008453 \tdiscriminator_loss: 1.2182193994522095 \tgenerator_loss: 0.8698474764823914\n",
            "For Step: 18850 recon_loss: 0.1867513507604599 \tdiscriminator_loss: 1.231971025466919 \tgenerator_loss: 0.9409806728363037\n",
            "For Step: 18860 recon_loss: 0.17592495679855347 \tdiscriminator_loss: 1.2739744186401367 \tgenerator_loss: 0.8822938799858093\n",
            "For Step: 18870 recon_loss: 0.17687729001045227 \tdiscriminator_loss: 1.3113343715667725 \tgenerator_loss: 0.9313476085662842\n",
            "For Step: 18880 recon_loss: 0.17486265301704407 \tdiscriminator_loss: 1.2676146030426025 \tgenerator_loss: 0.9237532019615173\n",
            "For Step: 18890 recon_loss: 0.17681242525577545 \tdiscriminator_loss: 1.2199487686157227 \tgenerator_loss: 1.04058837890625\n",
            "For Step: 18900 recon_loss: 0.1775253415107727 \tdiscriminator_loss: 1.0139596462249756 \tgenerator_loss: 0.9587340354919434\n",
            "For Step: 18910 recon_loss: 0.17849980294704437 \tdiscriminator_loss: 1.1541377305984497 \tgenerator_loss: 0.9481067657470703\n",
            "For Step: 18920 recon_loss: 0.17040149867534637 \tdiscriminator_loss: 1.2617446184158325 \tgenerator_loss: 0.9010149836540222\n",
            "For Step: 18930 recon_loss: 0.157131627202034 \tdiscriminator_loss: 1.2059423923492432 \tgenerator_loss: 0.9688397645950317\n",
            "For Step: 18940 recon_loss: 0.17357027530670166 \tdiscriminator_loss: 1.1729631423950195 \tgenerator_loss: 0.9126335382461548\n",
            "For Step: 18950 recon_loss: 0.18868152797222137 \tdiscriminator_loss: 1.1638544797897339 \tgenerator_loss: 0.9788367748260498\n",
            "For Step: 18960 recon_loss: 0.1827695667743683 \tdiscriminator_loss: 1.1419899463653564 \tgenerator_loss: 0.9662044048309326\n",
            "For Step: 18970 recon_loss: 0.1819514036178589 \tdiscriminator_loss: 1.2410790920257568 \tgenerator_loss: 0.9206984043121338\n",
            "For Step: 18980 recon_loss: 0.171469047665596 \tdiscriminator_loss: 1.225294589996338 \tgenerator_loss: 0.9298813343048096\n",
            "For Step: 18990 recon_loss: 0.18097913265228271 \tdiscriminator_loss: 1.2414512634277344 \tgenerator_loss: 0.9850280284881592\n",
            "For Step: 19000 recon_loss: 0.18963412940502167 \tdiscriminator_loss: 1.1477787494659424 \tgenerator_loss: 0.9525081515312195\n",
            "For Step: 19010 recon_loss: 0.1830422580242157 \tdiscriminator_loss: 1.1712698936462402 \tgenerator_loss: 0.8798445463180542\n",
            "For Step: 19020 recon_loss: 0.18823064863681793 \tdiscriminator_loss: 1.088652491569519 \tgenerator_loss: 0.98445063829422\n",
            "For Step: 19030 recon_loss: 0.16801689565181732 \tdiscriminator_loss: 1.3199679851531982 \tgenerator_loss: 0.922418475151062\n",
            "For Step: 19040 recon_loss: 0.17784591019153595 \tdiscriminator_loss: 1.1651231050491333 \tgenerator_loss: 0.9569611549377441\n",
            "For Step: 19050 recon_loss: 0.17793959379196167 \tdiscriminator_loss: 1.1233738660812378 \tgenerator_loss: 0.9923945069313049\n",
            "For Step: 19060 recon_loss: 0.1903480440378189 \tdiscriminator_loss: 1.1307156085968018 \tgenerator_loss: 1.013474464416504\n",
            "For Step: 19070 recon_loss: 0.1733561009168625 \tdiscriminator_loss: 1.245964765548706 \tgenerator_loss: 0.9648663997650146\n",
            "For Step: 19080 recon_loss: 0.1679167002439499 \tdiscriminator_loss: 1.211327075958252 \tgenerator_loss: 0.9582074880599976\n",
            "For Step: 19090 recon_loss: 0.16267962753772736 \tdiscriminator_loss: 1.2077372074127197 \tgenerator_loss: 0.905931830406189\n",
            "For Step: 19100 recon_loss: 0.17794004082679749 \tdiscriminator_loss: 1.1926039457321167 \tgenerator_loss: 0.9340630769729614\n",
            "For Step: 19110 recon_loss: 0.17409031093120575 \tdiscriminator_loss: 1.1772657632827759 \tgenerator_loss: 0.916486918926239\n",
            "For Step: 19120 recon_loss: 0.19095054268836975 \tdiscriminator_loss: 1.193847417831421 \tgenerator_loss: 0.9232380390167236\n",
            "For Step: 19130 recon_loss: 0.17001661658287048 \tdiscriminator_loss: 1.1666932106018066 \tgenerator_loss: 0.9987345933914185\n",
            "For Step: 19140 recon_loss: 0.17427824437618256 \tdiscriminator_loss: 1.2025792598724365 \tgenerator_loss: 1.0055797100067139\n",
            "For Step: 19150 recon_loss: 0.18578346073627472 \tdiscriminator_loss: 1.2203863859176636 \tgenerator_loss: 0.8963807821273804\n",
            "For Step: 19160 recon_loss: 0.17701302468776703 \tdiscriminator_loss: 1.312689185142517 \tgenerator_loss: 0.8705188035964966\n",
            "For Step: 19170 recon_loss: 0.19239559769630432 \tdiscriminator_loss: 1.2487640380859375 \tgenerator_loss: 1.0054206848144531\n",
            "For Step: 19180 recon_loss: 0.17388103902339935 \tdiscriminator_loss: 1.2461411952972412 \tgenerator_loss: 0.8964552283287048\n",
            "For Step: 19190 recon_loss: 0.17450839281082153 \tdiscriminator_loss: 1.2355549335479736 \tgenerator_loss: 0.9310299158096313\n",
            "For Step: 19200 recon_loss: 0.17775095999240875 \tdiscriminator_loss: 1.213602066040039 \tgenerator_loss: 0.9530897736549377\n",
            "For Step: 19210 recon_loss: 0.18576806783676147 \tdiscriminator_loss: 1.1670091152191162 \tgenerator_loss: 0.9902451038360596\n",
            "For Step: 19220 recon_loss: 0.17330379784107208 \tdiscriminator_loss: 1.2468773126602173 \tgenerator_loss: 0.9946574568748474\n",
            "For Step: 19230 recon_loss: 0.1686042696237564 \tdiscriminator_loss: 1.1814074516296387 \tgenerator_loss: 0.9592465162277222\n",
            "For Step: 19240 recon_loss: 0.1667989045381546 \tdiscriminator_loss: 1.1489126682281494 \tgenerator_loss: 1.0273640155792236\n",
            "For Step: 19250 recon_loss: 0.17068028450012207 \tdiscriminator_loss: 1.19825279712677 \tgenerator_loss: 0.913719892501831\n",
            "For Step: 19260 recon_loss: 0.18028800189495087 \tdiscriminator_loss: 1.2253023386001587 \tgenerator_loss: 0.9176113605499268\n",
            "For Step: 19270 recon_loss: 0.16486121714115143 \tdiscriminator_loss: 1.1620593070983887 \tgenerator_loss: 0.9102899432182312\n",
            "For Step: 19280 recon_loss: 0.17628750205039978 \tdiscriminator_loss: 1.1058614253997803 \tgenerator_loss: 0.9296334981918335\n",
            "For Step: 19290 recon_loss: 0.17016176879405975 \tdiscriminator_loss: 1.1241765022277832 \tgenerator_loss: 0.9305164813995361\n",
            "For Step: 19300 recon_loss: 0.18410435318946838 \tdiscriminator_loss: 1.163880705833435 \tgenerator_loss: 0.8884156942367554\n",
            "For Step: 19310 recon_loss: 0.17690803110599518 \tdiscriminator_loss: 1.045178771018982 \tgenerator_loss: 0.9721851348876953\n",
            "For Step: 19320 recon_loss: 0.1693454533815384 \tdiscriminator_loss: 1.2124345302581787 \tgenerator_loss: 0.953532338142395\n",
            "For Step: 19330 recon_loss: 0.1525120586156845 \tdiscriminator_loss: 1.1762080192565918 \tgenerator_loss: 0.8788928985595703\n",
            "For Step: 19340 recon_loss: 0.1744866669178009 \tdiscriminator_loss: 1.0922825336456299 \tgenerator_loss: 0.9415627717971802\n",
            "For Step: 19350 recon_loss: 0.17225034534931183 \tdiscriminator_loss: 1.1091549396514893 \tgenerator_loss: 0.9929265379905701\n",
            "For Step: 19360 recon_loss: 0.16461297869682312 \tdiscriminator_loss: 1.0390119552612305 \tgenerator_loss: 1.016800880432129\n",
            "For Step: 19370 recon_loss: 0.18066145479679108 \tdiscriminator_loss: 1.1086170673370361 \tgenerator_loss: 0.9587452411651611\n",
            "For Step: 19380 recon_loss: 0.17159901559352875 \tdiscriminator_loss: 1.2981114387512207 \tgenerator_loss: 0.9663800001144409\n",
            "For Step: 19390 recon_loss: 0.17245367169380188 \tdiscriminator_loss: 1.187842607498169 \tgenerator_loss: 1.004388689994812\n",
            "For Step: 19400 recon_loss: 0.173669695854187 \tdiscriminator_loss: 1.2617366313934326 \tgenerator_loss: 0.9431453943252563\n",
            "For Step: 19410 recon_loss: 0.1754804402589798 \tdiscriminator_loss: 1.1464664936065674 \tgenerator_loss: 0.9918258786201477\n",
            "For Step: 19420 recon_loss: 0.18328449130058289 \tdiscriminator_loss: 1.287373423576355 \tgenerator_loss: 0.985651969909668\n",
            "For Step: 19430 recon_loss: 0.17462146282196045 \tdiscriminator_loss: 1.102780818939209 \tgenerator_loss: 1.0222649574279785\n",
            "For Step: 19440 recon_loss: 0.17631806433200836 \tdiscriminator_loss: 1.1503589153289795 \tgenerator_loss: 0.9516552686691284\n",
            "For Step: 19450 recon_loss: 0.17827214300632477 \tdiscriminator_loss: 1.1879253387451172 \tgenerator_loss: 0.9737555384635925\n",
            "For Step: 19460 recon_loss: 0.1676185429096222 \tdiscriminator_loss: 1.164860725402832 \tgenerator_loss: 0.9665454626083374\n",
            "For Step: 19470 recon_loss: 0.1779072880744934 \tdiscriminator_loss: 1.1937472820281982 \tgenerator_loss: 1.0051848888397217\n",
            "For Step: 19480 recon_loss: 0.17024701833724976 \tdiscriminator_loss: 1.0593494176864624 \tgenerator_loss: 0.9805808067321777\n",
            "For Step: 19490 recon_loss: 0.1852094531059265 \tdiscriminator_loss: 1.073716163635254 \tgenerator_loss: 1.0366952419281006\n",
            "For Step: 19500 recon_loss: 0.17991237342357635 \tdiscriminator_loss: 1.271418809890747 \tgenerator_loss: 0.8923735022544861\n",
            "For Step: 19510 recon_loss: 0.18378551304340363 \tdiscriminator_loss: 1.1504714488983154 \tgenerator_loss: 0.9760875701904297\n",
            "For Step: 19520 recon_loss: 0.1807766705751419 \tdiscriminator_loss: 1.2662273645401 \tgenerator_loss: 0.9384462833404541\n",
            "For Step: 19530 recon_loss: 0.18111707270145416 \tdiscriminator_loss: 1.2950398921966553 \tgenerator_loss: 0.9192657470703125\n",
            "For Step: 19540 recon_loss: 0.17967045307159424 \tdiscriminator_loss: 1.3026787042617798 \tgenerator_loss: 0.9464224576950073\n",
            "For Step: 19550 recon_loss: 0.17899595201015472 \tdiscriminator_loss: 1.12823486328125 \tgenerator_loss: 0.9298145771026611\n",
            "For Step: 19560 recon_loss: 0.18789774179458618 \tdiscriminator_loss: 1.0858927965164185 \tgenerator_loss: 1.010050892829895\n",
            "For Step: 19570 recon_loss: 0.1758430451154709 \tdiscriminator_loss: 1.1365944147109985 \tgenerator_loss: 0.9277040362358093\n",
            "For Step: 19580 recon_loss: 0.17891603708267212 \tdiscriminator_loss: 1.280516266822815 \tgenerator_loss: 0.941290020942688\n",
            "For Step: 19590 recon_loss: 0.17698177695274353 \tdiscriminator_loss: 1.2190251350402832 \tgenerator_loss: 1.0066628456115723\n",
            "For Step: 19600 recon_loss: 0.16589190065860748 \tdiscriminator_loss: 1.3538551330566406 \tgenerator_loss: 0.9090816378593445\n",
            "For Step: 19610 recon_loss: 0.17385032773017883 \tdiscriminator_loss: 1.1672108173370361 \tgenerator_loss: 0.9205411672592163\n",
            "For Step: 19620 recon_loss: 0.1812317967414856 \tdiscriminator_loss: 1.225720763206482 \tgenerator_loss: 0.9517641067504883\n",
            "For Step: 19630 recon_loss: 0.17200051248073578 \tdiscriminator_loss: 1.1876122951507568 \tgenerator_loss: 0.9430684447288513\n",
            "For Step: 19640 recon_loss: 0.17645587027072906 \tdiscriminator_loss: 1.2135295867919922 \tgenerator_loss: 0.9003657102584839\n",
            "For Step: 19650 recon_loss: 0.1575777679681778 \tdiscriminator_loss: 1.1507327556610107 \tgenerator_loss: 0.9788023233413696\n",
            "For Step: 19660 recon_loss: 0.1795034259557724 \tdiscriminator_loss: 1.255152940750122 \tgenerator_loss: 0.9507339000701904\n",
            "For Step: 19670 recon_loss: 0.17189013957977295 \tdiscriminator_loss: 1.2932631969451904 \tgenerator_loss: 0.9826116561889648\n",
            "For Step: 19680 recon_loss: 0.1815040111541748 \tdiscriminator_loss: 1.1636362075805664 \tgenerator_loss: 0.9257801175117493\n",
            "For Step: 19690 recon_loss: 0.17338770627975464 \tdiscriminator_loss: 1.1414073705673218 \tgenerator_loss: 1.0252456665039062\n",
            "For Step: 19700 recon_loss: 0.16720804572105408 \tdiscriminator_loss: 1.1733102798461914 \tgenerator_loss: 0.9800878167152405\n",
            "For Step: 19710 recon_loss: 0.18066780269145966 \tdiscriminator_loss: 1.223917007446289 \tgenerator_loss: 0.9729963541030884\n",
            "For Step: 19720 recon_loss: 0.16721445322036743 \tdiscriminator_loss: 1.1174523830413818 \tgenerator_loss: 1.0205427408218384\n",
            "For Step: 19730 recon_loss: 0.17147214710712433 \tdiscriminator_loss: 1.2396764755249023 \tgenerator_loss: 0.8919910192489624\n",
            "For Step: 19740 recon_loss: 0.17559853196144104 \tdiscriminator_loss: 1.1917684078216553 \tgenerator_loss: 0.8978937864303589\n",
            "For Step: 19750 recon_loss: 0.17618225514888763 \tdiscriminator_loss: 1.2235136032104492 \tgenerator_loss: 0.9269171953201294\n",
            "For Step: 19760 recon_loss: 0.17304688692092896 \tdiscriminator_loss: 1.3475691080093384 \tgenerator_loss: 0.8997332453727722\n",
            "For Step: 19770 recon_loss: 0.17963804304599762 \tdiscriminator_loss: 1.1416096687316895 \tgenerator_loss: 0.9370231628417969\n",
            "For Step: 19780 recon_loss: 0.168410986661911 \tdiscriminator_loss: 1.0209898948669434 \tgenerator_loss: 0.9707202315330505\n",
            "For Step: 19790 recon_loss: 0.17364077270030975 \tdiscriminator_loss: 1.2431614398956299 \tgenerator_loss: 0.9853798151016235\n",
            "For Step: 19800 recon_loss: 0.1676144152879715 \tdiscriminator_loss: 1.2118229866027832 \tgenerator_loss: 0.9319795966148376\n",
            "For Step: 19810 recon_loss: 0.1780548244714737 \tdiscriminator_loss: 1.149307370185852 \tgenerator_loss: 0.9590120315551758\n",
            "For Step: 19820 recon_loss: 0.16755454242229462 \tdiscriminator_loss: 1.1760239601135254 \tgenerator_loss: 0.9510272741317749\n",
            "For Step: 19830 recon_loss: 0.17241355776786804 \tdiscriminator_loss: 1.2438879013061523 \tgenerator_loss: 0.9156397581100464\n",
            "For Step: 19840 recon_loss: 0.16211654245853424 \tdiscriminator_loss: 1.1709980964660645 \tgenerator_loss: 0.9134615659713745\n",
            "For Step: 19850 recon_loss: 0.16813534498214722 \tdiscriminator_loss: 1.2013583183288574 \tgenerator_loss: 1.0116177797317505\n",
            "For Step: 19860 recon_loss: 0.1833939254283905 \tdiscriminator_loss: 1.198317289352417 \tgenerator_loss: 0.9840399622917175\n",
            "For Step: 19870 recon_loss: 0.18774211406707764 \tdiscriminator_loss: 1.2219895124435425 \tgenerator_loss: 0.9939354062080383\n",
            "For Step: 19880 recon_loss: 0.17697371542453766 \tdiscriminator_loss: 1.2218291759490967 \tgenerator_loss: 0.9524072408676147\n",
            "For Step: 19890 recon_loss: 0.16649755835533142 \tdiscriminator_loss: 1.2075886726379395 \tgenerator_loss: 0.9367208480834961\n",
            "For Step: 19900 recon_loss: 0.18904203176498413 \tdiscriminator_loss: 1.1310824155807495 \tgenerator_loss: 0.9891662001609802\n",
            "For Step: 19910 recon_loss: 0.16199104487895966 \tdiscriminator_loss: 1.2443970441818237 \tgenerator_loss: 0.9281582832336426\n",
            "For Step: 19920 recon_loss: 0.19481760263442993 \tdiscriminator_loss: 1.1143581867218018 \tgenerator_loss: 0.9726647734642029\n",
            "For Step: 19930 recon_loss: 0.15888360142707825 \tdiscriminator_loss: 1.1476283073425293 \tgenerator_loss: 0.9191624522209167\n",
            "For Step: 19940 recon_loss: 0.18247106671333313 \tdiscriminator_loss: 1.2297604084014893 \tgenerator_loss: 0.8805831670761108\n",
            "For Step: 19950 recon_loss: 0.17182134091854095 \tdiscriminator_loss: 1.2806987762451172 \tgenerator_loss: 0.9815502166748047\n",
            "For Step: 19960 recon_loss: 0.17176489531993866 \tdiscriminator_loss: 1.2408112287521362 \tgenerator_loss: 0.9732850790023804\n",
            "For Step: 19970 recon_loss: 0.1800096035003662 \tdiscriminator_loss: 1.2916278839111328 \tgenerator_loss: 0.9744583964347839\n",
            "For Step: 19980 recon_loss: 0.1781126856803894 \tdiscriminator_loss: 1.1148700714111328 \tgenerator_loss: 0.9664559364318848\n",
            "For Step: 19990 recon_loss: 0.17594337463378906 \tdiscriminator_loss: 1.2045080661773682 \tgenerator_loss: 0.9353796243667603\n",
            "For Step: 20000 recon_loss: 0.1652202159166336 \tdiscriminator_loss: 1.0171101093292236 \tgenerator_loss: 1.0788897275924683\n",
            "For Step: 20010 recon_loss: 0.17673522233963013 \tdiscriminator_loss: 1.1793153285980225 \tgenerator_loss: 0.9529271125793457\n",
            "For Step: 20020 recon_loss: 0.1699191927909851 \tdiscriminator_loss: 1.198326826095581 \tgenerator_loss: 0.966554582118988\n",
            "For Step: 20030 recon_loss: 0.17420776188373566 \tdiscriminator_loss: 1.2517027854919434 \tgenerator_loss: 0.9086261987686157\n",
            "For Step: 20040 recon_loss: 0.17215505242347717 \tdiscriminator_loss: 1.235609531402588 \tgenerator_loss: 0.990760087966919\n",
            "For Step: 20050 recon_loss: 0.17023904621601105 \tdiscriminator_loss: 1.1391149759292603 \tgenerator_loss: 1.012917160987854\n",
            "For Step: 20060 recon_loss: 0.18082542717456818 \tdiscriminator_loss: 1.287532091140747 \tgenerator_loss: 0.9554319381713867\n",
            "For Step: 20070 recon_loss: 0.18277345597743988 \tdiscriminator_loss: 1.2526893615722656 \tgenerator_loss: 0.9057295918464661\n",
            "For Step: 20080 recon_loss: 0.17149882018566132 \tdiscriminator_loss: 1.2112940549850464 \tgenerator_loss: 0.9929099082946777\n",
            "For Step: 20090 recon_loss: 0.18715839087963104 \tdiscriminator_loss: 1.1190882921218872 \tgenerator_loss: 1.0184576511383057\n",
            "For Step: 20100 recon_loss: 0.17894844710826874 \tdiscriminator_loss: 1.322008490562439 \tgenerator_loss: 0.9597049951553345\n",
            "For Step: 20110 recon_loss: 0.17039617896080017 \tdiscriminator_loss: 1.1969225406646729 \tgenerator_loss: 0.9266027212142944\n",
            "For Step: 20120 recon_loss: 0.18194910883903503 \tdiscriminator_loss: 1.2630465030670166 \tgenerator_loss: 0.8934858441352844\n",
            "For Step: 20130 recon_loss: 0.17190983891487122 \tdiscriminator_loss: 1.0078692436218262 \tgenerator_loss: 1.013840913772583\n",
            "For Step: 20140 recon_loss: 0.18512102961540222 \tdiscriminator_loss: 1.3077317476272583 \tgenerator_loss: 0.9558343887329102\n",
            "For Step: 20150 recon_loss: 0.17222987115383148 \tdiscriminator_loss: 1.3219854831695557 \tgenerator_loss: 1.0065453052520752\n",
            "For Step: 20160 recon_loss: 0.16059397161006927 \tdiscriminator_loss: 1.357128381729126 \tgenerator_loss: 0.9138169288635254\n",
            "For Step: 20170 recon_loss: 0.17116452753543854 \tdiscriminator_loss: 1.2298259735107422 \tgenerator_loss: 0.9827771186828613\n",
            "For Step: 20180 recon_loss: 0.1766345202922821 \tdiscriminator_loss: 1.3056349754333496 \tgenerator_loss: 0.8867754340171814\n",
            "For Step: 20190 recon_loss: 0.17329418659210205 \tdiscriminator_loss: 1.2854855060577393 \tgenerator_loss: 0.8980468511581421\n",
            "For Step: 20200 recon_loss: 0.17086778581142426 \tdiscriminator_loss: 1.2608182430267334 \tgenerator_loss: 0.9952272772789001\n",
            "For Step: 20210 recon_loss: 0.1750948578119278 \tdiscriminator_loss: 1.2760998010635376 \tgenerator_loss: 0.9223322868347168\n",
            "For Step: 20220 recon_loss: 0.16401124000549316 \tdiscriminator_loss: 1.1473801136016846 \tgenerator_loss: 0.9590725898742676\n",
            "For Step: 20230 recon_loss: 0.17184020578861237 \tdiscriminator_loss: 1.1385120153427124 \tgenerator_loss: 0.9793097972869873\n",
            "For Step: 20240 recon_loss: 0.17887350916862488 \tdiscriminator_loss: 1.2233612537384033 \tgenerator_loss: 0.9457573294639587\n",
            "For Step: 20250 recon_loss: 0.1771480143070221 \tdiscriminator_loss: 1.070047378540039 \tgenerator_loss: 1.0342578887939453\n",
            "For Step: 20260 recon_loss: 0.19052182137966156 \tdiscriminator_loss: 1.1235885620117188 \tgenerator_loss: 0.9324289560317993\n",
            "For Step: 20270 recon_loss: 0.17895427346229553 \tdiscriminator_loss: 1.1607789993286133 \tgenerator_loss: 0.9630638957023621\n",
            "For Step: 20280 recon_loss: 0.1745339184999466 \tdiscriminator_loss: 1.2624988555908203 \tgenerator_loss: 0.9695920348167419\n",
            "For Step: 20290 recon_loss: 0.18605874478816986 \tdiscriminator_loss: 1.2565124034881592 \tgenerator_loss: 1.0010926723480225\n",
            "For Step: 20300 recon_loss: 0.17522664368152618 \tdiscriminator_loss: 1.183039665222168 \tgenerator_loss: 0.9855340719223022\n",
            "For Step: 20310 recon_loss: 0.16166073083877563 \tdiscriminator_loss: 1.2347358465194702 \tgenerator_loss: 0.9518059492111206\n",
            "For Step: 20320 recon_loss: 0.18322163820266724 \tdiscriminator_loss: 1.1703567504882812 \tgenerator_loss: 0.8448785543441772\n",
            "For Step: 20330 recon_loss: 0.1828482449054718 \tdiscriminator_loss: 1.1150223016738892 \tgenerator_loss: 0.9962663650512695\n",
            "For Step: 20340 recon_loss: 0.1818600296974182 \tdiscriminator_loss: 1.1861603260040283 \tgenerator_loss: 0.995541512966156\n",
            "For Step: 20350 recon_loss: 0.17863211035728455 \tdiscriminator_loss: 1.1068278551101685 \tgenerator_loss: 1.0030335187911987\n",
            "For Step: 20360 recon_loss: 0.18320809304714203 \tdiscriminator_loss: 1.2276456356048584 \tgenerator_loss: 1.0360742807388306\n",
            "For Step: 20370 recon_loss: 0.16931204497814178 \tdiscriminator_loss: 1.2381806373596191 \tgenerator_loss: 0.9382688403129578\n",
            "For Step: 20380 recon_loss: 0.18847733736038208 \tdiscriminator_loss: 1.1990118026733398 \tgenerator_loss: 0.9490998983383179\n",
            "For Step: 20390 recon_loss: 0.17022667825222015 \tdiscriminator_loss: 1.1489564180374146 \tgenerator_loss: 0.9359918832778931\n",
            "For Step: 20400 recon_loss: 0.17216803133487701 \tdiscriminator_loss: 1.1984214782714844 \tgenerator_loss: 0.8937332034111023\n",
            "For Step: 20410 recon_loss: 0.17906108498573303 \tdiscriminator_loss: 1.2226886749267578 \tgenerator_loss: 0.9833618402481079\n",
            "For Step: 20420 recon_loss: 0.17081420123577118 \tdiscriminator_loss: 1.216282606124878 \tgenerator_loss: 0.9420896768569946\n",
            "For Step: 20430 recon_loss: 0.17792391777038574 \tdiscriminator_loss: 1.1721322536468506 \tgenerator_loss: 0.9621844291687012\n",
            "For Step: 20440 recon_loss: 0.18480142951011658 \tdiscriminator_loss: 1.1432863473892212 \tgenerator_loss: 1.0077183246612549\n",
            "For Step: 20450 recon_loss: 0.18157456815242767 \tdiscriminator_loss: 1.1427507400512695 \tgenerator_loss: 0.9549056887626648\n",
            "For Step: 20460 recon_loss: 0.16750070452690125 \tdiscriminator_loss: 1.305287480354309 \tgenerator_loss: 0.9418109655380249\n",
            "For Step: 20470 recon_loss: 0.15755194425582886 \tdiscriminator_loss: 1.2634668350219727 \tgenerator_loss: 0.9084736704826355\n",
            "For Step: 20480 recon_loss: 0.17491543292999268 \tdiscriminator_loss: 1.2405927181243896 \tgenerator_loss: 0.9406365156173706\n",
            "For Step: 20490 recon_loss: 0.1758982539176941 \tdiscriminator_loss: 1.2652904987335205 \tgenerator_loss: 0.9133366346359253\n",
            "For Step: 20500 recon_loss: 0.18341459333896637 \tdiscriminator_loss: 1.202345371246338 \tgenerator_loss: 0.9857319593429565\n",
            "For Step: 20510 recon_loss: 0.1754208207130432 \tdiscriminator_loss: 1.1566120386123657 \tgenerator_loss: 0.9903549551963806\n",
            "For Step: 20520 recon_loss: 0.17602267861366272 \tdiscriminator_loss: 1.1134616136550903 \tgenerator_loss: 1.0018125772476196\n",
            "For Step: 20530 recon_loss: 0.1751142293214798 \tdiscriminator_loss: 1.2184070348739624 \tgenerator_loss: 0.9788110852241516\n",
            "For Step: 20540 recon_loss: 0.17179283499717712 \tdiscriminator_loss: 1.1842231750488281 \tgenerator_loss: 0.9599533081054688\n",
            "For Step: 20550 recon_loss: 0.17317259311676025 \tdiscriminator_loss: 1.2449727058410645 \tgenerator_loss: 0.9182525873184204\n",
            "For Step: 20560 recon_loss: 0.17937776446342468 \tdiscriminator_loss: 1.134279727935791 \tgenerator_loss: 0.9014370441436768\n",
            "For Step: 20570 recon_loss: 0.16038233041763306 \tdiscriminator_loss: 1.157327651977539 \tgenerator_loss: 0.9902381896972656\n",
            "For Step: 20580 recon_loss: 0.1651339828968048 \tdiscriminator_loss: 1.1660090684890747 \tgenerator_loss: 0.9591400623321533\n",
            "For Step: 20590 recon_loss: 0.18110086023807526 \tdiscriminator_loss: 1.1873530149459839 \tgenerator_loss: 0.9360436201095581\n",
            "For Step: 20600 recon_loss: 0.17386175692081451 \tdiscriminator_loss: 1.3068499565124512 \tgenerator_loss: 0.9012272357940674\n",
            "For Step: 20610 recon_loss: 0.15915289521217346 \tdiscriminator_loss: 1.1425392627716064 \tgenerator_loss: 0.99110347032547\n",
            "For Step: 20620 recon_loss: 0.18679440021514893 \tdiscriminator_loss: 1.2371325492858887 \tgenerator_loss: 0.9476591944694519\n",
            "For Step: 20630 recon_loss: 0.17567038536071777 \tdiscriminator_loss: 1.2865521907806396 \tgenerator_loss: 0.8973212242126465\n",
            "For Step: 20640 recon_loss: 0.17221581935882568 \tdiscriminator_loss: 1.0949807167053223 \tgenerator_loss: 0.9601947069168091\n",
            "For Step: 20650 recon_loss: 0.1775214672088623 \tdiscriminator_loss: 1.236461877822876 \tgenerator_loss: 0.9013751745223999\n",
            "For Step: 20660 recon_loss: 0.17451366782188416 \tdiscriminator_loss: 1.2438201904296875 \tgenerator_loss: 0.9319478273391724\n",
            "For Step: 20670 recon_loss: 0.18636803328990936 \tdiscriminator_loss: 1.0657037496566772 \tgenerator_loss: 1.01283597946167\n",
            "For Step: 20680 recon_loss: 0.18126030266284943 \tdiscriminator_loss: 1.1974482536315918 \tgenerator_loss: 0.9952495098114014\n",
            "For Step: 20690 recon_loss: 0.17853166162967682 \tdiscriminator_loss: 1.2097246646881104 \tgenerator_loss: 0.8814278244972229\n",
            "For Step: 20700 recon_loss: 0.17408190667629242 \tdiscriminator_loss: 1.19803786277771 \tgenerator_loss: 0.9712481498718262\n",
            "For Step: 20710 recon_loss: 0.16699884831905365 \tdiscriminator_loss: 1.3092272281646729 \tgenerator_loss: 0.8894187211990356\n",
            "For Step: 20720 recon_loss: 0.17168748378753662 \tdiscriminator_loss: 1.223308801651001 \tgenerator_loss: 0.9464183449745178\n",
            "For Step: 20730 recon_loss: 0.18261577188968658 \tdiscriminator_loss: 1.2464951276779175 \tgenerator_loss: 0.8894752860069275\n",
            "For Step: 20740 recon_loss: 0.16492411494255066 \tdiscriminator_loss: 1.2154051065444946 \tgenerator_loss: 0.9372149705886841\n",
            "For Step: 20750 recon_loss: 0.18573704361915588 \tdiscriminator_loss: 1.1427581310272217 \tgenerator_loss: 0.8811321258544922\n",
            "For Step: 20760 recon_loss: 0.19302327930927277 \tdiscriminator_loss: 1.1549389362335205 \tgenerator_loss: 1.0094728469848633\n",
            "For Step: 20770 recon_loss: 0.16647104918956757 \tdiscriminator_loss: 1.1042251586914062 \tgenerator_loss: 0.9807995557785034\n",
            "For Step: 20780 recon_loss: 0.1688908189535141 \tdiscriminator_loss: 1.0866546630859375 \tgenerator_loss: 0.8921528458595276\n",
            "For Step: 20790 recon_loss: 0.17211061716079712 \tdiscriminator_loss: 1.1947417259216309 \tgenerator_loss: 0.9861379861831665\n",
            "For Step: 20800 recon_loss: 0.18233820796012878 \tdiscriminator_loss: 1.1631097793579102 \tgenerator_loss: 0.9614155292510986\n",
            "For Step: 20810 recon_loss: 0.1788141131401062 \tdiscriminator_loss: 1.1691457033157349 \tgenerator_loss: 0.9313812851905823\n",
            "For Step: 20820 recon_loss: 0.1716901957988739 \tdiscriminator_loss: 1.1432571411132812 \tgenerator_loss: 0.9443469047546387\n",
            "For Step: 20830 recon_loss: 0.18275533616542816 \tdiscriminator_loss: 1.185083031654358 \tgenerator_loss: 0.9351657032966614\n",
            "For Step: 20840 recon_loss: 0.17772357165813446 \tdiscriminator_loss: 1.1881792545318604 \tgenerator_loss: 0.9706553220748901\n",
            "For Step: 20850 recon_loss: 0.17804710566997528 \tdiscriminator_loss: 1.1908438205718994 \tgenerator_loss: 0.9838355779647827\n",
            "For Step: 20860 recon_loss: 0.1700172871351242 \tdiscriminator_loss: 1.2097103595733643 \tgenerator_loss: 0.9581798911094666\n",
            "For Step: 20870 recon_loss: 0.16950953006744385 \tdiscriminator_loss: 1.2358661890029907 \tgenerator_loss: 0.96563720703125\n",
            "For Step: 20880 recon_loss: 0.17258238792419434 \tdiscriminator_loss: 1.3042402267456055 \tgenerator_loss: 0.9444010257720947\n",
            "For Step: 20890 recon_loss: 0.17345945537090302 \tdiscriminator_loss: 1.129692554473877 \tgenerator_loss: 0.9705249071121216\n",
            "For Step: 20900 recon_loss: 0.18566590547561646 \tdiscriminator_loss: 1.1482398509979248 \tgenerator_loss: 0.9584211111068726\n",
            "For Step: 20910 recon_loss: 0.18296624720096588 \tdiscriminator_loss: 1.2625443935394287 \tgenerator_loss: 0.9500278234481812\n",
            "For Step: 20920 recon_loss: 0.1721137911081314 \tdiscriminator_loss: 1.2795472145080566 \tgenerator_loss: 0.8952908515930176\n",
            "For Step: 20930 recon_loss: 0.18132951855659485 \tdiscriminator_loss: 1.2325999736785889 \tgenerator_loss: 0.8876269459724426\n",
            "For Step: 20940 recon_loss: 0.17958760261535645 \tdiscriminator_loss: 1.262651801109314 \tgenerator_loss: 0.8851314783096313\n",
            "For Step: 20950 recon_loss: 0.1558416336774826 \tdiscriminator_loss: 1.2790155410766602 \tgenerator_loss: 0.9089055061340332\n",
            "For Step: 20960 recon_loss: 0.18152832984924316 \tdiscriminator_loss: 1.256882667541504 \tgenerator_loss: 0.9315551519393921\n",
            "For Step: 20970 recon_loss: 0.177016943693161 \tdiscriminator_loss: 1.0843567848205566 \tgenerator_loss: 0.9873220324516296\n",
            "For Step: 20980 recon_loss: 0.17277763783931732 \tdiscriminator_loss: 1.1443085670471191 \tgenerator_loss: 0.9365271329879761\n",
            "For Step: 20990 recon_loss: 0.18127933144569397 \tdiscriminator_loss: 1.2011361122131348 \tgenerator_loss: 0.9373791217803955\n",
            "For Step: 21000 recon_loss: 0.16560769081115723 \tdiscriminator_loss: 1.2146275043487549 \tgenerator_loss: 0.8749322891235352\n",
            "For Step: 21010 recon_loss: 0.1633806973695755 \tdiscriminator_loss: 1.2511202096939087 \tgenerator_loss: 0.9683183431625366\n",
            "For Step: 21020 recon_loss: 0.1691419780254364 \tdiscriminator_loss: 1.204411268234253 \tgenerator_loss: 0.9328157901763916\n",
            "For Step: 21030 recon_loss: 0.17420798540115356 \tdiscriminator_loss: 1.164857029914856 \tgenerator_loss: 0.9858793616294861\n",
            "For Step: 21040 recon_loss: 0.19082245230674744 \tdiscriminator_loss: 1.1456834077835083 \tgenerator_loss: 0.937980592250824\n",
            "For Step: 21050 recon_loss: 0.18706296384334564 \tdiscriminator_loss: 1.2105741500854492 \tgenerator_loss: 0.9088165760040283\n",
            "For Step: 21060 recon_loss: 0.181946262717247 \tdiscriminator_loss: 1.3031666278839111 \tgenerator_loss: 0.9274143576622009\n",
            "For Step: 21070 recon_loss: 0.17895545065402985 \tdiscriminator_loss: 1.0778398513793945 \tgenerator_loss: 0.9128785133361816\n",
            "For Step: 21080 recon_loss: 0.1769648641347885 \tdiscriminator_loss: 1.3033926486968994 \tgenerator_loss: 0.8680802583694458\n",
            "For Step: 21090 recon_loss: 0.18203602731227875 \tdiscriminator_loss: 1.2123973369598389 \tgenerator_loss: 0.9537971615791321\n",
            "For Step: 21100 recon_loss: 0.17250050604343414 \tdiscriminator_loss: 1.1288111209869385 \tgenerator_loss: 0.9635670185089111\n",
            "For Step: 21110 recon_loss: 0.17947539687156677 \tdiscriminator_loss: 1.1045300960540771 \tgenerator_loss: 0.9607219696044922\n",
            "For Step: 21120 recon_loss: 0.16493913531303406 \tdiscriminator_loss: 1.2054272890090942 \tgenerator_loss: 0.9099479913711548\n",
            "For Step: 21130 recon_loss: 0.17941731214523315 \tdiscriminator_loss: 1.2410252094268799 \tgenerator_loss: 0.9724447131156921\n",
            "For Step: 21140 recon_loss: 0.16879117488861084 \tdiscriminator_loss: 1.1908233165740967 \tgenerator_loss: 1.0165444612503052\n",
            "For Step: 21150 recon_loss: 0.16812698543071747 \tdiscriminator_loss: 1.2384676933288574 \tgenerator_loss: 0.9224857091903687\n",
            "For Step: 21160 recon_loss: 0.18438759446144104 \tdiscriminator_loss: 1.3693262338638306 \tgenerator_loss: 0.8860276937484741\n",
            "For Step: 21170 recon_loss: 0.18251639604568481 \tdiscriminator_loss: 1.1993778944015503 \tgenerator_loss: 0.9740244746208191\n",
            "For Step: 21180 recon_loss: 0.17288246750831604 \tdiscriminator_loss: 1.1848533153533936 \tgenerator_loss: 0.907259464263916\n",
            "For Step: 21190 recon_loss: 0.17797166109085083 \tdiscriminator_loss: 1.193272352218628 \tgenerator_loss: 1.0129727125167847\n",
            "For Step: 21200 recon_loss: 0.17205116152763367 \tdiscriminator_loss: 1.102212905883789 \tgenerator_loss: 0.9905649423599243\n",
            "For Step: 21210 recon_loss: 0.17870046198368073 \tdiscriminator_loss: 1.2581963539123535 \tgenerator_loss: 0.9230902194976807\n",
            "For Step: 21220 recon_loss: 0.16768479347229004 \tdiscriminator_loss: 1.1943682432174683 \tgenerator_loss: 0.9958454966545105\n",
            "For Step: 21230 recon_loss: 0.1781928837299347 \tdiscriminator_loss: 1.2782362699508667 \tgenerator_loss: 0.8874252438545227\n",
            "For Step: 21240 recon_loss: 0.1697758287191391 \tdiscriminator_loss: 1.1815727949142456 \tgenerator_loss: 0.934988260269165\n",
            "For Step: 21250 recon_loss: 0.17224901914596558 \tdiscriminator_loss: 1.1803524494171143 \tgenerator_loss: 0.9574253559112549\n",
            "For Step: 21260 recon_loss: 0.18067088723182678 \tdiscriminator_loss: 1.246741533279419 \tgenerator_loss: 0.9279321432113647\n",
            "For Step: 21270 recon_loss: 0.1732117384672165 \tdiscriminator_loss: 1.2457548379898071 \tgenerator_loss: 0.9574195742607117\n",
            "For Step: 21280 recon_loss: 0.17132645845413208 \tdiscriminator_loss: 1.1880731582641602 \tgenerator_loss: 0.968567967414856\n",
            "For Step: 21290 recon_loss: 0.16363003849983215 \tdiscriminator_loss: 1.2773282527923584 \tgenerator_loss: 0.9234472513198853\n",
            "For Step: 21300 recon_loss: 0.17587997019290924 \tdiscriminator_loss: 1.2217304706573486 \tgenerator_loss: 0.8646353483200073\n",
            "For Step: 21310 recon_loss: 0.1830766201019287 \tdiscriminator_loss: 1.2723066806793213 \tgenerator_loss: 0.9569059610366821\n",
            "For Step: 21320 recon_loss: 0.19059810042381287 \tdiscriminator_loss: 1.2088918685913086 \tgenerator_loss: 0.9177758693695068\n",
            "For Step: 21330 recon_loss: 0.17592233419418335 \tdiscriminator_loss: 1.2860608100891113 \tgenerator_loss: 0.8781008124351501\n",
            "For Step: 21340 recon_loss: 0.1567256897687912 \tdiscriminator_loss: 1.2807058095932007 \tgenerator_loss: 0.897411584854126\n",
            "For Step: 21350 recon_loss: 0.1856442242860794 \tdiscriminator_loss: 1.2495450973510742 \tgenerator_loss: 0.9238274097442627\n",
            "For Step: 21360 recon_loss: 0.17894718050956726 \tdiscriminator_loss: 1.2653708457946777 \tgenerator_loss: 0.911896288394928\n",
            "For Step: 21370 recon_loss: 0.16570723056793213 \tdiscriminator_loss: 1.1897010803222656 \tgenerator_loss: 0.9001575112342834\n",
            "For Step: 21380 recon_loss: 0.1660483181476593 \tdiscriminator_loss: 1.1017756462097168 \tgenerator_loss: 0.88255375623703\n",
            "For Step: 21390 recon_loss: 0.1879020482301712 \tdiscriminator_loss: 1.202267050743103 \tgenerator_loss: 0.9586596488952637\n",
            "For Step: 21400 recon_loss: 0.16214585304260254 \tdiscriminator_loss: 1.3553577661514282 \tgenerator_loss: 0.8905337452888489\n",
            "For Step: 21410 recon_loss: 0.1686560958623886 \tdiscriminator_loss: 1.1511077880859375 \tgenerator_loss: 0.9878875017166138\n",
            "For Step: 21420 recon_loss: 0.17714877426624298 \tdiscriminator_loss: 1.2321012020111084 \tgenerator_loss: 0.9302496910095215\n",
            "For Step: 21430 recon_loss: 0.17208580672740936 \tdiscriminator_loss: 1.2747998237609863 \tgenerator_loss: 0.9208376407623291\n",
            "For Step: 21440 recon_loss: 0.1665831208229065 \tdiscriminator_loss: 1.309859037399292 \tgenerator_loss: 0.9201318025588989\n",
            "For Step: 21450 recon_loss: 0.1849355697631836 \tdiscriminator_loss: 1.2102044820785522 \tgenerator_loss: 0.9291858673095703\n",
            "For Step: 21460 recon_loss: 0.17254893481731415 \tdiscriminator_loss: 1.182283878326416 \tgenerator_loss: 0.9448677897453308\n",
            "For Step: 21470 recon_loss: 0.16835980117321014 \tdiscriminator_loss: 1.2192103862762451 \tgenerator_loss: 0.9478561878204346\n",
            "For Step: 21480 recon_loss: 0.1661601960659027 \tdiscriminator_loss: 1.2263524532318115 \tgenerator_loss: 0.8866719603538513\n",
            "For Step: 21490 recon_loss: 0.17343470454216003 \tdiscriminator_loss: 1.2796539068222046 \tgenerator_loss: 0.9512346982955933\n",
            "For Step: 21500 recon_loss: 0.18131123483181 \tdiscriminator_loss: 1.2184978723526 \tgenerator_loss: 0.8826121687889099\n",
            "For Step: 21510 recon_loss: 0.16832388937473297 \tdiscriminator_loss: 1.1801812648773193 \tgenerator_loss: 0.961775541305542\n",
            "For Step: 21520 recon_loss: 0.18041153252124786 \tdiscriminator_loss: 1.2015548944473267 \tgenerator_loss: 1.0143568515777588\n",
            "For Step: 21530 recon_loss: 0.17902740836143494 \tdiscriminator_loss: 1.2789331674575806 \tgenerator_loss: 0.9440169334411621\n",
            "For Step: 21540 recon_loss: 0.18302370607852936 \tdiscriminator_loss: 1.1438748836517334 \tgenerator_loss: 0.9662647247314453\n",
            "For Step: 21550 recon_loss: 0.1694195568561554 \tdiscriminator_loss: 1.054051160812378 \tgenerator_loss: 0.9934914112091064\n",
            "For Step: 21560 recon_loss: 0.17317888140678406 \tdiscriminator_loss: 1.2450027465820312 \tgenerator_loss: 0.9741016626358032\n",
            "For Step: 21570 recon_loss: 0.1830310821533203 \tdiscriminator_loss: 1.1959245204925537 \tgenerator_loss: 0.9408959746360779\n",
            "For Step: 21580 recon_loss: 0.16394655406475067 \tdiscriminator_loss: 1.1778391599655151 \tgenerator_loss: 0.9678806066513062\n",
            "For Step: 21590 recon_loss: 0.1750897765159607 \tdiscriminator_loss: 1.2765817642211914 \tgenerator_loss: 0.9090460538864136\n",
            "For Step: 21600 recon_loss: 0.167231947183609 \tdiscriminator_loss: 1.214050054550171 \tgenerator_loss: 1.0092110633850098\n",
            "For Step: 21610 recon_loss: 0.16935773193836212 \tdiscriminator_loss: 1.1131523847579956 \tgenerator_loss: 0.9998971223831177\n",
            "For Step: 21620 recon_loss: 0.17117951810359955 \tdiscriminator_loss: 1.2068220376968384 \tgenerator_loss: 0.8975961804389954\n",
            "For Step: 21630 recon_loss: 0.181672140955925 \tdiscriminator_loss: 1.1618249416351318 \tgenerator_loss: 1.00192129611969\n",
            "For Step: 21640 recon_loss: 0.1735171675682068 \tdiscriminator_loss: 1.0935873985290527 \tgenerator_loss: 0.9687101244926453\n",
            "For Step: 21650 recon_loss: 0.17932461202144623 \tdiscriminator_loss: 1.139955997467041 \tgenerator_loss: 0.971300482749939\n",
            "For Step: 21660 recon_loss: 0.16937097907066345 \tdiscriminator_loss: 1.2054710388183594 \tgenerator_loss: 0.9134041666984558\n",
            "For Step: 21670 recon_loss: 0.16735273599624634 \tdiscriminator_loss: 1.099118947982788 \tgenerator_loss: 0.9765169620513916\n",
            "For Step: 21680 recon_loss: 0.18299931287765503 \tdiscriminator_loss: 1.0991995334625244 \tgenerator_loss: 0.9445152878761292\n",
            "For Step: 21690 recon_loss: 0.17985449731349945 \tdiscriminator_loss: 1.333641529083252 \tgenerator_loss: 0.9231870174407959\n",
            "For Step: 21700 recon_loss: 0.17417706549167633 \tdiscriminator_loss: 1.1360301971435547 \tgenerator_loss: 0.9271372556686401\n",
            "For Step: 21710 recon_loss: 0.1655310094356537 \tdiscriminator_loss: 1.3356269598007202 \tgenerator_loss: 0.8798871040344238\n",
            "For Step: 21720 recon_loss: 0.19203536212444305 \tdiscriminator_loss: 1.1927686929702759 \tgenerator_loss: 0.8918706774711609\n",
            "For Step: 21730 recon_loss: 0.17158496379852295 \tdiscriminator_loss: 1.1836600303649902 \tgenerator_loss: 0.9317064881324768\n",
            "For Step: 21740 recon_loss: 0.17326760292053223 \tdiscriminator_loss: 1.1180243492126465 \tgenerator_loss: 0.8569591641426086\n",
            "For Step: 21750 recon_loss: 0.1765955537557602 \tdiscriminator_loss: 1.3662655353546143 \tgenerator_loss: 0.9091468453407288\n",
            "For Step: 21760 recon_loss: 0.17066223919391632 \tdiscriminator_loss: 1.2398788928985596 \tgenerator_loss: 0.9020265340805054\n",
            "For Step: 21770 recon_loss: 0.17078976333141327 \tdiscriminator_loss: 1.211332082748413 \tgenerator_loss: 0.9297698736190796\n",
            "For Step: 21780 recon_loss: 0.18416035175323486 \tdiscriminator_loss: 1.2803176641464233 \tgenerator_loss: 0.9241883158683777\n",
            "For Step: 21790 recon_loss: 0.18330110609531403 \tdiscriminator_loss: 1.2454419136047363 \tgenerator_loss: 0.9057904481887817\n",
            "For Step: 21800 recon_loss: 0.18453803658485413 \tdiscriminator_loss: 1.2811822891235352 \tgenerator_loss: 0.7743328809738159\n",
            "For Step: 21810 recon_loss: 0.16101042926311493 \tdiscriminator_loss: 1.2013969421386719 \tgenerator_loss: 0.9370819926261902\n",
            "For Step: 21820 recon_loss: 0.17784784734249115 \tdiscriminator_loss: 1.162333369255066 \tgenerator_loss: 0.9693386554718018\n",
            "For Step: 21830 recon_loss: 0.17442020773887634 \tdiscriminator_loss: 1.3096662759780884 \tgenerator_loss: 0.8081554770469666\n",
            "For Step: 21840 recon_loss: 0.1776682585477829 \tdiscriminator_loss: 1.2412261962890625 \tgenerator_loss: 0.9069443941116333\n",
            "For Step: 21850 recon_loss: 0.1670820116996765 \tdiscriminator_loss: 1.2917089462280273 \tgenerator_loss: 0.9312185049057007\n",
            "For Step: 21860 recon_loss: 0.16769786179065704 \tdiscriminator_loss: 1.1831634044647217 \tgenerator_loss: 0.9264671206474304\n",
            "For Step: 21870 recon_loss: 0.17455852031707764 \tdiscriminator_loss: 1.1305880546569824 \tgenerator_loss: 0.9597921371459961\n",
            "For Step: 21880 recon_loss: 0.16868151724338531 \tdiscriminator_loss: 1.4109622240066528 \tgenerator_loss: 0.8346083164215088\n",
            "For Step: 21890 recon_loss: 0.17581693828105927 \tdiscriminator_loss: 1.171849012374878 \tgenerator_loss: 0.924410343170166\n",
            "For Step: 21900 recon_loss: 0.173617422580719 \tdiscriminator_loss: 1.2239469289779663 \tgenerator_loss: 0.8664005398750305\n",
            "For Step: 21910 recon_loss: 0.18046975135803223 \tdiscriminator_loss: 1.2624378204345703 \tgenerator_loss: 0.9483458995819092\n",
            "For Step: 21920 recon_loss: 0.1691412627696991 \tdiscriminator_loss: 1.2190122604370117 \tgenerator_loss: 0.8731182217597961\n",
            "For Step: 21930 recon_loss: 0.17163081467151642 \tdiscriminator_loss: 1.2868506908416748 \tgenerator_loss: 0.9310963153839111\n",
            "For Step: 21940 recon_loss: 0.1691260188817978 \tdiscriminator_loss: 1.155799388885498 \tgenerator_loss: 0.9673197865486145\n",
            "For Step: 21950 recon_loss: 0.17574633657932281 \tdiscriminator_loss: 1.1643825769424438 \tgenerator_loss: 0.9724141955375671\n",
            "For Step: 21960 recon_loss: 0.16765150427818298 \tdiscriminator_loss: 1.215867042541504 \tgenerator_loss: 0.9842348694801331\n",
            "For Step: 21970 recon_loss: 0.18692205846309662 \tdiscriminator_loss: 1.2274799346923828 \tgenerator_loss: 0.9475452899932861\n",
            "For Step: 21980 recon_loss: 0.16992422938346863 \tdiscriminator_loss: 1.3081245422363281 \tgenerator_loss: 0.8710681200027466\n",
            "For Step: 21990 recon_loss: 0.17337514460086823 \tdiscriminator_loss: 1.341954231262207 \tgenerator_loss: 0.9179653525352478\n",
            "For Step: 22000 recon_loss: 0.1779908388853073 \tdiscriminator_loss: 1.212137222290039 \tgenerator_loss: 0.8953683376312256\n",
            "For Step: 22010 recon_loss: 0.1649460345506668 \tdiscriminator_loss: 1.1835665702819824 \tgenerator_loss: 0.9137145280838013\n",
            "For Step: 22020 recon_loss: 0.17807452380657196 \tdiscriminator_loss: 1.1576274633407593 \tgenerator_loss: 0.9934937953948975\n",
            "For Step: 22030 recon_loss: 0.18453273177146912 \tdiscriminator_loss: 1.164163589477539 \tgenerator_loss: 0.9659419655799866\n",
            "For Step: 22040 recon_loss: 0.18282151222229004 \tdiscriminator_loss: 1.2371602058410645 \tgenerator_loss: 0.9533921480178833\n",
            "For Step: 22050 recon_loss: 0.1738893687725067 \tdiscriminator_loss: 1.1720166206359863 \tgenerator_loss: 0.9267655611038208\n",
            "For Step: 22060 recon_loss: 0.16945458948612213 \tdiscriminator_loss: 1.1891714334487915 \tgenerator_loss: 0.9127540588378906\n",
            "For Step: 22070 recon_loss: 0.1747387796640396 \tdiscriminator_loss: 1.103013038635254 \tgenerator_loss: 0.9510927200317383\n",
            "For Step: 22080 recon_loss: 0.17818503081798553 \tdiscriminator_loss: 1.194031000137329 \tgenerator_loss: 0.9294948577880859\n",
            "For Step: 22090 recon_loss: 0.1884848177433014 \tdiscriminator_loss: 1.0441970825195312 \tgenerator_loss: 1.036675214767456\n",
            "For Step: 22100 recon_loss: 0.18129439651966095 \tdiscriminator_loss: 1.2227312326431274 \tgenerator_loss: 0.904839038848877\n",
            "For Step: 22110 recon_loss: 0.16655580699443817 \tdiscriminator_loss: 1.271309733390808 \tgenerator_loss: 0.8618876934051514\n",
            "For Step: 22120 recon_loss: 0.18159045279026031 \tdiscriminator_loss: 1.1304234266281128 \tgenerator_loss: 0.955552339553833\n",
            "For Step: 22130 recon_loss: 0.18169289827346802 \tdiscriminator_loss: 1.2145169973373413 \tgenerator_loss: 0.9005957841873169\n",
            "For Step: 22140 recon_loss: 0.16649435460567474 \tdiscriminator_loss: 1.2446513175964355 \tgenerator_loss: 0.9232958555221558\n",
            "For Step: 22150 recon_loss: 0.17554423213005066 \tdiscriminator_loss: 1.2415108680725098 \tgenerator_loss: 0.9275888204574585\n",
            "For Step: 22160 recon_loss: 0.1724567413330078 \tdiscriminator_loss: 1.1422719955444336 \tgenerator_loss: 0.9401540756225586\n",
            "For Step: 22170 recon_loss: 0.17911791801452637 \tdiscriminator_loss: 1.0203638076782227 \tgenerator_loss: 0.9586924910545349\n",
            "For Step: 22180 recon_loss: 0.1752147674560547 \tdiscriminator_loss: 1.2456557750701904 \tgenerator_loss: 0.9521801471710205\n",
            "For Step: 22190 recon_loss: 0.1848425418138504 \tdiscriminator_loss: 1.2256169319152832 \tgenerator_loss: 0.9022583365440369\n",
            "For Step: 22200 recon_loss: 0.16190166771411896 \tdiscriminator_loss: 1.1830096244812012 \tgenerator_loss: 0.9682455062866211\n",
            "For Step: 22210 recon_loss: 0.17263397574424744 \tdiscriminator_loss: 1.3145241737365723 \tgenerator_loss: 0.845990777015686\n",
            "For Step: 22220 recon_loss: 0.17430245876312256 \tdiscriminator_loss: 1.2678368091583252 \tgenerator_loss: 0.9607076644897461\n",
            "For Step: 22230 recon_loss: 0.17009703814983368 \tdiscriminator_loss: 1.1938717365264893 \tgenerator_loss: 0.9148458242416382\n",
            "For Step: 22240 recon_loss: 0.1670665293931961 \tdiscriminator_loss: 1.3017210960388184 \tgenerator_loss: 0.9432475566864014\n",
            "For Step: 22250 recon_loss: 0.17055004835128784 \tdiscriminator_loss: 1.2218676805496216 \tgenerator_loss: 0.9776775240898132\n",
            "For Step: 22260 recon_loss: 0.15801428258419037 \tdiscriminator_loss: 1.17544686794281 \tgenerator_loss: 0.9775702953338623\n",
            "For Step: 22270 recon_loss: 0.1639859974384308 \tdiscriminator_loss: 1.2418897151947021 \tgenerator_loss: 0.9190372228622437\n",
            "For Step: 22280 recon_loss: 0.17906470596790314 \tdiscriminator_loss: 1.2618381977081299 \tgenerator_loss: 0.9029526114463806\n",
            "For Step: 22290 recon_loss: 0.17852142453193665 \tdiscriminator_loss: 1.1316813230514526 \tgenerator_loss: 1.0096654891967773\n",
            "For Step: 22300 recon_loss: 0.17240869998931885 \tdiscriminator_loss: 1.2413870096206665 \tgenerator_loss: 0.8967166543006897\n",
            "For Step: 22310 recon_loss: 0.16698957979679108 \tdiscriminator_loss: 1.2662038803100586 \tgenerator_loss: 0.9718296527862549\n",
            "For Step: 22320 recon_loss: 0.16009344160556793 \tdiscriminator_loss: 1.1948838233947754 \tgenerator_loss: 0.9118316769599915\n",
            "For Step: 22330 recon_loss: 0.17359648644924164 \tdiscriminator_loss: 1.1749718189239502 \tgenerator_loss: 0.964914083480835\n",
            "For Step: 22340 recon_loss: 0.1703069508075714 \tdiscriminator_loss: 1.217461109161377 \tgenerator_loss: 0.8936386108398438\n",
            "For Step: 22350 recon_loss: 0.17027390003204346 \tdiscriminator_loss: 1.175485610961914 \tgenerator_loss: 0.929501473903656\n",
            "For Step: 22360 recon_loss: 0.17535361647605896 \tdiscriminator_loss: 1.282328724861145 \tgenerator_loss: 0.8908731937408447\n",
            "For Step: 22370 recon_loss: 0.17532512545585632 \tdiscriminator_loss: 1.3158667087554932 \tgenerator_loss: 0.9964181780815125\n",
            "For Step: 22380 recon_loss: 0.17902125418186188 \tdiscriminator_loss: 1.2016191482543945 \tgenerator_loss: 0.9279636144638062\n",
            "For Step: 22390 recon_loss: 0.17848263680934906 \tdiscriminator_loss: 1.2168534994125366 \tgenerator_loss: 0.9298998117446899\n",
            "For Step: 22400 recon_loss: 0.1515599489212036 \tdiscriminator_loss: 1.1516762971878052 \tgenerator_loss: 0.9289761781692505\n",
            "For Step: 22410 recon_loss: 0.17943532764911652 \tdiscriminator_loss: 1.244455337524414 \tgenerator_loss: 0.8784636855125427\n",
            "For Step: 22420 recon_loss: 0.17581447958946228 \tdiscriminator_loss: 1.2591488361358643 \tgenerator_loss: 0.9567263126373291\n",
            "For Step: 22430 recon_loss: 0.1888582855463028 \tdiscriminator_loss: 1.178126335144043 \tgenerator_loss: 1.0089040994644165\n",
            "For Step: 22440 recon_loss: 0.17247964441776276 \tdiscriminator_loss: 1.1215310096740723 \tgenerator_loss: 0.914726197719574\n",
            "For Step: 22450 recon_loss: 0.17775140702724457 \tdiscriminator_loss: 1.3670710325241089 \tgenerator_loss: 0.8926651477813721\n",
            "For Step: 22460 recon_loss: 0.17634718120098114 \tdiscriminator_loss: 1.3239872455596924 \tgenerator_loss: 0.9011387228965759\n",
            "For Step: 22470 recon_loss: 0.18376916646957397 \tdiscriminator_loss: 1.167015790939331 \tgenerator_loss: 0.9805805087089539\n",
            "For Step: 22480 recon_loss: 0.17974841594696045 \tdiscriminator_loss: 1.0782911777496338 \tgenerator_loss: 0.9616829752922058\n",
            "For Step: 22490 recon_loss: 0.18635332584381104 \tdiscriminator_loss: 1.2613461017608643 \tgenerator_loss: 0.9285187125205994\n",
            "For Step: 22500 recon_loss: 0.17779342830181122 \tdiscriminator_loss: 1.245785117149353 \tgenerator_loss: 0.952547013759613\n",
            "For Step: 22510 recon_loss: 0.1818217635154724 \tdiscriminator_loss: 1.2105724811553955 \tgenerator_loss: 0.9432576894760132\n",
            "For Step: 22520 recon_loss: 0.16517814993858337 \tdiscriminator_loss: 1.2963451147079468 \tgenerator_loss: 0.8661083579063416\n",
            "For Step: 22530 recon_loss: 0.17902474105358124 \tdiscriminator_loss: 1.2906906604766846 \tgenerator_loss: 1.0066986083984375\n",
            "For Step: 22540 recon_loss: 0.166581392288208 \tdiscriminator_loss: 1.1539233922958374 \tgenerator_loss: 0.9290014505386353\n",
            "For Step: 22550 recon_loss: 0.1719723343849182 \tdiscriminator_loss: 1.2831273078918457 \tgenerator_loss: 0.9188075065612793\n",
            "For Step: 22560 recon_loss: 0.18672728538513184 \tdiscriminator_loss: 1.1958723068237305 \tgenerator_loss: 1.0128130912780762\n",
            "For Step: 22570 recon_loss: 0.16927313804626465 \tdiscriminator_loss: 1.218306541442871 \tgenerator_loss: 0.9374194741249084\n",
            "For Step: 22580 recon_loss: 0.17186838388442993 \tdiscriminator_loss: 1.1839174032211304 \tgenerator_loss: 0.9052212238311768\n",
            "For Step: 22590 recon_loss: 0.16723302006721497 \tdiscriminator_loss: 1.163826823234558 \tgenerator_loss: 0.9390175342559814\n",
            "For Step: 22600 recon_loss: 0.16714924573898315 \tdiscriminator_loss: 1.2270736694335938 \tgenerator_loss: 0.8813868761062622\n",
            "For Step: 22610 recon_loss: 0.16821293532848358 \tdiscriminator_loss: 1.2620095014572144 \tgenerator_loss: 0.8726370334625244\n",
            "For Step: 22620 recon_loss: 0.16354478895664215 \tdiscriminator_loss: 1.3224371671676636 \tgenerator_loss: 0.843008279800415\n",
            "For Step: 22630 recon_loss: 0.17809027433395386 \tdiscriminator_loss: 1.3082351684570312 \tgenerator_loss: 0.9178866744041443\n",
            "For Step: 22640 recon_loss: 0.19378158450126648 \tdiscriminator_loss: 1.2256152629852295 \tgenerator_loss: 0.9430265426635742\n",
            "For Step: 22650 recon_loss: 0.1797439604997635 \tdiscriminator_loss: 1.2720593214035034 \tgenerator_loss: 0.9007056951522827\n",
            "For Step: 22660 recon_loss: 0.17694541811943054 \tdiscriminator_loss: 1.2844715118408203 \tgenerator_loss: 0.917471706867218\n",
            "For Step: 22670 recon_loss: 0.18045900762081146 \tdiscriminator_loss: 1.1578781604766846 \tgenerator_loss: 0.9836567640304565\n",
            "For Step: 22680 recon_loss: 0.184219628572464 \tdiscriminator_loss: 1.2295196056365967 \tgenerator_loss: 0.9077030420303345\n",
            "For Step: 22690 recon_loss: 0.17111817002296448 \tdiscriminator_loss: 1.2652084827423096 \tgenerator_loss: 0.9379485845565796\n",
            "For Step: 22700 recon_loss: 0.17946851253509521 \tdiscriminator_loss: 1.2721874713897705 \tgenerator_loss: 0.8636340498924255\n",
            "For Step: 22710 recon_loss: 0.17146766185760498 \tdiscriminator_loss: 1.2900869846343994 \tgenerator_loss: 0.8968631029129028\n",
            "For Step: 22720 recon_loss: 0.17102566361427307 \tdiscriminator_loss: 1.145532488822937 \tgenerator_loss: 0.9342272877693176\n",
            "For Step: 22730 recon_loss: 0.16971132159233093 \tdiscriminator_loss: 1.1626062393188477 \tgenerator_loss: 0.9489893317222595\n",
            "For Step: 22740 recon_loss: 0.16697901487350464 \tdiscriminator_loss: 1.2816758155822754 \tgenerator_loss: 0.9028591513633728\n",
            "For Step: 22750 recon_loss: 0.16334044933319092 \tdiscriminator_loss: 1.3006455898284912 \tgenerator_loss: 0.8095744848251343\n",
            "For Step: 22760 recon_loss: 0.18730108439922333 \tdiscriminator_loss: 1.1965792179107666 \tgenerator_loss: 0.9445359706878662\n",
            "For Step: 22770 recon_loss: 0.1680469959974289 \tdiscriminator_loss: 1.2283596992492676 \tgenerator_loss: 0.8519901633262634\n",
            "For Step: 22780 recon_loss: 0.16303306818008423 \tdiscriminator_loss: 1.303924798965454 \tgenerator_loss: 0.8703838586807251\n",
            "For Step: 22790 recon_loss: 0.19015389680862427 \tdiscriminator_loss: 1.1475465297698975 \tgenerator_loss: 0.9576086401939392\n",
            "For Step: 22800 recon_loss: 0.16884426772594452 \tdiscriminator_loss: 1.2381435632705688 \tgenerator_loss: 0.9703845381736755\n",
            "For Step: 22810 recon_loss: 0.17670580744743347 \tdiscriminator_loss: 1.1797821521759033 \tgenerator_loss: 0.9305674433708191\n",
            "For Step: 22820 recon_loss: 0.18003420531749725 \tdiscriminator_loss: 1.2469830513000488 \tgenerator_loss: 0.9190257787704468\n",
            "For Step: 22830 recon_loss: 0.18070290982723236 \tdiscriminator_loss: 1.325019359588623 \tgenerator_loss: 0.8631078004837036\n",
            "For Step: 22840 recon_loss: 0.17012934386730194 \tdiscriminator_loss: 1.2767845392227173 \tgenerator_loss: 0.8925671577453613\n",
            "For Step: 22850 recon_loss: 0.18783767521381378 \tdiscriminator_loss: 1.215718388557434 \tgenerator_loss: 0.9473901987075806\n",
            "For Step: 22860 recon_loss: 0.1887979656457901 \tdiscriminator_loss: 1.2001649141311646 \tgenerator_loss: 0.9308532476425171\n",
            "For Step: 22870 recon_loss: 0.17742972075939178 \tdiscriminator_loss: 1.1941708326339722 \tgenerator_loss: 0.8398013710975647\n",
            "For Step: 22880 recon_loss: 0.17672733962535858 \tdiscriminator_loss: 1.2517932653427124 \tgenerator_loss: 0.8778326511383057\n",
            "For Step: 22890 recon_loss: 0.17927227914333344 \tdiscriminator_loss: 1.1670244932174683 \tgenerator_loss: 0.9109542369842529\n",
            "For Step: 22900 recon_loss: 0.1724274754524231 \tdiscriminator_loss: 1.2669165134429932 \tgenerator_loss: 0.8971859812736511\n",
            "For Step: 22910 recon_loss: 0.1709286868572235 \tdiscriminator_loss: 1.2855682373046875 \tgenerator_loss: 0.882672905921936\n",
            "For Step: 22920 recon_loss: 0.17020538449287415 \tdiscriminator_loss: 1.1705273389816284 \tgenerator_loss: 0.977949857711792\n",
            "For Step: 22930 recon_loss: 0.17466223239898682 \tdiscriminator_loss: 1.201165795326233 \tgenerator_loss: 0.9099707007408142\n",
            "For Step: 22940 recon_loss: 0.1771308332681656 \tdiscriminator_loss: 1.2672882080078125 \tgenerator_loss: 0.9042870998382568\n",
            "For Step: 22950 recon_loss: 0.17452743649482727 \tdiscriminator_loss: 1.2532050609588623 \tgenerator_loss: 0.9069748520851135\n",
            "For Step: 22960 recon_loss: 0.1808185577392578 \tdiscriminator_loss: 1.218268632888794 \tgenerator_loss: 0.9127204418182373\n",
            "For Step: 22970 recon_loss: 0.16129626333713531 \tdiscriminator_loss: 1.3864822387695312 \tgenerator_loss: 0.9274808764457703\n",
            "For Step: 22980 recon_loss: 0.16968366503715515 \tdiscriminator_loss: 1.1787936687469482 \tgenerator_loss: 0.9549953937530518\n",
            "For Step: 22990 recon_loss: 0.1858464628458023 \tdiscriminator_loss: 1.1555715799331665 \tgenerator_loss: 0.8898247480392456\n",
            "For Step: 23000 recon_loss: 0.19012700021266937 \tdiscriminator_loss: 1.1462597846984863 \tgenerator_loss: 1.023545265197754\n",
            "For Step: 23010 recon_loss: 0.18458648025989532 \tdiscriminator_loss: 1.3537867069244385 \tgenerator_loss: 0.9170398712158203\n",
            "For Step: 23020 recon_loss: 0.1829889863729477 \tdiscriminator_loss: 1.2235406637191772 \tgenerator_loss: 0.9947528839111328\n",
            "For Step: 23030 recon_loss: 0.1711549609899521 \tdiscriminator_loss: 1.3213136196136475 \tgenerator_loss: 0.8873786926269531\n",
            "For Step: 23040 recon_loss: 0.182037353515625 \tdiscriminator_loss: 1.1519746780395508 \tgenerator_loss: 0.9094065427780151\n",
            "For Step: 23050 recon_loss: 0.16938267648220062 \tdiscriminator_loss: 1.2571090459823608 \tgenerator_loss: 0.9125022888183594\n",
            "For Step: 23060 recon_loss: 0.17208831012248993 \tdiscriminator_loss: 1.1817448139190674 \tgenerator_loss: 0.9624389410018921\n",
            "For Step: 23070 recon_loss: 0.18040697276592255 \tdiscriminator_loss: 1.227818250656128 \tgenerator_loss: 0.9650450944900513\n",
            "For Step: 23080 recon_loss: 0.17340520024299622 \tdiscriminator_loss: 1.2584660053253174 \tgenerator_loss: 0.9132241010665894\n",
            "For Step: 23090 recon_loss: 0.15938067436218262 \tdiscriminator_loss: 1.231123447418213 \tgenerator_loss: 0.9027479887008667\n",
            "For Step: 23100 recon_loss: 0.17885178327560425 \tdiscriminator_loss: 1.1882147789001465 \tgenerator_loss: 0.9199967384338379\n",
            "For Step: 23110 recon_loss: 0.17823262512683868 \tdiscriminator_loss: 1.2664016485214233 \tgenerator_loss: 0.960128903388977\n",
            "For Step: 23120 recon_loss: 0.16844919323921204 \tdiscriminator_loss: 1.2649179697036743 \tgenerator_loss: 0.9247080087661743\n",
            "For Step: 23130 recon_loss: 0.17951829731464386 \tdiscriminator_loss: 1.1336010694503784 \tgenerator_loss: 0.8738008737564087\n",
            "For Step: 23140 recon_loss: 0.1773420125246048 \tdiscriminator_loss: 1.270869255065918 \tgenerator_loss: 0.8833562135696411\n",
            "For Step: 23150 recon_loss: 0.16516761481761932 \tdiscriminator_loss: 1.292858600616455 \tgenerator_loss: 0.919793426990509\n",
            "For Step: 23160 recon_loss: 0.16563740372657776 \tdiscriminator_loss: 1.1404666900634766 \tgenerator_loss: 0.9111506938934326\n",
            "For Step: 23170 recon_loss: 0.1766984462738037 \tdiscriminator_loss: 1.192924976348877 \tgenerator_loss: 0.8866153359413147\n",
            "For Step: 23180 recon_loss: 0.17867501080036163 \tdiscriminator_loss: 1.29081392288208 \tgenerator_loss: 0.9420254230499268\n",
            "For Step: 23190 recon_loss: 0.1788988709449768 \tdiscriminator_loss: 1.1707322597503662 \tgenerator_loss: 0.9350651502609253\n",
            "For Step: 23200 recon_loss: 0.1612197458744049 \tdiscriminator_loss: 1.2379405498504639 \tgenerator_loss: 0.9082224369049072\n",
            "For Step: 23210 recon_loss: 0.1680201292037964 \tdiscriminator_loss: 1.1460435390472412 \tgenerator_loss: 0.9486517906188965\n",
            "For Step: 23220 recon_loss: 0.17070980370044708 \tdiscriminator_loss: 1.308685064315796 \tgenerator_loss: 0.9654125571250916\n",
            "For Step: 23230 recon_loss: 0.1878679245710373 \tdiscriminator_loss: 1.2595828771591187 \tgenerator_loss: 0.9240135550498962\n",
            "For Step: 23240 recon_loss: 0.16609789431095123 \tdiscriminator_loss: 1.1222128868103027 \tgenerator_loss: 0.8529255390167236\n",
            "For Step: 23250 recon_loss: 0.18805712461471558 \tdiscriminator_loss: 1.1255927085876465 \tgenerator_loss: 0.9306551218032837\n",
            "For Step: 23260 recon_loss: 0.1695186048746109 \tdiscriminator_loss: 1.1495991945266724 \tgenerator_loss: 0.935396671295166\n",
            "For Step: 23270 recon_loss: 0.1733047366142273 \tdiscriminator_loss: 1.2058582305908203 \tgenerator_loss: 0.94362473487854\n",
            "For Step: 23280 recon_loss: 0.18063335120677948 \tdiscriminator_loss: 1.1730574369430542 \tgenerator_loss: 0.8743119239807129\n",
            "For Step: 23290 recon_loss: 0.16855189204216003 \tdiscriminator_loss: 1.114715337753296 \tgenerator_loss: 0.9935556650161743\n",
            "For Step: 23300 recon_loss: 0.17280462384223938 \tdiscriminator_loss: 1.3154805898666382 \tgenerator_loss: 0.8747857809066772\n",
            "For Step: 23310 recon_loss: 0.17083652317523956 \tdiscriminator_loss: 1.1399749517440796 \tgenerator_loss: 0.9153342843055725\n",
            "For Step: 23320 recon_loss: 0.18476994335651398 \tdiscriminator_loss: 1.2181693315505981 \tgenerator_loss: 0.8534061908721924\n",
            "For Step: 23330 recon_loss: 0.18361137807369232 \tdiscriminator_loss: 1.1562628746032715 \tgenerator_loss: 0.8904440999031067\n",
            "For Step: 23340 recon_loss: 0.16942577064037323 \tdiscriminator_loss: 1.2925612926483154 \tgenerator_loss: 0.8931952714920044\n",
            "For Step: 23350 recon_loss: 0.1769660860300064 \tdiscriminator_loss: 1.1923900842666626 \tgenerator_loss: 0.9099985361099243\n",
            "For Step: 23360 recon_loss: 0.1650010645389557 \tdiscriminator_loss: 1.1879780292510986 \tgenerator_loss: 0.8975434303283691\n",
            "For Step: 23370 recon_loss: 0.17390142381191254 \tdiscriminator_loss: 1.2288784980773926 \tgenerator_loss: 0.8615261316299438\n",
            "For Step: 23380 recon_loss: 0.17231856286525726 \tdiscriminator_loss: 1.1514114141464233 \tgenerator_loss: 1.032280683517456\n",
            "For Step: 23390 recon_loss: 0.15259550511837006 \tdiscriminator_loss: 1.255291223526001 \tgenerator_loss: 0.9193730354309082\n",
            "For Step: 23400 recon_loss: 0.1732204556465149 \tdiscriminator_loss: 1.2623529434204102 \tgenerator_loss: 0.8679606318473816\n",
            "For Step: 23410 recon_loss: 0.17733554542064667 \tdiscriminator_loss: 1.358087182044983 \tgenerator_loss: 0.8356339335441589\n",
            "For Step: 23420 recon_loss: 0.18021441996097565 \tdiscriminator_loss: 1.1813387870788574 \tgenerator_loss: 0.857367753982544\n",
            "For Step: 23430 recon_loss: 0.1776462346315384 \tdiscriminator_loss: 1.2331421375274658 \tgenerator_loss: 0.9272825717926025\n",
            "For Step: 23440 recon_loss: 0.1721133142709732 \tdiscriminator_loss: 1.2383004426956177 \tgenerator_loss: 0.8913997411727905\n",
            "For Step: 23450 recon_loss: 0.16337795555591583 \tdiscriminator_loss: 1.3085041046142578 \tgenerator_loss: 0.9279887080192566\n",
            "For Step: 23460 recon_loss: 0.16950537264347076 \tdiscriminator_loss: 1.1687630414962769 \tgenerator_loss: 0.9095169305801392\n",
            "For Step: 23470 recon_loss: 0.16454008221626282 \tdiscriminator_loss: 1.1124789714813232 \tgenerator_loss: 0.900436520576477\n",
            "For Step: 23480 recon_loss: 0.17026418447494507 \tdiscriminator_loss: 1.280134677886963 \tgenerator_loss: 0.9631887674331665\n",
            "For Step: 23490 recon_loss: 0.18004167079925537 \tdiscriminator_loss: 1.1721811294555664 \tgenerator_loss: 0.9303357601165771\n",
            "For Step: 23500 recon_loss: 0.17668144404888153 \tdiscriminator_loss: 1.1802572011947632 \tgenerator_loss: 0.8685985207557678\n",
            "For Step: 23510 recon_loss: 0.16512811183929443 \tdiscriminator_loss: 1.290066123008728 \tgenerator_loss: 0.8912180662155151\n",
            "For Step: 23520 recon_loss: 0.1711571216583252 \tdiscriminator_loss: 1.1710479259490967 \tgenerator_loss: 0.8935049772262573\n",
            "For Step: 23530 recon_loss: 0.17170369625091553 \tdiscriminator_loss: 1.1970391273498535 \tgenerator_loss: 0.9029179215431213\n",
            "For Step: 23540 recon_loss: 0.17907442152500153 \tdiscriminator_loss: 1.2329384088516235 \tgenerator_loss: 0.9269962310791016\n",
            "For Step: 23550 recon_loss: 0.1802935153245926 \tdiscriminator_loss: 1.1917823553085327 \tgenerator_loss: 0.9254050850868225\n",
            "For Step: 23560 recon_loss: 0.17462512850761414 \tdiscriminator_loss: 1.2182459831237793 \tgenerator_loss: 0.9518647789955139\n",
            "For Step: 23570 recon_loss: 0.16498233377933502 \tdiscriminator_loss: 1.1823116540908813 \tgenerator_loss: 0.8801949620246887\n",
            "For Step: 23580 recon_loss: 0.17440445721149445 \tdiscriminator_loss: 1.2737131118774414 \tgenerator_loss: 0.9066643118858337\n",
            "For Step: 23590 recon_loss: 0.18307505548000336 \tdiscriminator_loss: 1.1916399002075195 \tgenerator_loss: 0.9090076684951782\n",
            "For Step: 23600 recon_loss: 0.18176868557929993 \tdiscriminator_loss: 1.301719307899475 \tgenerator_loss: 0.979199230670929\n",
            "For Step: 23610 recon_loss: 0.1709650456905365 \tdiscriminator_loss: 1.3065807819366455 \tgenerator_loss: 0.9090301990509033\n",
            "For Step: 23620 recon_loss: 0.17180457711219788 \tdiscriminator_loss: 1.3119803667068481 \tgenerator_loss: 0.8670008778572083\n",
            "For Step: 23630 recon_loss: 0.16617564857006073 \tdiscriminator_loss: 1.1084669828414917 \tgenerator_loss: 0.971637487411499\n",
            "For Step: 23640 recon_loss: 0.1809498518705368 \tdiscriminator_loss: 1.316657304763794 \tgenerator_loss: 0.8931595087051392\n",
            "For Step: 23650 recon_loss: 0.16527430713176727 \tdiscriminator_loss: 1.2400569915771484 \tgenerator_loss: 0.9545114040374756\n",
            "For Step: 23660 recon_loss: 0.1789906620979309 \tdiscriminator_loss: 1.2107093334197998 \tgenerator_loss: 0.8920027017593384\n",
            "For Step: 23670 recon_loss: 0.1840410679578781 \tdiscriminator_loss: 1.2161935567855835 \tgenerator_loss: 0.9221704006195068\n",
            "For Step: 23680 recon_loss: 0.16752289235591888 \tdiscriminator_loss: 1.315079689025879 \tgenerator_loss: 0.8904539346694946\n",
            "For Step: 23690 recon_loss: 0.18088212609291077 \tdiscriminator_loss: 1.1462602615356445 \tgenerator_loss: 0.9269206523895264\n",
            "For Step: 23700 recon_loss: 0.17502188682556152 \tdiscriminator_loss: 1.2222009897232056 \tgenerator_loss: 0.9408941268920898\n",
            "For Step: 23710 recon_loss: 0.18337669968605042 \tdiscriminator_loss: 1.2822308540344238 \tgenerator_loss: 0.9259815216064453\n",
            "For Step: 23720 recon_loss: 0.17944180965423584 \tdiscriminator_loss: 1.313081979751587 \tgenerator_loss: 0.8544073104858398\n",
            "For Step: 23730 recon_loss: 0.17586232721805573 \tdiscriminator_loss: 1.250275731086731 \tgenerator_loss: 0.9756889343261719\n",
            "For Step: 23740 recon_loss: 0.17182475328445435 \tdiscriminator_loss: 1.1940405368804932 \tgenerator_loss: 0.9356247782707214\n",
            "For Step: 23750 recon_loss: 0.1786908060312271 \tdiscriminator_loss: 1.1781952381134033 \tgenerator_loss: 0.910310685634613\n",
            "For Step: 23760 recon_loss: 0.18603146076202393 \tdiscriminator_loss: 1.285395622253418 \tgenerator_loss: 0.8911778926849365\n",
            "For Step: 23770 recon_loss: 0.17691567540168762 \tdiscriminator_loss: 1.2855764627456665 \tgenerator_loss: 0.8973323106765747\n",
            "For Step: 23780 recon_loss: 0.16731616854667664 \tdiscriminator_loss: 1.1818668842315674 \tgenerator_loss: 0.9354191422462463\n",
            "For Step: 23790 recon_loss: 0.18171089887619019 \tdiscriminator_loss: 1.1980621814727783 \tgenerator_loss: 0.9435418844223022\n",
            "For Step: 23800 recon_loss: 0.1783977746963501 \tdiscriminator_loss: 1.2996461391448975 \tgenerator_loss: 0.9358906745910645\n",
            "For Step: 23810 recon_loss: 0.16450737416744232 \tdiscriminator_loss: 1.3040482997894287 \tgenerator_loss: 0.8657346367835999\n",
            "For Step: 23820 recon_loss: 0.16184766590595245 \tdiscriminator_loss: 1.1928752660751343 \tgenerator_loss: 0.9192172884941101\n",
            "For Step: 23830 recon_loss: 0.1634724736213684 \tdiscriminator_loss: 1.2669647932052612 \tgenerator_loss: 0.931277871131897\n",
            "For Step: 23840 recon_loss: 0.17720633745193481 \tdiscriminator_loss: 1.164961338043213 \tgenerator_loss: 0.9113773703575134\n",
            "For Step: 23850 recon_loss: 0.17551693320274353 \tdiscriminator_loss: 1.295316457748413 \tgenerator_loss: 0.895021915435791\n",
            "For Step: 23860 recon_loss: 0.17864422500133514 \tdiscriminator_loss: 1.2596063613891602 \tgenerator_loss: 0.9252938032150269\n",
            "For Step: 23870 recon_loss: 0.1725129932165146 \tdiscriminator_loss: 1.3116668462753296 \tgenerator_loss: 0.9370242357254028\n",
            "For Step: 23880 recon_loss: 0.1798773854970932 \tdiscriminator_loss: 1.2786012887954712 \tgenerator_loss: 0.8882821202278137\n",
            "For Step: 23890 recon_loss: 0.17559656500816345 \tdiscriminator_loss: 1.2311851978302002 \tgenerator_loss: 0.9282439947128296\n",
            "For Step: 23900 recon_loss: 0.17987872660160065 \tdiscriminator_loss: 1.2734012603759766 \tgenerator_loss: 0.9109790325164795\n",
            "For Step: 23910 recon_loss: 0.17577879130840302 \tdiscriminator_loss: 1.1944005489349365 \tgenerator_loss: 0.9319361448287964\n",
            "For Step: 23920 recon_loss: 0.1717843860387802 \tdiscriminator_loss: 1.140119194984436 \tgenerator_loss: 0.9008828401565552\n",
            "For Step: 23930 recon_loss: 0.1855536848306656 \tdiscriminator_loss: 1.163049340248108 \tgenerator_loss: 0.9555402994155884\n",
            "For Step: 23940 recon_loss: 0.17557790875434875 \tdiscriminator_loss: 1.1238476037979126 \tgenerator_loss: 1.003328800201416\n",
            "For Step: 23950 recon_loss: 0.17516303062438965 \tdiscriminator_loss: 1.3209648132324219 \tgenerator_loss: 0.8578078746795654\n",
            "For Step: 23960 recon_loss: 0.1823982149362564 \tdiscriminator_loss: 1.3088057041168213 \tgenerator_loss: 0.9128327369689941\n",
            "For Step: 23970 recon_loss: 0.16399042308330536 \tdiscriminator_loss: 1.236993670463562 \tgenerator_loss: 0.8775052428245544\n",
            "For Step: 23980 recon_loss: 0.16995561122894287 \tdiscriminator_loss: 1.274513840675354 \tgenerator_loss: 0.8467953205108643\n",
            "For Step: 23990 recon_loss: 0.18066348135471344 \tdiscriminator_loss: 1.2060902118682861 \tgenerator_loss: 0.9128048419952393\n",
            "For Step: 24000 recon_loss: 0.1732216626405716 \tdiscriminator_loss: 1.2255561351776123 \tgenerator_loss: 0.9244576692581177\n",
            "For Step: 24010 recon_loss: 0.19470839202404022 \tdiscriminator_loss: 1.2764649391174316 \tgenerator_loss: 0.8734305500984192\n",
            "For Step: 24020 recon_loss: 0.16890540719032288 \tdiscriminator_loss: 1.0970211029052734 \tgenerator_loss: 0.915113091468811\n",
            "For Step: 24030 recon_loss: 0.1768762171268463 \tdiscriminator_loss: 1.1321778297424316 \tgenerator_loss: 0.9113314151763916\n",
            "For Step: 24040 recon_loss: 0.18174241483211517 \tdiscriminator_loss: 1.1690384149551392 \tgenerator_loss: 0.9948024749755859\n",
            "For Step: 24050 recon_loss: 0.17601549625396729 \tdiscriminator_loss: 1.2345805168151855 \tgenerator_loss: 0.9277759790420532\n",
            "For Step: 24060 recon_loss: 0.1719587743282318 \tdiscriminator_loss: 1.3009278774261475 \tgenerator_loss: 0.8929376602172852\n",
            "For Step: 24070 recon_loss: 0.1795770227909088 \tdiscriminator_loss: 1.1268562078475952 \tgenerator_loss: 0.9824631214141846\n",
            "For Step: 24080 recon_loss: 0.17383600771427155 \tdiscriminator_loss: 1.200021743774414 \tgenerator_loss: 0.934611976146698\n",
            "For Step: 24090 recon_loss: 0.17950256168842316 \tdiscriminator_loss: 1.2913920879364014 \tgenerator_loss: 0.8830804228782654\n",
            "For Step: 24100 recon_loss: 0.17304719984531403 \tdiscriminator_loss: 1.2351813316345215 \tgenerator_loss: 0.8976876735687256\n",
            "For Step: 24110 recon_loss: 0.17407549917697906 \tdiscriminator_loss: 1.2293200492858887 \tgenerator_loss: 0.9429985284805298\n",
            "For Step: 24120 recon_loss: 0.18253645300865173 \tdiscriminator_loss: 1.2502261400222778 \tgenerator_loss: 0.8802411556243896\n",
            "For Step: 24130 recon_loss: 0.17381605505943298 \tdiscriminator_loss: 1.2307878732681274 \tgenerator_loss: 0.9129223823547363\n",
            "For Step: 24140 recon_loss: 0.17807485163211823 \tdiscriminator_loss: 1.2385284900665283 \tgenerator_loss: 0.9581999778747559\n",
            "For Step: 24150 recon_loss: 0.16636908054351807 \tdiscriminator_loss: 1.1779446601867676 \tgenerator_loss: 0.9629358649253845\n",
            "For Step: 24160 recon_loss: 0.17491963505744934 \tdiscriminator_loss: 1.3252737522125244 \tgenerator_loss: 0.9033018946647644\n",
            "For Step: 24170 recon_loss: 0.18519611656665802 \tdiscriminator_loss: 1.2388086318969727 \tgenerator_loss: 0.8921517133712769\n",
            "For Step: 24180 recon_loss: 0.17380578815937042 \tdiscriminator_loss: 1.2106750011444092 \tgenerator_loss: 0.8830661773681641\n",
            "For Step: 24190 recon_loss: 0.1690097153186798 \tdiscriminator_loss: 1.2463723421096802 \tgenerator_loss: 0.8255123496055603\n",
            "For Step: 24200 recon_loss: 0.17155788838863373 \tdiscriminator_loss: 1.218351125717163 \tgenerator_loss: 0.8484854698181152\n",
            "For Step: 24210 recon_loss: 0.17830346524715424 \tdiscriminator_loss: 1.1944279670715332 \tgenerator_loss: 0.9091385006904602\n",
            "For Step: 24220 recon_loss: 0.1678415983915329 \tdiscriminator_loss: 1.1202588081359863 \tgenerator_loss: 1.0145461559295654\n",
            "For Step: 24230 recon_loss: 0.1684051752090454 \tdiscriminator_loss: 1.1826910972595215 \tgenerator_loss: 0.90897536277771\n",
            "For Step: 24240 recon_loss: 0.176566943526268 \tdiscriminator_loss: 1.2604540586471558 \tgenerator_loss: 0.8521544933319092\n",
            "For Step: 24250 recon_loss: 0.1624213457107544 \tdiscriminator_loss: 1.1800589561462402 \tgenerator_loss: 0.9215201139450073\n",
            "For Step: 24260 recon_loss: 0.1615571826696396 \tdiscriminator_loss: 1.305168867111206 \tgenerator_loss: 0.8921405076980591\n",
            "For Step: 24270 recon_loss: 0.1704549789428711 \tdiscriminator_loss: 1.229853868484497 \tgenerator_loss: 0.9374223351478577\n",
            "For Step: 24280 recon_loss: 0.15783213078975677 \tdiscriminator_loss: 1.2039523124694824 \tgenerator_loss: 0.8562876582145691\n",
            "For Step: 24290 recon_loss: 0.17815208435058594 \tdiscriminator_loss: 1.2888340950012207 \tgenerator_loss: 0.8484430313110352\n",
            "For Step: 24300 recon_loss: 0.18374396860599518 \tdiscriminator_loss: 1.0857996940612793 \tgenerator_loss: 0.9817682504653931\n",
            "For Step: 24310 recon_loss: 0.18094947934150696 \tdiscriminator_loss: 1.2303829193115234 \tgenerator_loss: 0.955723762512207\n",
            "For Step: 24320 recon_loss: 0.17185336351394653 \tdiscriminator_loss: 1.2573561668395996 \tgenerator_loss: 0.922843337059021\n",
            "For Step: 24330 recon_loss: 0.1629456728696823 \tdiscriminator_loss: 1.2425389289855957 \tgenerator_loss: 0.9004902839660645\n",
            "For Step: 24340 recon_loss: 0.17483119666576385 \tdiscriminator_loss: 1.2957563400268555 \tgenerator_loss: 0.9256534576416016\n",
            "For Step: 24350 recon_loss: 0.17694608867168427 \tdiscriminator_loss: 1.1602160930633545 \tgenerator_loss: 0.8595410585403442\n",
            "For Step: 24360 recon_loss: 0.17940469086170197 \tdiscriminator_loss: 1.1765902042388916 \tgenerator_loss: 0.9593660831451416\n",
            "For Step: 24370 recon_loss: 0.18291492760181427 \tdiscriminator_loss: 1.2452044486999512 \tgenerator_loss: 0.8542524576187134\n",
            "For Step: 24380 recon_loss: 0.1759086549282074 \tdiscriminator_loss: 1.2143173217773438 \tgenerator_loss: 0.9778463840484619\n",
            "For Step: 24390 recon_loss: 0.16830164194107056 \tdiscriminator_loss: 1.2285135984420776 \tgenerator_loss: 0.8607706427574158\n",
            "For Step: 24400 recon_loss: 0.170041024684906 \tdiscriminator_loss: 1.1941126585006714 \tgenerator_loss: 0.8940541744232178\n",
            "For Step: 24410 recon_loss: 0.17107586562633514 \tdiscriminator_loss: 1.2424933910369873 \tgenerator_loss: 0.9025590419769287\n",
            "For Step: 24420 recon_loss: 0.16003718972206116 \tdiscriminator_loss: 1.260772466659546 \tgenerator_loss: 0.9051631689071655\n",
            "For Step: 24430 recon_loss: 0.16905975341796875 \tdiscriminator_loss: 1.22702956199646 \tgenerator_loss: 0.8948017358779907\n",
            "For Step: 24440 recon_loss: 0.17918622493743896 \tdiscriminator_loss: 1.2498356103897095 \tgenerator_loss: 0.8739365339279175\n",
            "For Step: 24450 recon_loss: 0.17944760620594025 \tdiscriminator_loss: 1.2774684429168701 \tgenerator_loss: 0.922906756401062\n",
            "For Step: 24460 recon_loss: 0.18974722921848297 \tdiscriminator_loss: 1.2930147647857666 \tgenerator_loss: 0.9173474311828613\n",
            "For Step: 24470 recon_loss: 0.20409488677978516 \tdiscriminator_loss: 1.2405946254730225 \tgenerator_loss: 0.9167664051055908\n",
            "For Step: 24480 recon_loss: 0.17745259404182434 \tdiscriminator_loss: 1.2601149082183838 \tgenerator_loss: 0.9459943771362305\n",
            "For Step: 24490 recon_loss: 0.18075932562351227 \tdiscriminator_loss: 1.3172415494918823 \tgenerator_loss: 0.875482439994812\n",
            "For Step: 24500 recon_loss: 0.17839883267879486 \tdiscriminator_loss: 1.165773868560791 \tgenerator_loss: 0.9609413146972656\n",
            "For Step: 24510 recon_loss: 0.17804881930351257 \tdiscriminator_loss: 1.2623143196105957 \tgenerator_loss: 0.9031085968017578\n",
            "For Step: 24520 recon_loss: 0.17503158748149872 \tdiscriminator_loss: 1.1768443584442139 \tgenerator_loss: 0.9164453744888306\n",
            "For Step: 24530 recon_loss: 0.1820964217185974 \tdiscriminator_loss: 1.2070231437683105 \tgenerator_loss: 0.9498347043991089\n",
            "For Step: 24540 recon_loss: 0.18503358960151672 \tdiscriminator_loss: 1.1120824813842773 \tgenerator_loss: 0.973119854927063\n",
            "For Step: 24550 recon_loss: 0.1856989562511444 \tdiscriminator_loss: 1.3476097583770752 \tgenerator_loss: 0.896845817565918\n",
            "For Step: 24560 recon_loss: 0.16805343329906464 \tdiscriminator_loss: 1.2201263904571533 \tgenerator_loss: 0.965070366859436\n",
            "For Step: 24570 recon_loss: 0.1705159991979599 \tdiscriminator_loss: 1.2932038307189941 \tgenerator_loss: 0.8525265455245972\n",
            "For Step: 24580 recon_loss: 0.15634989738464355 \tdiscriminator_loss: 1.2125778198242188 \tgenerator_loss: 0.9032644629478455\n",
            "For Step: 24590 recon_loss: 0.18644407391548157 \tdiscriminator_loss: 1.2011494636535645 \tgenerator_loss: 0.9485715627670288\n",
            "For Step: 24600 recon_loss: 0.17814026772975922 \tdiscriminator_loss: 1.2523648738861084 \tgenerator_loss: 0.9094092845916748\n",
            "For Step: 24610 recon_loss: 0.1771792620420456 \tdiscriminator_loss: 1.307616949081421 \tgenerator_loss: 0.8296499848365784\n",
            "For Step: 24620 recon_loss: 0.17767326533794403 \tdiscriminator_loss: 1.1994014978408813 \tgenerator_loss: 0.9428619146347046\n",
            "For Step: 24630 recon_loss: 0.18089337646961212 \tdiscriminator_loss: 1.1627612113952637 \tgenerator_loss: 0.9529078006744385\n",
            "For Step: 24640 recon_loss: 0.16836057603359222 \tdiscriminator_loss: 1.210869550704956 \tgenerator_loss: 0.8374015092849731\n",
            "For Step: 24650 recon_loss: 0.17995698750019073 \tdiscriminator_loss: 1.0948829650878906 \tgenerator_loss: 0.9197636842727661\n",
            "For Step: 24660 recon_loss: 0.1770940124988556 \tdiscriminator_loss: 1.2165472507476807 \tgenerator_loss: 0.9303293228149414\n",
            "For Step: 24670 recon_loss: 0.16883760690689087 \tdiscriminator_loss: 1.319697618484497 \tgenerator_loss: 0.8869051933288574\n",
            "For Step: 24680 recon_loss: 0.16886721551418304 \tdiscriminator_loss: 1.2422479391098022 \tgenerator_loss: 0.8978204131126404\n",
            "For Step: 24690 recon_loss: 0.176360085606575 \tdiscriminator_loss: 1.2032432556152344 \tgenerator_loss: 0.9099977612495422\n",
            "For Step: 24700 recon_loss: 0.18411901593208313 \tdiscriminator_loss: 1.2966970205307007 \tgenerator_loss: 0.8954432010650635\n",
            "For Step: 24710 recon_loss: 0.1693425327539444 \tdiscriminator_loss: 1.2577916383743286 \tgenerator_loss: 0.8768557906150818\n",
            "For Step: 24720 recon_loss: 0.1785966157913208 \tdiscriminator_loss: 1.2368444204330444 \tgenerator_loss: 0.8963853120803833\n",
            "For Step: 24730 recon_loss: 0.17907439172267914 \tdiscriminator_loss: 1.1493018865585327 \tgenerator_loss: 0.9502928256988525\n",
            "For Step: 24740 recon_loss: 0.1767340898513794 \tdiscriminator_loss: 1.2213633060455322 \tgenerator_loss: 0.8948997259140015\n",
            "For Step: 24750 recon_loss: 0.16916626691818237 \tdiscriminator_loss: 1.3641513586044312 \tgenerator_loss: 0.8675631284713745\n",
            "For Step: 24760 recon_loss: 0.19328691065311432 \tdiscriminator_loss: 1.1443151235580444 \tgenerator_loss: 0.9474164843559265\n",
            "For Step: 24770 recon_loss: 0.1809869259595871 \tdiscriminator_loss: 1.2406805753707886 \tgenerator_loss: 0.873079776763916\n",
            "For Step: 24780 recon_loss: 0.16511301696300507 \tdiscriminator_loss: 1.2401913404464722 \tgenerator_loss: 0.8943378925323486\n",
            "For Step: 24790 recon_loss: 0.1681012660264969 \tdiscriminator_loss: 1.278036117553711 \tgenerator_loss: 0.9480060935020447\n",
            "For Step: 24800 recon_loss: 0.18349573016166687 \tdiscriminator_loss: 1.2763921022415161 \tgenerator_loss: 0.8977401256561279\n",
            "For Step: 24810 recon_loss: 0.1802159696817398 \tdiscriminator_loss: 1.191257119178772 \tgenerator_loss: 0.9183075428009033\n",
            "For Step: 24820 recon_loss: 0.17278175055980682 \tdiscriminator_loss: 1.1973953247070312 \tgenerator_loss: 0.9132640361785889\n",
            "For Step: 24830 recon_loss: 0.16540442407131195 \tdiscriminator_loss: 1.1809309720993042 \tgenerator_loss: 0.8820548057556152\n",
            "For Step: 24840 recon_loss: 0.18150995671749115 \tdiscriminator_loss: 1.2785388231277466 \tgenerator_loss: 0.9002843499183655\n",
            "For Step: 24850 recon_loss: 0.17218340933322906 \tdiscriminator_loss: 1.2550623416900635 \tgenerator_loss: 0.8322956562042236\n",
            "For Step: 24860 recon_loss: 0.15976448357105255 \tdiscriminator_loss: 1.1998240947723389 \tgenerator_loss: 0.8585579991340637\n",
            "For Step: 24870 recon_loss: 0.18154431879520416 \tdiscriminator_loss: 1.218093752861023 \tgenerator_loss: 0.9764715433120728\n",
            "For Step: 24880 recon_loss: 0.1623009592294693 \tdiscriminator_loss: 1.3212233781814575 \tgenerator_loss: 0.8372246026992798\n",
            "For Step: 24890 recon_loss: 0.1757522076368332 \tdiscriminator_loss: 1.273129940032959 \tgenerator_loss: 0.9431796669960022\n",
            "For Step: 24900 recon_loss: 0.17449580132961273 \tdiscriminator_loss: 1.3104736804962158 \tgenerator_loss: 0.9205371141433716\n",
            "For Step: 24910 recon_loss: 0.18494866788387299 \tdiscriminator_loss: 1.3294273614883423 \tgenerator_loss: 0.8874726295471191\n",
            "For Step: 24920 recon_loss: 0.1654912233352661 \tdiscriminator_loss: 1.2797324657440186 \tgenerator_loss: 0.9102219343185425\n",
            "For Step: 24930 recon_loss: 0.1732231229543686 \tdiscriminator_loss: 1.1015312671661377 \tgenerator_loss: 0.9220700263977051\n",
            "For Step: 24940 recon_loss: 0.16545994579792023 \tdiscriminator_loss: 1.2037594318389893 \tgenerator_loss: 0.9465800523757935\n",
            "For Step: 24950 recon_loss: 0.17527490854263306 \tdiscriminator_loss: 1.219871997833252 \tgenerator_loss: 0.9493878483772278\n",
            "For Step: 24960 recon_loss: 0.1807059794664383 \tdiscriminator_loss: 1.2878594398498535 \tgenerator_loss: 0.8730607032775879\n",
            "For Step: 24970 recon_loss: 0.16587010025978088 \tdiscriminator_loss: 1.1608277559280396 \tgenerator_loss: 0.96036696434021\n",
            "For Step: 24980 recon_loss: 0.1757073849439621 \tdiscriminator_loss: 1.140634536743164 \tgenerator_loss: 0.942043662071228\n",
            "For Step: 24990 recon_loss: 0.17785580456256866 \tdiscriminator_loss: 1.208289623260498 \tgenerator_loss: 0.8617732524871826\n",
            "For Step: 25000 recon_loss: 0.17697696387767792 \tdiscriminator_loss: 1.064735770225525 \tgenerator_loss: 0.9620953798294067\n",
            "For Step: 25010 recon_loss: 0.17815041542053223 \tdiscriminator_loss: 1.3172060251235962 \tgenerator_loss: 0.9423859119415283\n",
            "For Step: 25020 recon_loss: 0.17071840167045593 \tdiscriminator_loss: 1.1827399730682373 \tgenerator_loss: 0.9362075924873352\n",
            "For Step: 25030 recon_loss: 0.17272667586803436 \tdiscriminator_loss: 1.345884084701538 \tgenerator_loss: 0.9147393107414246\n",
            "For Step: 25040 recon_loss: 0.1780979037284851 \tdiscriminator_loss: 1.1484782695770264 \tgenerator_loss: 0.9554823040962219\n",
            "For Step: 25050 recon_loss: 0.16759644448757172 \tdiscriminator_loss: 1.2985074520111084 \tgenerator_loss: 0.940192699432373\n",
            "For Step: 25060 recon_loss: 0.17087949812412262 \tdiscriminator_loss: 1.2748773097991943 \tgenerator_loss: 0.910210907459259\n",
            "For Step: 25070 recon_loss: 0.17223811149597168 \tdiscriminator_loss: 1.2713121175765991 \tgenerator_loss: 0.9051274061203003\n",
            "For Step: 25080 recon_loss: 0.1668836772441864 \tdiscriminator_loss: 1.277944803237915 \tgenerator_loss: 0.8395569324493408\n",
            "For Step: 25090 recon_loss: 0.17112883925437927 \tdiscriminator_loss: 1.192237377166748 \tgenerator_loss: 0.9504231214523315\n",
            "For Step: 25100 recon_loss: 0.1710406392812729 \tdiscriminator_loss: 1.198615550994873 \tgenerator_loss: 0.8810059428215027\n",
            "For Step: 25110 recon_loss: 0.18911556899547577 \tdiscriminator_loss: 1.3395252227783203 \tgenerator_loss: 0.8796010613441467\n",
            "For Step: 25120 recon_loss: 0.15555799007415771 \tdiscriminator_loss: 1.2572088241577148 \tgenerator_loss: 0.910509467124939\n",
            "For Step: 25130 recon_loss: 0.18198488652706146 \tdiscriminator_loss: 1.1537061929702759 \tgenerator_loss: 0.903899073600769\n",
            "For Step: 25140 recon_loss: 0.17404316365718842 \tdiscriminator_loss: 1.0903526544570923 \tgenerator_loss: 0.9723000526428223\n",
            "For Step: 25150 recon_loss: 0.17527134716510773 \tdiscriminator_loss: 1.2067581415176392 \tgenerator_loss: 0.9391031265258789\n",
            "For Step: 25160 recon_loss: 0.176010400056839 \tdiscriminator_loss: 1.2076737880706787 \tgenerator_loss: 0.8956029415130615\n",
            "For Step: 25170 recon_loss: 0.1731315404176712 \tdiscriminator_loss: 1.1760666370391846 \tgenerator_loss: 0.9089910984039307\n",
            "For Step: 25180 recon_loss: 0.1802385151386261 \tdiscriminator_loss: 1.104710340499878 \tgenerator_loss: 0.9765558838844299\n",
            "For Step: 25190 recon_loss: 0.16527768969535828 \tdiscriminator_loss: 1.217420220375061 \tgenerator_loss: 0.9058274030685425\n",
            "For Step: 25200 recon_loss: 0.1707841157913208 \tdiscriminator_loss: 1.2978360652923584 \tgenerator_loss: 0.9805841445922852\n",
            "For Step: 25210 recon_loss: 0.17673943936824799 \tdiscriminator_loss: 1.1016392707824707 \tgenerator_loss: 0.9551941752433777\n",
            "For Step: 25220 recon_loss: 0.17542293667793274 \tdiscriminator_loss: 1.2024259567260742 \tgenerator_loss: 0.8557982444763184\n",
            "For Step: 25230 recon_loss: 0.1711457371711731 \tdiscriminator_loss: 1.1589784622192383 \tgenerator_loss: 0.9670647382736206\n",
            "For Step: 25240 recon_loss: 0.16661368310451508 \tdiscriminator_loss: 1.1599011421203613 \tgenerator_loss: 0.9129641652107239\n",
            "For Step: 25250 recon_loss: 0.17800885438919067 \tdiscriminator_loss: 1.209047555923462 \tgenerator_loss: 0.9118008613586426\n",
            "For Step: 25260 recon_loss: 0.17143312096595764 \tdiscriminator_loss: 1.1705224514007568 \tgenerator_loss: 0.9532877206802368\n",
            "For Step: 25270 recon_loss: 0.1840663105249405 \tdiscriminator_loss: 1.3725097179412842 \tgenerator_loss: 0.8770101070404053\n",
            "For Step: 25280 recon_loss: 0.18384318053722382 \tdiscriminator_loss: 1.1819324493408203 \tgenerator_loss: 0.9654508829116821\n",
            "For Step: 25290 recon_loss: 0.16770891845226288 \tdiscriminator_loss: 1.2793753147125244 \tgenerator_loss: 0.9266303777694702\n",
            "For Step: 25300 recon_loss: 0.16275638341903687 \tdiscriminator_loss: 1.3020840883255005 \tgenerator_loss: 0.8712086081504822\n",
            "For Step: 25310 recon_loss: 0.17329351603984833 \tdiscriminator_loss: 1.2583765983581543 \tgenerator_loss: 0.9401423335075378\n",
            "For Step: 25320 recon_loss: 0.17542296648025513 \tdiscriminator_loss: 1.2058464288711548 \tgenerator_loss: 0.9298558235168457\n",
            "For Step: 25330 recon_loss: 0.17302651703357697 \tdiscriminator_loss: 1.2042834758758545 \tgenerator_loss: 0.8645669221878052\n",
            "For Step: 25340 recon_loss: 0.16390958428382874 \tdiscriminator_loss: 1.2159957885742188 \tgenerator_loss: 0.9354862570762634\n",
            "For Step: 25350 recon_loss: 0.18578198552131653 \tdiscriminator_loss: 1.2350473403930664 \tgenerator_loss: 0.9266306161880493\n",
            "For Step: 25360 recon_loss: 0.17474612593650818 \tdiscriminator_loss: 1.3350951671600342 \tgenerator_loss: 0.9001979827880859\n",
            "For Step: 25370 recon_loss: 0.1606820523738861 \tdiscriminator_loss: 1.1554832458496094 \tgenerator_loss: 0.9575921297073364\n",
            "For Step: 25380 recon_loss: 0.17939579486846924 \tdiscriminator_loss: 1.2971506118774414 \tgenerator_loss: 0.8694993257522583\n",
            "For Step: 25390 recon_loss: 0.18223439157009125 \tdiscriminator_loss: 1.1981778144836426 \tgenerator_loss: 0.9776862859725952\n",
            "For Step: 25400 recon_loss: 0.1865692287683487 \tdiscriminator_loss: 1.2194387912750244 \tgenerator_loss: 0.9027605056762695\n",
            "For Step: 25410 recon_loss: 0.17533594369888306 \tdiscriminator_loss: 1.303453803062439 \tgenerator_loss: 0.893905520439148\n",
            "For Step: 25420 recon_loss: 0.17305436730384827 \tdiscriminator_loss: 1.2855896949768066 \tgenerator_loss: 0.9230629205703735\n",
            "For Step: 25430 recon_loss: 0.1794915497303009 \tdiscriminator_loss: 1.1246938705444336 \tgenerator_loss: 0.9934366941452026\n",
            "For Step: 25440 recon_loss: 0.1737132966518402 \tdiscriminator_loss: 1.3181232213974 \tgenerator_loss: 0.899533212184906\n",
            "For Step: 25450 recon_loss: 0.17471502721309662 \tdiscriminator_loss: 1.2976114749908447 \tgenerator_loss: 0.8671348094940186\n",
            "For Step: 25460 recon_loss: 0.16697317361831665 \tdiscriminator_loss: 1.3777852058410645 \tgenerator_loss: 0.8293814659118652\n",
            "For Step: 25470 recon_loss: 0.1689678430557251 \tdiscriminator_loss: 1.174544095993042 \tgenerator_loss: 0.9346989393234253\n",
            "For Step: 25480 recon_loss: 0.16767801344394684 \tdiscriminator_loss: 1.2153704166412354 \tgenerator_loss: 0.8560863733291626\n",
            "For Step: 25490 recon_loss: 0.162939190864563 \tdiscriminator_loss: 1.2994303703308105 \tgenerator_loss: 0.9770507216453552\n",
            "For Step: 25500 recon_loss: 0.17495211958885193 \tdiscriminator_loss: 1.2336738109588623 \tgenerator_loss: 0.8673261404037476\n",
            "For Step: 25510 recon_loss: 0.18038378655910492 \tdiscriminator_loss: 1.189577341079712 \tgenerator_loss: 0.8875734806060791\n",
            "For Step: 25520 recon_loss: 0.18929022550582886 \tdiscriminator_loss: 1.1970287561416626 \tgenerator_loss: 0.966298520565033\n",
            "For Step: 25530 recon_loss: 0.166629359126091 \tdiscriminator_loss: 1.2104014158248901 \tgenerator_loss: 0.8992915749549866\n",
            "For Step: 25540 recon_loss: 0.16760973632335663 \tdiscriminator_loss: 1.1643955707550049 \tgenerator_loss: 0.9230332374572754\n",
            "For Step: 25550 recon_loss: 0.17434121668338776 \tdiscriminator_loss: 1.2025415897369385 \tgenerator_loss: 0.887553334236145\n",
            "For Step: 25560 recon_loss: 0.18020795285701752 \tdiscriminator_loss: 1.2157232761383057 \tgenerator_loss: 0.9187074303627014\n",
            "For Step: 25570 recon_loss: 0.17896288633346558 \tdiscriminator_loss: 1.1718621253967285 \tgenerator_loss: 0.9085114002227783\n",
            "For Step: 25580 recon_loss: 0.1818990856409073 \tdiscriminator_loss: 1.1801438331604004 \tgenerator_loss: 1.0101217031478882\n",
            "For Step: 25590 recon_loss: 0.16236771643161774 \tdiscriminator_loss: 1.2188295125961304 \tgenerator_loss: 0.9411886930465698\n",
            "For Step: 25600 recon_loss: 0.1734251230955124 \tdiscriminator_loss: 1.213674783706665 \tgenerator_loss: 0.9155257940292358\n",
            "For Step: 25610 recon_loss: 0.161472886800766 \tdiscriminator_loss: 1.1981356143951416 \tgenerator_loss: 0.8617311716079712\n",
            "For Step: 25620 recon_loss: 0.17669110000133514 \tdiscriminator_loss: 1.2220051288604736 \tgenerator_loss: 0.9025346040725708\n",
            "For Step: 25630 recon_loss: 0.1761138141155243 \tdiscriminator_loss: 1.1253186464309692 \tgenerator_loss: 0.9932370781898499\n",
            "For Step: 25640 recon_loss: 0.17135295271873474 \tdiscriminator_loss: 1.194381594657898 \tgenerator_loss: 0.8777167201042175\n",
            "For Step: 25650 recon_loss: 0.15894968807697296 \tdiscriminator_loss: 1.1578741073608398 \tgenerator_loss: 0.9444168210029602\n",
            "For Step: 25660 recon_loss: 0.17785429954528809 \tdiscriminator_loss: 1.2096055746078491 \tgenerator_loss: 0.8714838027954102\n",
            "For Step: 25670 recon_loss: 0.17912177741527557 \tdiscriminator_loss: 1.3942092657089233 \tgenerator_loss: 0.8119139671325684\n",
            "For Step: 25680 recon_loss: 0.18778222799301147 \tdiscriminator_loss: 1.2300002574920654 \tgenerator_loss: 0.9092189073562622\n",
            "For Step: 25690 recon_loss: 0.1784580796957016 \tdiscriminator_loss: 1.216947078704834 \tgenerator_loss: 0.9285463094711304\n",
            "For Step: 25700 recon_loss: 0.16503022611141205 \tdiscriminator_loss: 1.3241523504257202 \tgenerator_loss: 0.8403115272521973\n",
            "For Step: 25710 recon_loss: 0.1830034703016281 \tdiscriminator_loss: 1.1341872215270996 \tgenerator_loss: 0.9035176038742065\n",
            "For Step: 25720 recon_loss: 0.17508269846439362 \tdiscriminator_loss: 1.298100233078003 \tgenerator_loss: 0.9156904220581055\n",
            "For Step: 25730 recon_loss: 0.17184549570083618 \tdiscriminator_loss: 1.383589267730713 \tgenerator_loss: 0.8927998542785645\n",
            "For Step: 25740 recon_loss: 0.16511952877044678 \tdiscriminator_loss: 1.1987287998199463 \tgenerator_loss: 0.9293832778930664\n",
            "For Step: 25750 recon_loss: 0.18270085752010345 \tdiscriminator_loss: 1.1936407089233398 \tgenerator_loss: 0.9223812818527222\n",
            "For Step: 25760 recon_loss: 0.18094299733638763 \tdiscriminator_loss: 1.1464719772338867 \tgenerator_loss: 0.9814803600311279\n",
            "For Step: 25770 recon_loss: 0.17898541688919067 \tdiscriminator_loss: 1.2292637825012207 \tgenerator_loss: 0.9260927438735962\n",
            "For Step: 25780 recon_loss: 0.1628381758928299 \tdiscriminator_loss: 1.242525577545166 \tgenerator_loss: 0.9204928278923035\n",
            "For Step: 25790 recon_loss: 0.18759223818778992 \tdiscriminator_loss: 1.2575880289077759 \tgenerator_loss: 0.9333140850067139\n",
            "For Step: 25800 recon_loss: 0.18387249112129211 \tdiscriminator_loss: 1.1097168922424316 \tgenerator_loss: 0.9356841444969177\n",
            "For Step: 25810 recon_loss: 0.17346015572547913 \tdiscriminator_loss: 1.3006651401519775 \tgenerator_loss: 0.8082634210586548\n",
            "For Step: 25820 recon_loss: 0.161538228392601 \tdiscriminator_loss: 1.2753353118896484 \tgenerator_loss: 0.9512287378311157\n",
            "For Step: 25830 recon_loss: 0.18913179636001587 \tdiscriminator_loss: 1.2607406377792358 \tgenerator_loss: 0.9546216726303101\n",
            "For Step: 25840 recon_loss: 0.16983744502067566 \tdiscriminator_loss: 1.2177836894989014 \tgenerator_loss: 0.9429950714111328\n",
            "For Step: 25850 recon_loss: 0.1793828308582306 \tdiscriminator_loss: 1.2263845205307007 \tgenerator_loss: 0.9373005628585815\n",
            "For Step: 25860 recon_loss: 0.17192088067531586 \tdiscriminator_loss: 1.3186697959899902 \tgenerator_loss: 0.9091088771820068\n",
            "For Step: 25870 recon_loss: 0.18214866518974304 \tdiscriminator_loss: 1.2979966402053833 \tgenerator_loss: 0.8982754349708557\n",
            "For Step: 25880 recon_loss: 0.17040511965751648 \tdiscriminator_loss: 1.3190019130706787 \tgenerator_loss: 0.907681941986084\n",
            "For Step: 25890 recon_loss: 0.17266547679901123 \tdiscriminator_loss: 1.3375837802886963 \tgenerator_loss: 0.8954383134841919\n",
            "For Step: 25900 recon_loss: 0.17723320424556732 \tdiscriminator_loss: 1.2973380088806152 \tgenerator_loss: 0.8819767236709595\n",
            "For Step: 25910 recon_loss: 0.18534189462661743 \tdiscriminator_loss: 1.1237750053405762 \tgenerator_loss: 0.9865598678588867\n",
            "For Step: 25920 recon_loss: 0.16721269488334656 \tdiscriminator_loss: 1.3259518146514893 \tgenerator_loss: 0.8472959995269775\n",
            "For Step: 25930 recon_loss: 0.1756419986486435 \tdiscriminator_loss: 1.220740556716919 \tgenerator_loss: 0.9368914365768433\n",
            "For Step: 25940 recon_loss: 0.17441336810588837 \tdiscriminator_loss: 1.1622209548950195 \tgenerator_loss: 0.9496572613716125\n",
            "For Step: 25950 recon_loss: 0.17365695536136627 \tdiscriminator_loss: 1.2014765739440918 \tgenerator_loss: 0.9079416394233704\n",
            "For Step: 25960 recon_loss: 0.17895427346229553 \tdiscriminator_loss: 1.1461918354034424 \tgenerator_loss: 0.9025579690933228\n",
            "For Step: 25970 recon_loss: 0.16071264445781708 \tdiscriminator_loss: 1.2111899852752686 \tgenerator_loss: 0.9085158705711365\n",
            "For Step: 25980 recon_loss: 0.18708330392837524 \tdiscriminator_loss: 1.173732042312622 \tgenerator_loss: 0.9340485334396362\n",
            "For Step: 25990 recon_loss: 0.17198185622692108 \tdiscriminator_loss: 1.2872817516326904 \tgenerator_loss: 0.8616819977760315\n",
            "For Step: 26000 recon_loss: 0.16611233353614807 \tdiscriminator_loss: 1.2952799797058105 \tgenerator_loss: 0.9185696840286255\n",
            "For Step: 26010 recon_loss: 0.17987246811389923 \tdiscriminator_loss: 1.1389760971069336 \tgenerator_loss: 0.9312841892242432\n",
            "For Step: 26020 recon_loss: 0.18195554614067078 \tdiscriminator_loss: 1.165321707725525 \tgenerator_loss: 0.9797489643096924\n",
            "For Step: 26030 recon_loss: 0.1534353494644165 \tdiscriminator_loss: 1.2380216121673584 \tgenerator_loss: 0.8964210152626038\n",
            "For Step: 26040 recon_loss: 0.17760753631591797 \tdiscriminator_loss: 1.2209489345550537 \tgenerator_loss: 0.883611261844635\n",
            "For Step: 26050 recon_loss: 0.18399004638195038 \tdiscriminator_loss: 1.2222390174865723 \tgenerator_loss: 0.9554792642593384\n",
            "For Step: 26060 recon_loss: 0.17845454812049866 \tdiscriminator_loss: 1.260910987854004 \tgenerator_loss: 0.8734652996063232\n",
            "For Step: 26070 recon_loss: 0.17865994572639465 \tdiscriminator_loss: 1.2540720701217651 \tgenerator_loss: 0.8886904716491699\n",
            "For Step: 26080 recon_loss: 0.1676086187362671 \tdiscriminator_loss: 1.2047972679138184 \tgenerator_loss: 0.9134372472763062\n",
            "For Step: 26090 recon_loss: 0.17440854012966156 \tdiscriminator_loss: 1.252490520477295 \tgenerator_loss: 0.8986411094665527\n",
            "For Step: 26100 recon_loss: 0.16414597630500793 \tdiscriminator_loss: 1.1922001838684082 \tgenerator_loss: 0.9285038709640503\n",
            "For Step: 26110 recon_loss: 0.1762019544839859 \tdiscriminator_loss: 1.2923067808151245 \tgenerator_loss: 0.9259542226791382\n",
            "For Step: 26120 recon_loss: 0.17924562096595764 \tdiscriminator_loss: 1.4022183418273926 \tgenerator_loss: 0.8622219562530518\n",
            "For Step: 26130 recon_loss: 0.16989928483963013 \tdiscriminator_loss: 1.3197360038757324 \tgenerator_loss: 0.8577651977539062\n",
            "For Step: 26140 recon_loss: 0.170866921544075 \tdiscriminator_loss: 1.1759825944900513 \tgenerator_loss: 0.9943833351135254\n",
            "For Step: 26150 recon_loss: 0.1782115399837494 \tdiscriminator_loss: 1.2500038146972656 \tgenerator_loss: 0.9506039023399353\n",
            "For Step: 26160 recon_loss: 0.1802239567041397 \tdiscriminator_loss: 1.2716503143310547 \tgenerator_loss: 0.863724410533905\n",
            "For Step: 26170 recon_loss: 0.16230496764183044 \tdiscriminator_loss: 1.2608473300933838 \tgenerator_loss: 0.9868671894073486\n",
            "For Step: 26180 recon_loss: 0.1731271594762802 \tdiscriminator_loss: 1.2285667657852173 \tgenerator_loss: 0.8937543034553528\n",
            "For Step: 26190 recon_loss: 0.1838023066520691 \tdiscriminator_loss: 1.1762803792953491 \tgenerator_loss: 0.9032652378082275\n",
            "For Step: 26200 recon_loss: 0.17526555061340332 \tdiscriminator_loss: 1.2083662748336792 \tgenerator_loss: 0.9448180794715881\n",
            "For Step: 26210 recon_loss: 0.170991450548172 \tdiscriminator_loss: 1.2575262784957886 \tgenerator_loss: 0.889285147190094\n",
            "For Step: 26220 recon_loss: 0.18243546783924103 \tdiscriminator_loss: 1.3009101152420044 \tgenerator_loss: 0.9741419553756714\n",
            "For Step: 26230 recon_loss: 0.17526043951511383 \tdiscriminator_loss: 1.2309865951538086 \tgenerator_loss: 0.885269284248352\n",
            "For Step: 26240 recon_loss: 0.1575632244348526 \tdiscriminator_loss: 1.1959084272384644 \tgenerator_loss: 0.8742493391036987\n",
            "For Step: 26250 recon_loss: 0.17136706411838531 \tdiscriminator_loss: 1.3378710746765137 \tgenerator_loss: 0.8065834045410156\n",
            "For Step: 26260 recon_loss: 0.17868295311927795 \tdiscriminator_loss: 1.3360567092895508 \tgenerator_loss: 0.8935228586196899\n",
            "For Step: 26270 recon_loss: 0.17758230865001678 \tdiscriminator_loss: 1.1074330806732178 \tgenerator_loss: 0.9535874128341675\n",
            "For Step: 26280 recon_loss: 0.15524227917194366 \tdiscriminator_loss: 1.3224818706512451 \tgenerator_loss: 0.8270138502120972\n",
            "For Step: 26290 recon_loss: 0.183242529630661 \tdiscriminator_loss: 1.2912083864212036 \tgenerator_loss: 0.8764318823814392\n",
            "For Step: 26300 recon_loss: 0.18378669023513794 \tdiscriminator_loss: 1.2848749160766602 \tgenerator_loss: 0.8606041669845581\n",
            "For Step: 26310 recon_loss: 0.17690250277519226 \tdiscriminator_loss: 1.2598965167999268 \tgenerator_loss: 0.8815038204193115\n",
            "For Step: 26320 recon_loss: 0.1757706254720688 \tdiscriminator_loss: 1.2677958011627197 \tgenerator_loss: 0.9149848222732544\n",
            "For Step: 26330 recon_loss: 0.18586300313472748 \tdiscriminator_loss: 1.281074047088623 \tgenerator_loss: 0.9473904371261597\n",
            "For Step: 26340 recon_loss: 0.18150076270103455 \tdiscriminator_loss: 1.1727203130722046 \tgenerator_loss: 0.9771304130554199\n",
            "For Step: 26350 recon_loss: 0.17309486865997314 \tdiscriminator_loss: 1.2330071926116943 \tgenerator_loss: 0.88151615858078\n",
            "For Step: 26360 recon_loss: 0.17560552060604095 \tdiscriminator_loss: 1.344696044921875 \tgenerator_loss: 0.9023429155349731\n",
            "For Step: 26370 recon_loss: 0.1771015226840973 \tdiscriminator_loss: 1.2423276901245117 \tgenerator_loss: 0.8996719121932983\n",
            "For Step: 26380 recon_loss: 0.1686200052499771 \tdiscriminator_loss: 1.2818233966827393 \tgenerator_loss: 0.9397676587104797\n",
            "For Step: 26390 recon_loss: 0.17256347835063934 \tdiscriminator_loss: 1.1896545886993408 \tgenerator_loss: 0.8955441117286682\n",
            "For Step: 26400 recon_loss: 0.17481638491153717 \tdiscriminator_loss: 1.2016339302062988 \tgenerator_loss: 0.8801154494285583\n",
            "For Step: 26410 recon_loss: 0.15144826471805573 \tdiscriminator_loss: 1.2123640775680542 \tgenerator_loss: 0.8765907287597656\n",
            "For Step: 26420 recon_loss: 0.1843874603509903 \tdiscriminator_loss: 1.2295329570770264 \tgenerator_loss: 0.9551348686218262\n",
            "For Step: 26430 recon_loss: 0.1774665266275406 \tdiscriminator_loss: 1.1576392650604248 \tgenerator_loss: 0.9698599576950073\n",
            "For Step: 26440 recon_loss: 0.17904120683670044 \tdiscriminator_loss: 1.2303602695465088 \tgenerator_loss: 0.8786033391952515\n",
            "For Step: 26450 recon_loss: 0.16516296565532684 \tdiscriminator_loss: 1.1657447814941406 \tgenerator_loss: 0.979200541973114\n",
            "For Step: 26460 recon_loss: 0.17881597578525543 \tdiscriminator_loss: 1.1366949081420898 \tgenerator_loss: 0.8817868828773499\n",
            "For Step: 26470 recon_loss: 0.1756938248872757 \tdiscriminator_loss: 1.27194344997406 \tgenerator_loss: 0.8967700600624084\n",
            "For Step: 26480 recon_loss: 0.18694444000720978 \tdiscriminator_loss: 1.2463734149932861 \tgenerator_loss: 0.906694769859314\n",
            "For Step: 26490 recon_loss: 0.17971205711364746 \tdiscriminator_loss: 1.2035874128341675 \tgenerator_loss: 0.9334676861763\n",
            "For Step: 26500 recon_loss: 0.1809505671262741 \tdiscriminator_loss: 1.2091901302337646 \tgenerator_loss: 0.9465271234512329\n",
            "For Step: 26510 recon_loss: 0.1751190423965454 \tdiscriminator_loss: 1.3282043933868408 \tgenerator_loss: 0.8539191484451294\n",
            "For Step: 26520 recon_loss: 0.17260640859603882 \tdiscriminator_loss: 1.2941808700561523 \tgenerator_loss: 0.8464663028717041\n",
            "For Step: 26530 recon_loss: 0.1885441243648529 \tdiscriminator_loss: 1.2573704719543457 \tgenerator_loss: 0.9179103374481201\n",
            "For Step: 26540 recon_loss: 0.1784753054380417 \tdiscriminator_loss: 1.1797301769256592 \tgenerator_loss: 0.9047258496284485\n",
            "For Step: 26550 recon_loss: 0.18474170565605164 \tdiscriminator_loss: 1.262598991394043 \tgenerator_loss: 0.9297903180122375\n",
            "For Step: 26560 recon_loss: 0.17879287898540497 \tdiscriminator_loss: 1.2325477600097656 \tgenerator_loss: 0.9070571660995483\n",
            "For Step: 26570 recon_loss: 0.16051271557807922 \tdiscriminator_loss: 1.0991146564483643 \tgenerator_loss: 0.876082181930542\n",
            "For Step: 26580 recon_loss: 0.17087610065937042 \tdiscriminator_loss: 1.1799736022949219 \tgenerator_loss: 0.9557453989982605\n",
            "For Step: 26590 recon_loss: 0.16407278180122375 \tdiscriminator_loss: 1.2101020812988281 \tgenerator_loss: 0.9362689256668091\n",
            "For Step: 26600 recon_loss: 0.16583804786205292 \tdiscriminator_loss: 1.3195945024490356 \tgenerator_loss: 0.9006682634353638\n",
            "For Step: 26610 recon_loss: 0.1794556826353073 \tdiscriminator_loss: 1.2907874584197998 \tgenerator_loss: 0.8676465749740601\n",
            "For Step: 26620 recon_loss: 0.16507823765277863 \tdiscriminator_loss: 1.2702884674072266 \tgenerator_loss: 0.8442573547363281\n",
            "For Step: 26630 recon_loss: 0.18705017864704132 \tdiscriminator_loss: 1.2484828233718872 \tgenerator_loss: 0.9686973094940186\n",
            "For Step: 26640 recon_loss: 0.17570100724697113 \tdiscriminator_loss: 1.1803412437438965 \tgenerator_loss: 0.9092631340026855\n",
            "For Step: 26650 recon_loss: 0.17138487100601196 \tdiscriminator_loss: 1.2301928997039795 \tgenerator_loss: 0.9044234752655029\n",
            "For Step: 26660 recon_loss: 0.17536675930023193 \tdiscriminator_loss: 1.18412184715271 \tgenerator_loss: 0.9116669297218323\n",
            "For Step: 26670 recon_loss: 0.16163961589336395 \tdiscriminator_loss: 1.1957412958145142 \tgenerator_loss: 0.8734994530677795\n",
            "For Step: 26680 recon_loss: 0.18371045589447021 \tdiscriminator_loss: 1.2326645851135254 \tgenerator_loss: 0.9122273921966553\n",
            "For Step: 26690 recon_loss: 0.17806358635425568 \tdiscriminator_loss: 1.2624801397323608 \tgenerator_loss: 0.8513596653938293\n",
            "For Step: 26700 recon_loss: 0.18434523046016693 \tdiscriminator_loss: 1.2281701564788818 \tgenerator_loss: 0.917952835559845\n",
            "For Step: 26710 recon_loss: 0.16685786843299866 \tdiscriminator_loss: 1.2705854177474976 \tgenerator_loss: 0.9279513359069824\n",
            "For Step: 26720 recon_loss: 0.17319659888744354 \tdiscriminator_loss: 1.3327240943908691 \tgenerator_loss: 0.852861762046814\n",
            "For Step: 26730 recon_loss: 0.175296351313591 \tdiscriminator_loss: 1.225356101989746 \tgenerator_loss: 0.9219725131988525\n",
            "For Step: 26740 recon_loss: 0.16481627523899078 \tdiscriminator_loss: 1.159948706626892 \tgenerator_loss: 0.9339154958724976\n",
            "For Step: 26750 recon_loss: 0.1801297962665558 \tdiscriminator_loss: 1.196730136871338 \tgenerator_loss: 0.8520491123199463\n",
            "For Step: 26760 recon_loss: 0.17843405902385712 \tdiscriminator_loss: 1.2328848838806152 \tgenerator_loss: 0.9030227065086365\n",
            "For Step: 26770 recon_loss: 0.18094144761562347 \tdiscriminator_loss: 1.197807788848877 \tgenerator_loss: 0.9394281506538391\n",
            "For Step: 26780 recon_loss: 0.1766672283411026 \tdiscriminator_loss: 1.3421945571899414 \tgenerator_loss: 0.8795916438102722\n",
            "For Step: 26790 recon_loss: 0.1676018238067627 \tdiscriminator_loss: 1.1594301462173462 \tgenerator_loss: 0.9332157373428345\n",
            "For Step: 26800 recon_loss: 0.16861312091350555 \tdiscriminator_loss: 1.1438053846359253 \tgenerator_loss: 0.8917264342308044\n",
            "For Step: 26810 recon_loss: 0.1703065037727356 \tdiscriminator_loss: 1.2307807207107544 \tgenerator_loss: 0.893203854560852\n",
            "For Step: 26820 recon_loss: 0.16842550039291382 \tdiscriminator_loss: 1.3126530647277832 \tgenerator_loss: 0.868691086769104\n",
            "For Step: 26830 recon_loss: 0.17456930875778198 \tdiscriminator_loss: 1.3344385623931885 \tgenerator_loss: 0.839458703994751\n",
            "For Step: 26840 recon_loss: 0.17816676199436188 \tdiscriminator_loss: 1.210249900817871 \tgenerator_loss: 0.8935844302177429\n",
            "For Step: 26850 recon_loss: 0.16157490015029907 \tdiscriminator_loss: 1.222596526145935 \tgenerator_loss: 0.8990678191184998\n",
            "For Step: 26860 recon_loss: 0.16422498226165771 \tdiscriminator_loss: 1.2561206817626953 \tgenerator_loss: 0.9078184962272644\n",
            "For Step: 26870 recon_loss: 0.1772649586200714 \tdiscriminator_loss: 1.1477830410003662 \tgenerator_loss: 0.9269943833351135\n",
            "For Step: 26880 recon_loss: 0.17461517453193665 \tdiscriminator_loss: 1.211029052734375 \tgenerator_loss: 0.9275773763656616\n",
            "For Step: 26890 recon_loss: 0.1809159368276596 \tdiscriminator_loss: 1.2495085000991821 \tgenerator_loss: 0.9176435470581055\n",
            "For Step: 26900 recon_loss: 0.17649424076080322 \tdiscriminator_loss: 1.1185922622680664 \tgenerator_loss: 0.8931475877761841\n",
            "For Step: 26910 recon_loss: 0.17585432529449463 \tdiscriminator_loss: 1.332505464553833 \tgenerator_loss: 0.7942289113998413\n",
            "For Step: 26920 recon_loss: 0.17468711733818054 \tdiscriminator_loss: 1.2092087268829346 \tgenerator_loss: 0.9545388221740723\n",
            "For Step: 26930 recon_loss: 0.17669956386089325 \tdiscriminator_loss: 1.3054622411727905 \tgenerator_loss: 0.861706018447876\n",
            "For Step: 26940 recon_loss: 0.16351068019866943 \tdiscriminator_loss: 1.3443371057510376 \tgenerator_loss: 0.8519465327262878\n",
            "For Step: 26950 recon_loss: 0.18098850548267365 \tdiscriminator_loss: 1.1885485649108887 \tgenerator_loss: 0.9644564986228943\n",
            "For Step: 26960 recon_loss: 0.17790985107421875 \tdiscriminator_loss: 1.2281562089920044 \tgenerator_loss: 0.8855145573616028\n",
            "For Step: 26970 recon_loss: 0.17640550434589386 \tdiscriminator_loss: 1.184460163116455 \tgenerator_loss: 0.9575040340423584\n",
            "For Step: 26980 recon_loss: 0.18159161508083344 \tdiscriminator_loss: 1.3247178792953491 \tgenerator_loss: 0.8614407777786255\n",
            "For Step: 26990 recon_loss: 0.18672116100788116 \tdiscriminator_loss: 1.1944048404693604 \tgenerator_loss: 0.9265987873077393\n",
            "For Step: 27000 recon_loss: 0.17713285982608795 \tdiscriminator_loss: 1.2696231603622437 \tgenerator_loss: 0.8708612322807312\n",
            "For Step: 27010 recon_loss: 0.17628751695156097 \tdiscriminator_loss: 1.2187976837158203 \tgenerator_loss: 0.8924708366394043\n",
            "For Step: 27020 recon_loss: 0.17263329029083252 \tdiscriminator_loss: 1.1626505851745605 \tgenerator_loss: 0.9620503187179565\n",
            "For Step: 27030 recon_loss: 0.16903577744960785 \tdiscriminator_loss: 1.188642144203186 \tgenerator_loss: 0.8745217323303223\n",
            "For Step: 27040 recon_loss: 0.17506198585033417 \tdiscriminator_loss: 1.2726318836212158 \tgenerator_loss: 0.8155200481414795\n",
            "For Step: 27050 recon_loss: 0.177465558052063 \tdiscriminator_loss: 1.3056745529174805 \tgenerator_loss: 0.9169737100601196\n",
            "For Step: 27060 recon_loss: 0.15925870835781097 \tdiscriminator_loss: 1.2643070220947266 \tgenerator_loss: 0.8763900995254517\n",
            "For Step: 27070 recon_loss: 0.1889951527118683 \tdiscriminator_loss: 1.2562320232391357 \tgenerator_loss: 0.8744215965270996\n",
            "For Step: 27080 recon_loss: 0.17410531640052795 \tdiscriminator_loss: 1.2573013305664062 \tgenerator_loss: 0.8620866537094116\n",
            "For Step: 27090 recon_loss: 0.15668132901191711 \tdiscriminator_loss: 1.2644190788269043 \tgenerator_loss: 0.850040078163147\n",
            "For Step: 27100 recon_loss: 0.17541441321372986 \tdiscriminator_loss: 1.2416576147079468 \tgenerator_loss: 0.905812680721283\n",
            "For Step: 27110 recon_loss: 0.17927353084087372 \tdiscriminator_loss: 1.3044862747192383 \tgenerator_loss: 0.8254358172416687\n",
            "For Step: 27120 recon_loss: 0.1798478662967682 \tdiscriminator_loss: 1.2223196029663086 \tgenerator_loss: 0.8947988748550415\n",
            "For Step: 27130 recon_loss: 0.17119166254997253 \tdiscriminator_loss: 1.39793062210083 \tgenerator_loss: 0.8886308073997498\n",
            "For Step: 27140 recon_loss: 0.16159702837467194 \tdiscriminator_loss: 1.2437688112258911 \tgenerator_loss: 0.9163554310798645\n",
            "For Step: 27150 recon_loss: 0.17785580456256866 \tdiscriminator_loss: 1.1782872676849365 \tgenerator_loss: 0.9214438199996948\n",
            "For Step: 27160 recon_loss: 0.17419667541980743 \tdiscriminator_loss: 1.1955387592315674 \tgenerator_loss: 0.903958261013031\n",
            "For Step: 27170 recon_loss: 0.17586369812488556 \tdiscriminator_loss: 1.204545497894287 \tgenerator_loss: 0.9078073501586914\n",
            "For Step: 27180 recon_loss: 0.1721288114786148 \tdiscriminator_loss: 1.2411649227142334 \tgenerator_loss: 0.986778736114502\n",
            "For Step: 27190 recon_loss: 0.17208826541900635 \tdiscriminator_loss: 1.2157235145568848 \tgenerator_loss: 0.935280442237854\n",
            "For Step: 27200 recon_loss: 0.18146179616451263 \tdiscriminator_loss: 1.287018060684204 \tgenerator_loss: 0.8994818925857544\n",
            "For Step: 27210 recon_loss: 0.17478568851947784 \tdiscriminator_loss: 1.300187587738037 \tgenerator_loss: 0.8908548355102539\n",
            "For Step: 27220 recon_loss: 0.18008701503276825 \tdiscriminator_loss: 1.2723532915115356 \tgenerator_loss: 0.8852721452713013\n",
            "For Step: 27230 recon_loss: 0.16485515236854553 \tdiscriminator_loss: 1.2618885040283203 \tgenerator_loss: 0.8495692610740662\n",
            "For Step: 27240 recon_loss: 0.17603613436222076 \tdiscriminator_loss: 1.2721145153045654 \tgenerator_loss: 0.8942655920982361\n",
            "For Step: 27250 recon_loss: 0.17501360177993774 \tdiscriminator_loss: 1.2484490871429443 \tgenerator_loss: 0.9005895853042603\n",
            "For Step: 27260 recon_loss: 0.1843891739845276 \tdiscriminator_loss: 1.1363040208816528 \tgenerator_loss: 0.9528787136077881\n",
            "For Step: 27270 recon_loss: 0.18119685351848602 \tdiscriminator_loss: 1.4100720882415771 \tgenerator_loss: 0.846268892288208\n",
            "For Step: 27280 recon_loss: 0.17531269788742065 \tdiscriminator_loss: 1.103635311126709 \tgenerator_loss: 0.9530059695243835\n",
            "For Step: 27290 recon_loss: 0.16526156663894653 \tdiscriminator_loss: 1.3336491584777832 \tgenerator_loss: 0.8847439885139465\n",
            "For Step: 27300 recon_loss: 0.17724677920341492 \tdiscriminator_loss: 1.2432212829589844 \tgenerator_loss: 0.9149259328842163\n",
            "For Step: 27310 recon_loss: 0.19299112260341644 \tdiscriminator_loss: 1.2124230861663818 \tgenerator_loss: 0.8903001546859741\n",
            "For Step: 27320 recon_loss: 0.18144774436950684 \tdiscriminator_loss: 1.1886439323425293 \tgenerator_loss: 0.9377756714820862\n",
            "For Step: 27330 recon_loss: 0.18368034064769745 \tdiscriminator_loss: 1.2991856336593628 \tgenerator_loss: 0.7724605798721313\n",
            "For Step: 27340 recon_loss: 0.165069118142128 \tdiscriminator_loss: 1.241203784942627 \tgenerator_loss: 0.9032473564147949\n",
            "For Step: 27350 recon_loss: 0.18000219762325287 \tdiscriminator_loss: 1.3669826984405518 \tgenerator_loss: 0.8367879390716553\n",
            "For Step: 27360 recon_loss: 0.18384259939193726 \tdiscriminator_loss: 1.1666884422302246 \tgenerator_loss: 0.9174250364303589\n",
            "For Step: 27370 recon_loss: 0.17850013077259064 \tdiscriminator_loss: 1.2625694274902344 \tgenerator_loss: 0.953835666179657\n",
            "For Step: 27380 recon_loss: 0.1729000210762024 \tdiscriminator_loss: 1.2663257122039795 \tgenerator_loss: 0.8509473204612732\n",
            "For Step: 27390 recon_loss: 0.16262972354888916 \tdiscriminator_loss: 1.2317531108856201 \tgenerator_loss: 0.8556206226348877\n",
            "For Step: 27400 recon_loss: 0.1638226956129074 \tdiscriminator_loss: 1.177441120147705 \tgenerator_loss: 0.9560942053794861\n",
            "For Step: 27410 recon_loss: 0.17649003863334656 \tdiscriminator_loss: 1.197725534439087 \tgenerator_loss: 0.9222664833068848\n",
            "For Step: 27420 recon_loss: 0.16881994903087616 \tdiscriminator_loss: 1.1688752174377441 \tgenerator_loss: 0.894119918346405\n",
            "For Step: 27430 recon_loss: 0.176269993185997 \tdiscriminator_loss: 1.1384315490722656 \tgenerator_loss: 0.9146969318389893\n",
            "For Step: 27440 recon_loss: 0.17646746337413788 \tdiscriminator_loss: 1.282975196838379 \tgenerator_loss: 0.9347367286682129\n",
            "For Step: 27450 recon_loss: 0.16681113839149475 \tdiscriminator_loss: 1.226423740386963 \tgenerator_loss: 0.8804568648338318\n",
            "For Step: 27460 recon_loss: 0.17812061309814453 \tdiscriminator_loss: 1.2044936418533325 \tgenerator_loss: 0.9650938510894775\n",
            "For Step: 27470 recon_loss: 0.17010286450386047 \tdiscriminator_loss: 1.2105363607406616 \tgenerator_loss: 0.8848111629486084\n",
            "For Step: 27480 recon_loss: 0.17011888325214386 \tdiscriminator_loss: 1.2406296730041504 \tgenerator_loss: 0.9196601510047913\n",
            "For Step: 27490 recon_loss: 0.17157460749149323 \tdiscriminator_loss: 1.210164189338684 \tgenerator_loss: 0.8791245818138123\n",
            "For Step: 27500 recon_loss: 0.16949430108070374 \tdiscriminator_loss: 1.2558162212371826 \tgenerator_loss: 0.906169593334198\n",
            "For Step: 27510 recon_loss: 0.17921045422554016 \tdiscriminator_loss: 1.2347438335418701 \tgenerator_loss: 0.9339832663536072\n",
            "For Step: 27520 recon_loss: 0.1620359867811203 \tdiscriminator_loss: 1.2928550243377686 \tgenerator_loss: 0.9245724678039551\n",
            "For Step: 27530 recon_loss: 0.17368267476558685 \tdiscriminator_loss: 1.3055238723754883 \tgenerator_loss: 0.946627140045166\n",
            "For Step: 27540 recon_loss: 0.1773243099451065 \tdiscriminator_loss: 1.3010268211364746 \tgenerator_loss: 0.9530898332595825\n",
            "For Step: 27550 recon_loss: 0.17539465427398682 \tdiscriminator_loss: 1.3014347553253174 \tgenerator_loss: 0.8591158986091614\n",
            "For Step: 27560 recon_loss: 0.189971461892128 \tdiscriminator_loss: 1.3233641386032104 \tgenerator_loss: 0.80206698179245\n",
            "For Step: 27570 recon_loss: 0.176733136177063 \tdiscriminator_loss: 1.2623549699783325 \tgenerator_loss: 0.8346344232559204\n",
            "For Step: 27580 recon_loss: 0.16627450287342072 \tdiscriminator_loss: 1.2156904935836792 \tgenerator_loss: 0.8543091416358948\n",
            "For Step: 27590 recon_loss: 0.1602676808834076 \tdiscriminator_loss: 1.3602324724197388 \tgenerator_loss: 0.8633058667182922\n",
            "For Step: 27600 recon_loss: 0.16909249126911163 \tdiscriminator_loss: 1.2897534370422363 \tgenerator_loss: 0.8516677618026733\n",
            "For Step: 27610 recon_loss: 0.17747478187084198 \tdiscriminator_loss: 1.1341854333877563 \tgenerator_loss: 0.9547746181488037\n",
            "For Step: 27620 recon_loss: 0.17610977590084076 \tdiscriminator_loss: 1.2874343395233154 \tgenerator_loss: 0.8978290557861328\n",
            "For Step: 27630 recon_loss: 0.1801600605249405 \tdiscriminator_loss: 1.2239902019500732 \tgenerator_loss: 0.8926898241043091\n",
            "For Step: 27640 recon_loss: 0.17352306842803955 \tdiscriminator_loss: 1.323932409286499 \tgenerator_loss: 0.8728290796279907\n",
            "For Step: 27650 recon_loss: 0.16782672703266144 \tdiscriminator_loss: 1.2505340576171875 \tgenerator_loss: 0.9403114318847656\n",
            "For Step: 27660 recon_loss: 0.16639065742492676 \tdiscriminator_loss: 1.1662087440490723 \tgenerator_loss: 0.9018478393554688\n",
            "For Step: 27670 recon_loss: 0.16178156435489655 \tdiscriminator_loss: 1.1304908990859985 \tgenerator_loss: 0.9109630584716797\n",
            "For Step: 27680 recon_loss: 0.16079381108283997 \tdiscriminator_loss: 1.310478687286377 \tgenerator_loss: 0.8613675236701965\n",
            "For Step: 27690 recon_loss: 0.16235020756721497 \tdiscriminator_loss: 1.2520239353179932 \tgenerator_loss: 0.9038723111152649\n",
            "For Step: 27700 recon_loss: 0.16472408175468445 \tdiscriminator_loss: 1.1710137128829956 \tgenerator_loss: 0.9064044952392578\n",
            "For Step: 27710 recon_loss: 0.1691790074110031 \tdiscriminator_loss: 1.1952015161514282 \tgenerator_loss: 0.8930526971817017\n",
            "For Step: 27720 recon_loss: 0.17579738795757294 \tdiscriminator_loss: 1.2383713722229004 \tgenerator_loss: 0.916479229927063\n",
            "For Step: 27730 recon_loss: 0.16990584135055542 \tdiscriminator_loss: 1.1600358486175537 \tgenerator_loss: 0.8773478269577026\n",
            "For Step: 27740 recon_loss: 0.17085577547550201 \tdiscriminator_loss: 1.170175552368164 \tgenerator_loss: 0.919437050819397\n",
            "For Step: 27750 recon_loss: 0.17836590111255646 \tdiscriminator_loss: 1.275969386100769 \tgenerator_loss: 0.8584358096122742\n",
            "For Step: 27760 recon_loss: 0.1675834208726883 \tdiscriminator_loss: 1.2054789066314697 \tgenerator_loss: 0.8999348282814026\n",
            "For Step: 27770 recon_loss: 0.17598438262939453 \tdiscriminator_loss: 1.301741361618042 \tgenerator_loss: 0.9082455635070801\n",
            "For Step: 27780 recon_loss: 0.18056029081344604 \tdiscriminator_loss: 1.2223150730133057 \tgenerator_loss: 0.9551645517349243\n",
            "For Step: 27790 recon_loss: 0.16660355031490326 \tdiscriminator_loss: 1.289204716682434 \tgenerator_loss: 0.8389813899993896\n",
            "For Step: 27800 recon_loss: 0.16794084012508392 \tdiscriminator_loss: 1.2196664810180664 \tgenerator_loss: 0.8636629581451416\n",
            "For Step: 27810 recon_loss: 0.1804235875606537 \tdiscriminator_loss: 1.35918128490448 \tgenerator_loss: 0.884303092956543\n",
            "For Step: 27820 recon_loss: 0.17946597933769226 \tdiscriminator_loss: 1.1380946636199951 \tgenerator_loss: 0.9363175630569458\n",
            "For Step: 27830 recon_loss: 0.19139544665813446 \tdiscriminator_loss: 1.2362439632415771 \tgenerator_loss: 0.901714563369751\n",
            "For Step: 27840 recon_loss: 0.17021536827087402 \tdiscriminator_loss: 1.2468993663787842 \tgenerator_loss: 0.8940189480781555\n",
            "For Step: 27850 recon_loss: 0.17958609759807587 \tdiscriminator_loss: 1.2646903991699219 \tgenerator_loss: 0.9160909056663513\n",
            "For Step: 27860 recon_loss: 0.16220229864120483 \tdiscriminator_loss: 1.2633519172668457 \tgenerator_loss: 0.8870807886123657\n",
            "For Step: 27870 recon_loss: 0.17752012610435486 \tdiscriminator_loss: 1.288465976715088 \tgenerator_loss: 0.923607349395752\n",
            "For Step: 27880 recon_loss: 0.16841283440589905 \tdiscriminator_loss: 1.2815862894058228 \tgenerator_loss: 0.8484921455383301\n",
            "For Step: 27890 recon_loss: 0.17528358101844788 \tdiscriminator_loss: 1.3999991416931152 \tgenerator_loss: 0.8454574346542358\n",
            "For Step: 27900 recon_loss: 0.17770616710186005 \tdiscriminator_loss: 1.385108232498169 \tgenerator_loss: 0.8429067134857178\n",
            "For Step: 27910 recon_loss: 0.17080910503864288 \tdiscriminator_loss: 1.2282016277313232 \tgenerator_loss: 0.9441777467727661\n",
            "For Step: 27920 recon_loss: 0.17688331007957458 \tdiscriminator_loss: 1.191886067390442 \tgenerator_loss: 0.9471495151519775\n",
            "For Step: 27930 recon_loss: 0.1791539490222931 \tdiscriminator_loss: 1.22542142868042 \tgenerator_loss: 0.889044463634491\n",
            "For Step: 27940 recon_loss: 0.15637828409671783 \tdiscriminator_loss: 1.2475829124450684 \tgenerator_loss: 0.8653833270072937\n",
            "For Step: 27950 recon_loss: 0.1724712997674942 \tdiscriminator_loss: 1.104602336883545 \tgenerator_loss: 0.9677218794822693\n",
            "For Step: 27960 recon_loss: 0.157311350107193 \tdiscriminator_loss: 1.273640751838684 \tgenerator_loss: 0.8747254610061646\n",
            "For Step: 27970 recon_loss: 0.1718727946281433 \tdiscriminator_loss: 1.2465718984603882 \tgenerator_loss: 0.8397108912467957\n",
            "For Step: 27980 recon_loss: 0.17902176082134247 \tdiscriminator_loss: 1.330067753791809 \tgenerator_loss: 0.8537458181381226\n",
            "For Step: 27990 recon_loss: 0.17644435167312622 \tdiscriminator_loss: 1.2194808721542358 \tgenerator_loss: 0.9645150303840637\n",
            "For Step: 28000 recon_loss: 0.17500698566436768 \tdiscriminator_loss: 1.1580345630645752 \tgenerator_loss: 0.9242090582847595\n",
            "For Step: 28010 recon_loss: 0.17451301217079163 \tdiscriminator_loss: 1.2521114349365234 \tgenerator_loss: 0.8911564350128174\n",
            "For Step: 28020 recon_loss: 0.17508631944656372 \tdiscriminator_loss: 1.2651922702789307 \tgenerator_loss: 0.8877301216125488\n",
            "For Step: 28030 recon_loss: 0.16723404824733734 \tdiscriminator_loss: 1.228097677230835 \tgenerator_loss: 0.9408621788024902\n",
            "For Step: 28040 recon_loss: 0.1778849959373474 \tdiscriminator_loss: 1.1952791213989258 \tgenerator_loss: 0.9131690263748169\n",
            "For Step: 28050 recon_loss: 0.17457211017608643 \tdiscriminator_loss: 1.2218170166015625 \tgenerator_loss: 0.9215770363807678\n",
            "For Step: 28060 recon_loss: 0.19321773946285248 \tdiscriminator_loss: 1.3062429428100586 \tgenerator_loss: 0.8459979295730591\n",
            "For Step: 28070 recon_loss: 0.16828544437885284 \tdiscriminator_loss: 1.3494421243667603 \tgenerator_loss: 0.8694767951965332\n",
            "For Step: 28080 recon_loss: 0.17079664766788483 \tdiscriminator_loss: 1.1946277618408203 \tgenerator_loss: 0.9403303265571594\n",
            "For Step: 28090 recon_loss: 0.17399127781391144 \tdiscriminator_loss: 1.2478206157684326 \tgenerator_loss: 0.9121264219284058\n",
            "For Step: 28100 recon_loss: 0.17633379995822906 \tdiscriminator_loss: 1.2215180397033691 \tgenerator_loss: 0.9183761477470398\n",
            "For Step: 28110 recon_loss: 0.1666945070028305 \tdiscriminator_loss: 1.277116298675537 \tgenerator_loss: 0.8555800318717957\n",
            "For Step: 28120 recon_loss: 0.1903817057609558 \tdiscriminator_loss: 1.25472891330719 \tgenerator_loss: 0.8568445444107056\n",
            "For Step: 28130 recon_loss: 0.17317405343055725 \tdiscriminator_loss: 1.2359905242919922 \tgenerator_loss: 0.906533420085907\n",
            "For Step: 28140 recon_loss: 0.17401038110256195 \tdiscriminator_loss: 1.2627253532409668 \tgenerator_loss: 0.9144643545150757\n",
            "For Step: 28150 recon_loss: 0.17957013845443726 \tdiscriminator_loss: 1.2077217102050781 \tgenerator_loss: 0.8807978630065918\n",
            "For Step: 28160 recon_loss: 0.16883686184883118 \tdiscriminator_loss: 1.195141315460205 \tgenerator_loss: 0.956337571144104\n",
            "For Step: 28170 recon_loss: 0.16018618643283844 \tdiscriminator_loss: 1.2895392179489136 \tgenerator_loss: 0.9276269674301147\n",
            "For Step: 28180 recon_loss: 0.17333737015724182 \tdiscriminator_loss: 1.1577891111373901 \tgenerator_loss: 0.899215042591095\n",
            "For Step: 28190 recon_loss: 0.16114045679569244 \tdiscriminator_loss: 1.2441484928131104 \tgenerator_loss: 0.8442139625549316\n",
            "For Step: 28200 recon_loss: 0.17993409931659698 \tdiscriminator_loss: 1.2425613403320312 \tgenerator_loss: 0.8768153190612793\n",
            "For Step: 28210 recon_loss: 0.1783743053674698 \tdiscriminator_loss: 1.1547850370407104 \tgenerator_loss: 1.007735013961792\n",
            "For Step: 28220 recon_loss: 0.1744292676448822 \tdiscriminator_loss: 1.2495490312576294 \tgenerator_loss: 0.9245747327804565\n",
            "For Step: 28230 recon_loss: 0.18481561541557312 \tdiscriminator_loss: 1.2147018909454346 \tgenerator_loss: 0.8716146945953369\n",
            "For Step: 28240 recon_loss: 0.1911010891199112 \tdiscriminator_loss: 1.3193597793579102 \tgenerator_loss: 0.9046735167503357\n",
            "For Step: 28250 recon_loss: 0.1712735891342163 \tdiscriminator_loss: 1.2420270442962646 \tgenerator_loss: 0.8956190347671509\n",
            "For Step: 28260 recon_loss: 0.17125535011291504 \tdiscriminator_loss: 1.3310853242874146 \tgenerator_loss: 0.8706141710281372\n",
            "For Step: 28270 recon_loss: 0.18089033663272858 \tdiscriminator_loss: 1.1995060443878174 \tgenerator_loss: 0.891979992389679\n",
            "For Step: 28280 recon_loss: 0.17486567795276642 \tdiscriminator_loss: 1.3382158279418945 \tgenerator_loss: 0.9148553609848022\n",
            "For Step: 28290 recon_loss: 0.1758056879043579 \tdiscriminator_loss: 1.2128139734268188 \tgenerator_loss: 0.9460476040840149\n",
            "For Step: 28300 recon_loss: 0.18781767785549164 \tdiscriminator_loss: 1.2565068006515503 \tgenerator_loss: 0.905951201915741\n",
            "For Step: 28310 recon_loss: 0.15805065631866455 \tdiscriminator_loss: 1.3169848918914795 \tgenerator_loss: 0.9141336679458618\n",
            "For Step: 28320 recon_loss: 0.1715022474527359 \tdiscriminator_loss: 1.3525177240371704 \tgenerator_loss: 0.8544329404830933\n",
            "For Step: 28330 recon_loss: 0.15836434066295624 \tdiscriminator_loss: 1.3758890628814697 \tgenerator_loss: 0.8369219899177551\n",
            "For Step: 28340 recon_loss: 0.17062769830226898 \tdiscriminator_loss: 1.3004937171936035 \tgenerator_loss: 0.964347243309021\n",
            "For Step: 28350 recon_loss: 0.16551454365253448 \tdiscriminator_loss: 1.287018060684204 \tgenerator_loss: 0.8765282034873962\n",
            "For Step: 28360 recon_loss: 0.16387489438056946 \tdiscriminator_loss: 1.2168922424316406 \tgenerator_loss: 0.9093252420425415\n",
            "For Step: 28370 recon_loss: 0.16617761552333832 \tdiscriminator_loss: 1.340545415878296 \tgenerator_loss: 0.8963266611099243\n",
            "For Step: 28380 recon_loss: 0.17421378195285797 \tdiscriminator_loss: 1.3098945617675781 \tgenerator_loss: 0.8464359045028687\n",
            "For Step: 28390 recon_loss: 0.16949425637722015 \tdiscriminator_loss: 1.2474660873413086 \tgenerator_loss: 0.8969846367835999\n",
            "For Step: 28400 recon_loss: 0.1657377928495407 \tdiscriminator_loss: 1.3747992515563965 \tgenerator_loss: 0.8187193870544434\n",
            "For Step: 28410 recon_loss: 0.1731332242488861 \tdiscriminator_loss: 1.3726186752319336 \tgenerator_loss: 0.8510563969612122\n",
            "For Step: 28420 recon_loss: 0.1717301905155182 \tdiscriminator_loss: 1.2375564575195312 \tgenerator_loss: 0.9342143535614014\n",
            "For Step: 28430 recon_loss: 0.1630043238401413 \tdiscriminator_loss: 1.2863743305206299 \tgenerator_loss: 0.8974074125289917\n",
            "For Step: 28440 recon_loss: 0.16146530210971832 \tdiscriminator_loss: 1.240419626235962 \tgenerator_loss: 0.8872352242469788\n",
            "For Step: 28450 recon_loss: 0.18427638709545135 \tdiscriminator_loss: 1.2685258388519287 \tgenerator_loss: 0.8910641074180603\n",
            "For Step: 28460 recon_loss: 0.17692774534225464 \tdiscriminator_loss: 1.2149770259857178 \tgenerator_loss: 0.8605841398239136\n",
            "For Step: 28470 recon_loss: 0.17040181159973145 \tdiscriminator_loss: 1.2054921388626099 \tgenerator_loss: 0.9059519171714783\n",
            "For Step: 28480 recon_loss: 0.17997972667217255 \tdiscriminator_loss: 1.2880098819732666 \tgenerator_loss: 0.8088178038597107\n",
            "For Step: 28490 recon_loss: 0.15832921862602234 \tdiscriminator_loss: 1.2669329643249512 \tgenerator_loss: 0.8644998073577881\n",
            "For Step: 28500 recon_loss: 0.17483003437519073 \tdiscriminator_loss: 1.1828665733337402 \tgenerator_loss: 0.9095668792724609\n",
            "For Step: 28510 recon_loss: 0.17799091339111328 \tdiscriminator_loss: 1.196317434310913 \tgenerator_loss: 0.9076724052429199\n",
            "For Step: 28520 recon_loss: 0.16900686919689178 \tdiscriminator_loss: 1.1933505535125732 \tgenerator_loss: 0.8279634714126587\n",
            "For Step: 28530 recon_loss: 0.15846136212348938 \tdiscriminator_loss: 1.3054869174957275 \tgenerator_loss: 0.8663183450698853\n",
            "For Step: 28540 recon_loss: 0.17608775198459625 \tdiscriminator_loss: 1.2464518547058105 \tgenerator_loss: 0.8987728357315063\n",
            "For Step: 28550 recon_loss: 0.16607698798179626 \tdiscriminator_loss: 1.2622594833374023 \tgenerator_loss: 0.8911703824996948\n",
            "For Step: 28560 recon_loss: 0.1736799031496048 \tdiscriminator_loss: 1.2324559688568115 \tgenerator_loss: 0.8167513608932495\n",
            "For Step: 28570 recon_loss: 0.18330375850200653 \tdiscriminator_loss: 1.2813416719436646 \tgenerator_loss: 0.8541613221168518\n",
            "For Step: 28580 recon_loss: 0.1649402678012848 \tdiscriminator_loss: 1.295788288116455 \tgenerator_loss: 0.8695498704910278\n",
            "For Step: 28590 recon_loss: 0.1702198088169098 \tdiscriminator_loss: 1.2043402194976807 \tgenerator_loss: 0.8769113421440125\n",
            "For Step: 28600 recon_loss: 0.16723567247390747 \tdiscriminator_loss: 1.2253013849258423 \tgenerator_loss: 0.8859179019927979\n",
            "For Step: 28610 recon_loss: 0.18582631647586823 \tdiscriminator_loss: 1.225293517112732 \tgenerator_loss: 0.9247410297393799\n",
            "For Step: 28620 recon_loss: 0.17402096092700958 \tdiscriminator_loss: 1.2913398742675781 \tgenerator_loss: 0.9150390625\n",
            "For Step: 28630 recon_loss: 0.16553781926631927 \tdiscriminator_loss: 1.2631127834320068 \tgenerator_loss: 0.8528110980987549\n",
            "For Step: 28640 recon_loss: 0.1680334508419037 \tdiscriminator_loss: 1.259807825088501 \tgenerator_loss: 0.8261292576789856\n",
            "For Step: 28650 recon_loss: 0.17649048566818237 \tdiscriminator_loss: 1.2419297695159912 \tgenerator_loss: 0.8858472108840942\n",
            "For Step: 28660 recon_loss: 0.17566676437854767 \tdiscriminator_loss: 1.337228775024414 \tgenerator_loss: 0.8815566301345825\n",
            "For Step: 28670 recon_loss: 0.16904371976852417 \tdiscriminator_loss: 1.3171818256378174 \tgenerator_loss: 0.8412184119224548\n",
            "For Step: 28680 recon_loss: 0.1677037626504898 \tdiscriminator_loss: 1.1980555057525635 \tgenerator_loss: 0.884303629398346\n",
            "For Step: 28690 recon_loss: 0.1762990802526474 \tdiscriminator_loss: 1.1629468202590942 \tgenerator_loss: 0.8969862461090088\n",
            "For Step: 28700 recon_loss: 0.1725926697254181 \tdiscriminator_loss: 1.2467808723449707 \tgenerator_loss: 0.9240207076072693\n",
            "For Step: 28710 recon_loss: 0.17228254675865173 \tdiscriminator_loss: 1.2429442405700684 \tgenerator_loss: 0.8951038718223572\n",
            "For Step: 28720 recon_loss: 0.16224737465381622 \tdiscriminator_loss: 1.1187223196029663 \tgenerator_loss: 0.9001877307891846\n",
            "For Step: 28730 recon_loss: 0.16676564514636993 \tdiscriminator_loss: 1.331457495689392 \tgenerator_loss: 0.855317234992981\n",
            "For Step: 28740 recon_loss: 0.1793169230222702 \tdiscriminator_loss: 1.1509439945220947 \tgenerator_loss: 0.8972991704940796\n",
            "For Step: 28750 recon_loss: 0.1730145961046219 \tdiscriminator_loss: 1.2329773902893066 \tgenerator_loss: 0.8866468667984009\n",
            "For Step: 28760 recon_loss: 0.1801687330007553 \tdiscriminator_loss: 1.2751612663269043 \tgenerator_loss: 0.9212644100189209\n",
            "For Step: 28770 recon_loss: 0.17771416902542114 \tdiscriminator_loss: 1.2779858112335205 \tgenerator_loss: 0.8954870700836182\n",
            "For Step: 28780 recon_loss: 0.16942495107650757 \tdiscriminator_loss: 1.2599531412124634 \tgenerator_loss: 0.871863603591919\n",
            "For Step: 28790 recon_loss: 0.179230198264122 \tdiscriminator_loss: 1.2540063858032227 \tgenerator_loss: 0.8887157440185547\n",
            "For Step: 28800 recon_loss: 0.1718553751707077 \tdiscriminator_loss: 1.2295867204666138 \tgenerator_loss: 0.8565231561660767\n",
            "For Step: 28810 recon_loss: 0.17058691382408142 \tdiscriminator_loss: 1.232874870300293 \tgenerator_loss: 0.9426823854446411\n",
            "For Step: 28820 recon_loss: 0.1726539433002472 \tdiscriminator_loss: 1.2279531955718994 \tgenerator_loss: 0.8770720362663269\n",
            "For Step: 28830 recon_loss: 0.18242205679416656 \tdiscriminator_loss: 1.158387303352356 \tgenerator_loss: 0.9266068339347839\n",
            "For Step: 28840 recon_loss: 0.17815540730953217 \tdiscriminator_loss: 1.2784876823425293 \tgenerator_loss: 0.765656590461731\n",
            "For Step: 28850 recon_loss: 0.16646608710289001 \tdiscriminator_loss: 1.2011662721633911 \tgenerator_loss: 0.8896915912628174\n",
            "For Step: 28860 recon_loss: 0.1730249524116516 \tdiscriminator_loss: 1.2038116455078125 \tgenerator_loss: 0.9551607370376587\n",
            "For Step: 28870 recon_loss: 0.17519798874855042 \tdiscriminator_loss: 1.2909897565841675 \tgenerator_loss: 0.8516526222229004\n",
            "For Step: 28880 recon_loss: 0.16001178324222565 \tdiscriminator_loss: 1.202749252319336 \tgenerator_loss: 0.8673998713493347\n",
            "For Step: 28890 recon_loss: 0.16626976430416107 \tdiscriminator_loss: 1.271450400352478 \tgenerator_loss: 0.8799018859863281\n",
            "For Step: 28900 recon_loss: 0.16630712151527405 \tdiscriminator_loss: 1.3180561065673828 \tgenerator_loss: 0.8503293395042419\n",
            "For Step: 28910 recon_loss: 0.16375315189361572 \tdiscriminator_loss: 1.1657634973526 \tgenerator_loss: 0.8875744342803955\n",
            "For Step: 28920 recon_loss: 0.1834912747144699 \tdiscriminator_loss: 1.2055798768997192 \tgenerator_loss: 0.8805310726165771\n",
            "For Step: 28930 recon_loss: 0.17867283523082733 \tdiscriminator_loss: 1.3029487133026123 \tgenerator_loss: 0.8661186695098877\n",
            "For Step: 28940 recon_loss: 0.16958855092525482 \tdiscriminator_loss: 1.138366460800171 \tgenerator_loss: 0.870570957660675\n",
            "For Step: 28950 recon_loss: 0.17524902522563934 \tdiscriminator_loss: 1.2231788635253906 \tgenerator_loss: 0.9112005233764648\n",
            "For Step: 28960 recon_loss: 0.18588922917842865 \tdiscriminator_loss: 1.2855031490325928 \tgenerator_loss: 0.8858680725097656\n",
            "For Step: 28970 recon_loss: 0.16934441030025482 \tdiscriminator_loss: 1.2779951095581055 \tgenerator_loss: 0.8583639860153198\n",
            "For Step: 28980 recon_loss: 0.1667696237564087 \tdiscriminator_loss: 1.3119211196899414 \tgenerator_loss: 0.8086398839950562\n",
            "For Step: 28990 recon_loss: 0.17488791048526764 \tdiscriminator_loss: 1.2181622982025146 \tgenerator_loss: 0.8853158950805664\n",
            "For Step: 29000 recon_loss: 0.17469757795333862 \tdiscriminator_loss: 1.2645342350006104 \tgenerator_loss: 0.8461514115333557\n",
            "For Step: 29010 recon_loss: 0.1708502322435379 \tdiscriminator_loss: 1.3093065023422241 \tgenerator_loss: 0.9186805486679077\n",
            "For Step: 29020 recon_loss: 0.16628849506378174 \tdiscriminator_loss: 1.1698081493377686 \tgenerator_loss: 0.8727931380271912\n",
            "For Step: 29030 recon_loss: 0.17877402901649475 \tdiscriminator_loss: 1.2758269309997559 \tgenerator_loss: 0.8439755439758301\n",
            "For Step: 29040 recon_loss: 0.17046286165714264 \tdiscriminator_loss: 1.2977772951126099 \tgenerator_loss: 0.9322607517242432\n",
            "For Step: 29050 recon_loss: 0.16033154726028442 \tdiscriminator_loss: 1.2680716514587402 \tgenerator_loss: 0.8445860147476196\n",
            "For Step: 29060 recon_loss: 0.16883935034275055 \tdiscriminator_loss: 1.3038246631622314 \tgenerator_loss: 0.8541794419288635\n",
            "For Step: 29070 recon_loss: 0.17230765521526337 \tdiscriminator_loss: 1.2966426610946655 \tgenerator_loss: 0.9578481316566467\n",
            "For Step: 29080 recon_loss: 0.18473441898822784 \tdiscriminator_loss: 1.1497313976287842 \tgenerator_loss: 0.9369529485702515\n",
            "For Step: 29090 recon_loss: 0.17964528501033783 \tdiscriminator_loss: 1.1603660583496094 \tgenerator_loss: 0.9250819683074951\n",
            "For Step: 29100 recon_loss: 0.17169922590255737 \tdiscriminator_loss: 1.2429486513137817 \tgenerator_loss: 0.9124702215194702\n",
            "For Step: 29110 recon_loss: 0.1697467863559723 \tdiscriminator_loss: 1.2223832607269287 \tgenerator_loss: 0.941573441028595\n",
            "For Step: 29120 recon_loss: 0.17523625493049622 \tdiscriminator_loss: 1.16851806640625 \tgenerator_loss: 0.9183720350265503\n",
            "For Step: 29130 recon_loss: 0.1759236603975296 \tdiscriminator_loss: 1.257663369178772 \tgenerator_loss: 0.8666338920593262\n",
            "For Step: 29140 recon_loss: 0.17358186841011047 \tdiscriminator_loss: 1.2676746845245361 \tgenerator_loss: 0.9330124855041504\n",
            "For Step: 29150 recon_loss: 0.1805391162633896 \tdiscriminator_loss: 1.235703468322754 \tgenerator_loss: 0.881187915802002\n",
            "For Step: 29160 recon_loss: 0.16193236410617828 \tdiscriminator_loss: 1.2583764791488647 \tgenerator_loss: 0.8832782506942749\n",
            "For Step: 29170 recon_loss: 0.16725146770477295 \tdiscriminator_loss: 1.251516342163086 \tgenerator_loss: 0.8483350872993469\n",
            "For Step: 29180 recon_loss: 0.1753978729248047 \tdiscriminator_loss: 1.188734769821167 \tgenerator_loss: 0.8581045269966125\n",
            "For Step: 29190 recon_loss: 0.18250705301761627 \tdiscriminator_loss: 1.2283580303192139 \tgenerator_loss: 0.9088136553764343\n",
            "For Step: 29200 recon_loss: 0.15803255140781403 \tdiscriminator_loss: 1.193056344985962 \tgenerator_loss: 0.8830723762512207\n",
            "For Step: 29210 recon_loss: 0.18241176009178162 \tdiscriminator_loss: 1.191723346710205 \tgenerator_loss: 0.9116209745407104\n",
            "For Step: 29220 recon_loss: 0.17162242531776428 \tdiscriminator_loss: 1.2161507606506348 \tgenerator_loss: 0.898240327835083\n",
            "For Step: 29230 recon_loss: 0.1651913970708847 \tdiscriminator_loss: 1.2375465631484985 \tgenerator_loss: 0.8460062742233276\n",
            "For Step: 29240 recon_loss: 0.1825622320175171 \tdiscriminator_loss: 1.245936632156372 \tgenerator_loss: 0.9055439829826355\n",
            "For Step: 29250 recon_loss: 0.1739618480205536 \tdiscriminator_loss: 1.1741185188293457 \tgenerator_loss: 0.9032659530639648\n",
            "For Step: 29260 recon_loss: 0.1726454198360443 \tdiscriminator_loss: 1.2538114786148071 \tgenerator_loss: 0.8935797810554504\n",
            "For Step: 29270 recon_loss: 0.18071694672107697 \tdiscriminator_loss: 1.2267805337905884 \tgenerator_loss: 0.8678930401802063\n",
            "For Step: 29280 recon_loss: 0.1720781922340393 \tdiscriminator_loss: 1.1681995391845703 \tgenerator_loss: 0.8869159817695618\n",
            "For Step: 29290 recon_loss: 0.1630544513463974 \tdiscriminator_loss: 1.2686853408813477 \tgenerator_loss: 0.9403865337371826\n",
            "For Step: 29300 recon_loss: 0.18216745555400848 \tdiscriminator_loss: 1.245741605758667 \tgenerator_loss: 0.9502612352371216\n",
            "For Step: 29310 recon_loss: 0.17254841327667236 \tdiscriminator_loss: 1.241847038269043 \tgenerator_loss: 0.9421377778053284\n",
            "For Step: 29320 recon_loss: 0.17385686933994293 \tdiscriminator_loss: 1.1216087341308594 \tgenerator_loss: 0.8884932398796082\n",
            "For Step: 29330 recon_loss: 0.18559470772743225 \tdiscriminator_loss: 1.2249747514724731 \tgenerator_loss: 0.8690221309661865\n",
            "For Step: 29340 recon_loss: 0.1635275036096573 \tdiscriminator_loss: 1.3442310094833374 \tgenerator_loss: 0.8890791535377502\n",
            "For Step: 29350 recon_loss: 0.17606213688850403 \tdiscriminator_loss: 1.230839729309082 \tgenerator_loss: 0.9190534949302673\n",
            "For Step: 29360 recon_loss: 0.17522703111171722 \tdiscriminator_loss: 1.2917823791503906 \tgenerator_loss: 0.8971832990646362\n",
            "For Step: 29370 recon_loss: 0.16943036019802094 \tdiscriminator_loss: 1.3459219932556152 \tgenerator_loss: 0.85438472032547\n",
            "For Step: 29380 recon_loss: 0.17520102858543396 \tdiscriminator_loss: 1.2537060976028442 \tgenerator_loss: 0.8894040584564209\n",
            "For Step: 29390 recon_loss: 0.18560907244682312 \tdiscriminator_loss: 1.2311608791351318 \tgenerator_loss: 0.9579654932022095\n",
            "For Step: 29400 recon_loss: 0.1753467619419098 \tdiscriminator_loss: 1.2098236083984375 \tgenerator_loss: 0.9267804026603699\n",
            "For Step: 29410 recon_loss: 0.18398019671440125 \tdiscriminator_loss: 1.327728509902954 \tgenerator_loss: 0.8325756788253784\n",
            "For Step: 29420 recon_loss: 0.1577460765838623 \tdiscriminator_loss: 1.3309705257415771 \tgenerator_loss: 0.881321907043457\n",
            "For Step: 29430 recon_loss: 0.17293556034564972 \tdiscriminator_loss: 1.2714234590530396 \tgenerator_loss: 0.8600609302520752\n",
            "For Step: 29440 recon_loss: 0.18247051537036896 \tdiscriminator_loss: 1.3383691310882568 \tgenerator_loss: 0.8614025115966797\n",
            "For Step: 29450 recon_loss: 0.1729688197374344 \tdiscriminator_loss: 1.2637791633605957 \tgenerator_loss: 0.9082592725753784\n",
            "For Step: 29460 recon_loss: 0.17317797243595123 \tdiscriminator_loss: 1.303023099899292 \tgenerator_loss: 0.8947334885597229\n",
            "For Step: 29470 recon_loss: 0.1691020131111145 \tdiscriminator_loss: 1.2496230602264404 \tgenerator_loss: 0.8555043935775757\n",
            "For Step: 29480 recon_loss: 0.16479869186878204 \tdiscriminator_loss: 1.3037514686584473 \tgenerator_loss: 0.863782525062561\n",
            "For Step: 29490 recon_loss: 0.17261672019958496 \tdiscriminator_loss: 1.2072992324829102 \tgenerator_loss: 0.9071755409240723\n",
            "For Step: 29500 recon_loss: 0.16348755359649658 \tdiscriminator_loss: 1.2046198844909668 \tgenerator_loss: 0.9183032512664795\n",
            "For Step: 29510 recon_loss: 0.17497298121452332 \tdiscriminator_loss: 1.2215176820755005 \tgenerator_loss: 0.8363305330276489\n",
            "For Step: 29520 recon_loss: 0.18320342898368835 \tdiscriminator_loss: 1.2391955852508545 \tgenerator_loss: 0.8580722808837891\n",
            "For Step: 29530 recon_loss: 0.173370361328125 \tdiscriminator_loss: 1.2595971822738647 \tgenerator_loss: 0.8323693871498108\n",
            "For Step: 29540 recon_loss: 0.17192696034908295 \tdiscriminator_loss: 1.3012504577636719 \tgenerator_loss: 0.8022636771202087\n",
            "For Step: 29550 recon_loss: 0.1823517084121704 \tdiscriminator_loss: 1.1857925653457642 \tgenerator_loss: 0.892973780632019\n",
            "For Step: 29560 recon_loss: 0.17099399864673615 \tdiscriminator_loss: 1.2850863933563232 \tgenerator_loss: 0.8933378458023071\n",
            "For Step: 29570 recon_loss: 0.16578952968120575 \tdiscriminator_loss: 1.2495461702346802 \tgenerator_loss: 0.850481390953064\n",
            "For Step: 29580 recon_loss: 0.174186572432518 \tdiscriminator_loss: 1.2177791595458984 \tgenerator_loss: 0.934380292892456\n",
            "For Step: 29590 recon_loss: 0.1723450869321823 \tdiscriminator_loss: 1.0983424186706543 \tgenerator_loss: 0.8928912878036499\n",
            "For Step: 29600 recon_loss: 0.1766246259212494 \tdiscriminator_loss: 1.4067662954330444 \tgenerator_loss: 0.8834823966026306\n",
            "For Step: 29610 recon_loss: 0.16948167979717255 \tdiscriminator_loss: 1.2845851182937622 \tgenerator_loss: 0.8940773606300354\n",
            "For Step: 29620 recon_loss: 0.1657652109861374 \tdiscriminator_loss: 1.1884444952011108 \tgenerator_loss: 0.8561030626296997\n",
            "For Step: 29630 recon_loss: 0.17093613743782043 \tdiscriminator_loss: 1.2037315368652344 \tgenerator_loss: 0.898529052734375\n",
            "For Step: 29640 recon_loss: 0.17527209222316742 \tdiscriminator_loss: 1.3614354133605957 \tgenerator_loss: 0.8514913320541382\n",
            "For Step: 29650 recon_loss: 0.1703740358352661 \tdiscriminator_loss: 1.236945390701294 \tgenerator_loss: 0.8974215984344482\n",
            "For Step: 29660 recon_loss: 0.16791051626205444 \tdiscriminator_loss: 1.2561891078948975 \tgenerator_loss: 0.8716528415679932\n",
            "For Step: 29670 recon_loss: 0.16837859153747559 \tdiscriminator_loss: 1.123558521270752 \tgenerator_loss: 0.8903037309646606\n",
            "For Step: 29680 recon_loss: 0.16741935908794403 \tdiscriminator_loss: 1.3026841878890991 \tgenerator_loss: 0.8324109315872192\n",
            "For Step: 29690 recon_loss: 0.18087200820446014 \tdiscriminator_loss: 1.231548547744751 \tgenerator_loss: 0.8664333820343018\n",
            "For Step: 29700 recon_loss: 0.17827023565769196 \tdiscriminator_loss: 1.2040770053863525 \tgenerator_loss: 0.8850955963134766\n",
            "For Step: 29710 recon_loss: 0.1817174255847931 \tdiscriminator_loss: 1.1983153820037842 \tgenerator_loss: 0.9460687041282654\n",
            "For Step: 29720 recon_loss: 0.16416218876838684 \tdiscriminator_loss: 1.0983836650848389 \tgenerator_loss: 0.8957350254058838\n",
            "For Step: 29730 recon_loss: 0.17149780690670013 \tdiscriminator_loss: 1.3004209995269775 \tgenerator_loss: 0.8785399198532104\n",
            "For Step: 29740 recon_loss: 0.16533039510250092 \tdiscriminator_loss: 1.3110815286636353 \tgenerator_loss: 0.8269959688186646\n",
            "For Step: 29750 recon_loss: 0.16599686443805695 \tdiscriminator_loss: 1.3403910398483276 \tgenerator_loss: 0.8188502192497253\n",
            "For Step: 29760 recon_loss: 0.16803990304470062 \tdiscriminator_loss: 1.155869722366333 \tgenerator_loss: 0.9359650611877441\n",
            "For Step: 29770 recon_loss: 0.17693473398685455 \tdiscriminator_loss: 1.2822351455688477 \tgenerator_loss: 0.8603569269180298\n",
            "For Step: 29780 recon_loss: 0.16826055943965912 \tdiscriminator_loss: 1.2194346189498901 \tgenerator_loss: 0.9042852520942688\n",
            "For Step: 29790 recon_loss: 0.19504274427890778 \tdiscriminator_loss: 1.2658960819244385 \tgenerator_loss: 0.879106879234314\n",
            "For Step: 29800 recon_loss: 0.17905719578266144 \tdiscriminator_loss: 1.1797852516174316 \tgenerator_loss: 0.9163618683815002\n",
            "For Step: 29810 recon_loss: 0.1698535978794098 \tdiscriminator_loss: 1.345951795578003 \tgenerator_loss: 0.852446973323822\n",
            "For Step: 29820 recon_loss: 0.16839264333248138 \tdiscriminator_loss: 1.2807692289352417 \tgenerator_loss: 0.8281913995742798\n",
            "For Step: 29830 recon_loss: 0.1740577667951584 \tdiscriminator_loss: 1.2038174867630005 \tgenerator_loss: 0.8930474519729614\n",
            "For Step: 29840 recon_loss: 0.1657688021659851 \tdiscriminator_loss: 1.1765516996383667 \tgenerator_loss: 0.9225527048110962\n",
            "For Step: 29850 recon_loss: 0.1746802181005478 \tdiscriminator_loss: 1.2554457187652588 \tgenerator_loss: 0.8886452913284302\n",
            "For Step: 29860 recon_loss: 0.17124304175376892 \tdiscriminator_loss: 1.3155009746551514 \tgenerator_loss: 0.876243531703949\n",
            "For Step: 29870 recon_loss: 0.17894992232322693 \tdiscriminator_loss: 1.267040729522705 \tgenerator_loss: 0.8930166959762573\n",
            "For Step: 29880 recon_loss: 0.17361348867416382 \tdiscriminator_loss: 1.301895260810852 \tgenerator_loss: 0.8648169040679932\n",
            "For Step: 29890 recon_loss: 0.1680605411529541 \tdiscriminator_loss: 1.2883068323135376 \tgenerator_loss: 0.8750126361846924\n",
            "For Step: 29900 recon_loss: 0.15761227905750275 \tdiscriminator_loss: 1.1722171306610107 \tgenerator_loss: 0.8894554376602173\n",
            "For Step: 29910 recon_loss: 0.1653272658586502 \tdiscriminator_loss: 1.2533272504806519 \tgenerator_loss: 0.9722505807876587\n",
            "For Step: 29920 recon_loss: 0.18405699729919434 \tdiscriminator_loss: 1.2073636054992676 \tgenerator_loss: 0.8850269913673401\n",
            "For Step: 29930 recon_loss: 0.16384784877300262 \tdiscriminator_loss: 1.2517063617706299 \tgenerator_loss: 0.9036137461662292\n",
            "For Step: 29940 recon_loss: 0.17749658226966858 \tdiscriminator_loss: 1.2391960620880127 \tgenerator_loss: 0.8926748037338257\n",
            "For Step: 29950 recon_loss: 0.1845315396785736 \tdiscriminator_loss: 1.2089234590530396 \tgenerator_loss: 0.8747760057449341\n",
            "For Step: 29960 recon_loss: 0.1686105877161026 \tdiscriminator_loss: 1.2152743339538574 \tgenerator_loss: 0.8506618738174438\n",
            "For Step: 29970 recon_loss: 0.17672963440418243 \tdiscriminator_loss: 1.1893070936203003 \tgenerator_loss: 0.9089128971099854\n",
            "For Step: 29980 recon_loss: 0.1822439283132553 \tdiscriminator_loss: 1.2989764213562012 \tgenerator_loss: 0.8752202391624451\n",
            "For Step: 29990 recon_loss: 0.17168323695659637 \tdiscriminator_loss: 1.3176765441894531 \tgenerator_loss: 0.8397551774978638\n",
            "For Step: 30000 recon_loss: 0.16592863202095032 \tdiscriminator_loss: 1.1753073930740356 \tgenerator_loss: 0.8320441842079163\n",
            "For Step: 30010 recon_loss: 0.17966440320014954 \tdiscriminator_loss: 1.335348129272461 \tgenerator_loss: 0.8218660950660706\n",
            "For Step: 30020 recon_loss: 0.16719087958335876 \tdiscriminator_loss: 1.2301899194717407 \tgenerator_loss: 0.905803918838501\n",
            "For Step: 30030 recon_loss: 0.18100441992282867 \tdiscriminator_loss: 1.3113423585891724 \tgenerator_loss: 0.8733839988708496\n",
            "For Step: 30040 recon_loss: 0.18395841121673584 \tdiscriminator_loss: 1.1914775371551514 \tgenerator_loss: 0.8775416016578674\n",
            "For Step: 30050 recon_loss: 0.17463435232639313 \tdiscriminator_loss: 1.289243221282959 \tgenerator_loss: 0.9259655475616455\n",
            "For Step: 30060 recon_loss: 0.18330878019332886 \tdiscriminator_loss: 1.2788808345794678 \tgenerator_loss: 0.8832321763038635\n",
            "For Step: 30070 recon_loss: 0.17102953791618347 \tdiscriminator_loss: 1.1981981992721558 \tgenerator_loss: 0.9274287223815918\n",
            "For Step: 30080 recon_loss: 0.18516024947166443 \tdiscriminator_loss: 1.2759947776794434 \tgenerator_loss: 0.8694767355918884\n",
            "For Step: 30090 recon_loss: 0.17377769947052002 \tdiscriminator_loss: 1.1388463973999023 \tgenerator_loss: 0.927397608757019\n",
            "For Step: 30100 recon_loss: 0.17357507348060608 \tdiscriminator_loss: 1.2133092880249023 \tgenerator_loss: 0.9178368449211121\n",
            "For Step: 30110 recon_loss: 0.17657235264778137 \tdiscriminator_loss: 1.1166188716888428 \tgenerator_loss: 0.934626042842865\n",
            "For Step: 30120 recon_loss: 0.17696867883205414 \tdiscriminator_loss: 1.1471983194351196 \tgenerator_loss: 0.8968439102172852\n",
            "For Step: 30130 recon_loss: 0.18074393272399902 \tdiscriminator_loss: 1.2435046434402466 \tgenerator_loss: 0.8817351460456848\n",
            "For Step: 30140 recon_loss: 0.1760060042142868 \tdiscriminator_loss: 1.1497448682785034 \tgenerator_loss: 0.9185889959335327\n",
            "For Step: 30150 recon_loss: 0.17515629529953003 \tdiscriminator_loss: 1.3152388334274292 \tgenerator_loss: 0.9013334512710571\n",
            "For Step: 30160 recon_loss: 0.17611613869667053 \tdiscriminator_loss: 1.1417889595031738 \tgenerator_loss: 0.9212031960487366\n",
            "For Step: 30170 recon_loss: 0.16775988042354584 \tdiscriminator_loss: 1.375890851020813 \tgenerator_loss: 0.8227105140686035\n",
            "For Step: 30180 recon_loss: 0.18323497474193573 \tdiscriminator_loss: 1.1758047342300415 \tgenerator_loss: 0.8964903950691223\n",
            "For Step: 30190 recon_loss: 0.17290066182613373 \tdiscriminator_loss: 1.2356122732162476 \tgenerator_loss: 0.9068462252616882\n",
            "For Step: 30200 recon_loss: 0.18785718083381653 \tdiscriminator_loss: 1.2385375499725342 \tgenerator_loss: 0.9225672483444214\n",
            "For Step: 30210 recon_loss: 0.18206095695495605 \tdiscriminator_loss: 1.3571927547454834 \tgenerator_loss: 0.836412250995636\n",
            "For Step: 30220 recon_loss: 0.18390415608882904 \tdiscriminator_loss: 1.2601840496063232 \tgenerator_loss: 0.9462355375289917\n",
            "For Step: 30230 recon_loss: 0.1733608990907669 \tdiscriminator_loss: 1.2756974697113037 \tgenerator_loss: 0.9473645687103271\n",
            "For Step: 30240 recon_loss: 0.17856091260910034 \tdiscriminator_loss: 1.2267262935638428 \tgenerator_loss: 0.9108744859695435\n",
            "For Step: 30250 recon_loss: 0.17467999458312988 \tdiscriminator_loss: 1.28761887550354 \tgenerator_loss: 0.8413739800453186\n",
            "For Step: 30260 recon_loss: 0.17736992239952087 \tdiscriminator_loss: 1.2231237888336182 \tgenerator_loss: 0.832859456539154\n",
            "For Step: 30270 recon_loss: 0.1714400053024292 \tdiscriminator_loss: 1.1388943195343018 \tgenerator_loss: 0.8892531991004944\n",
            "For Step: 30280 recon_loss: 0.17372584342956543 \tdiscriminator_loss: 1.189322590827942 \tgenerator_loss: 0.9208820462226868\n",
            "For Step: 30290 recon_loss: 0.1620592623949051 \tdiscriminator_loss: 1.328761339187622 \tgenerator_loss: 0.8092494010925293\n",
            "For Step: 30300 recon_loss: 0.17201203107833862 \tdiscriminator_loss: 1.2722690105438232 \tgenerator_loss: 0.8338046073913574\n",
            "For Step: 30310 recon_loss: 0.17351539433002472 \tdiscriminator_loss: 1.2624328136444092 \tgenerator_loss: 0.8727982640266418\n",
            "For Step: 30320 recon_loss: 0.17892251908779144 \tdiscriminator_loss: 1.2397099733352661 \tgenerator_loss: 0.8817748427391052\n",
            "For Step: 30330 recon_loss: 0.177336648106575 \tdiscriminator_loss: 1.2303308248519897 \tgenerator_loss: 0.896613359451294\n",
            "For Step: 30340 recon_loss: 0.18295103311538696 \tdiscriminator_loss: 1.207350254058838 \tgenerator_loss: 0.908424437046051\n",
            "For Step: 30350 recon_loss: 0.18165917694568634 \tdiscriminator_loss: 1.2084028720855713 \tgenerator_loss: 0.8456555604934692\n",
            "For Step: 30360 recon_loss: 0.15992115437984467 \tdiscriminator_loss: 1.2333800792694092 \tgenerator_loss: 0.8899129629135132\n",
            "For Step: 30370 recon_loss: 0.1528593748807907 \tdiscriminator_loss: 1.208609938621521 \tgenerator_loss: 0.8983871340751648\n",
            "For Step: 30380 recon_loss: 0.17703497409820557 \tdiscriminator_loss: 1.185680627822876 \tgenerator_loss: 0.8943685293197632\n",
            "For Step: 30390 recon_loss: 0.17454448342323303 \tdiscriminator_loss: 1.2354166507720947 \tgenerator_loss: 0.910020112991333\n",
            "For Step: 30400 recon_loss: 0.16720961034297943 \tdiscriminator_loss: 1.295355200767517 \tgenerator_loss: 0.8873657584190369\n",
            "For Step: 30410 recon_loss: 0.17875951528549194 \tdiscriminator_loss: 1.2312657833099365 \tgenerator_loss: 0.8957090377807617\n",
            "For Step: 30420 recon_loss: 0.16906386613845825 \tdiscriminator_loss: 1.2520581483840942 \tgenerator_loss: 0.9243767261505127\n",
            "For Step: 30430 recon_loss: 0.18868015706539154 \tdiscriminator_loss: 1.2576366662979126 \tgenerator_loss: 0.920951783657074\n",
            "For Step: 30440 recon_loss: 0.1769443303346634 \tdiscriminator_loss: 1.238128900527954 \tgenerator_loss: 0.9007589817047119\n",
            "For Step: 30450 recon_loss: 0.1677190363407135 \tdiscriminator_loss: 1.353392243385315 \tgenerator_loss: 0.8826357126235962\n",
            "For Step: 30460 recon_loss: 0.17564353346824646 \tdiscriminator_loss: 1.2607104778289795 \tgenerator_loss: 0.8828457593917847\n",
            "For Step: 30470 recon_loss: 0.18160901963710785 \tdiscriminator_loss: 1.2402302026748657 \tgenerator_loss: 0.8939751982688904\n",
            "For Step: 30480 recon_loss: 0.16562175750732422 \tdiscriminator_loss: 1.261717677116394 \tgenerator_loss: 0.87844318151474\n",
            "For Step: 30490 recon_loss: 0.1772093027830124 \tdiscriminator_loss: 1.190593957901001 \tgenerator_loss: 0.9121966361999512\n",
            "For Step: 30500 recon_loss: 0.1743026077747345 \tdiscriminator_loss: 1.2139129638671875 \tgenerator_loss: 0.9276215434074402\n",
            "For Step: 30510 recon_loss: 0.16663435101509094 \tdiscriminator_loss: 1.309617042541504 \tgenerator_loss: 0.8574522733688354\n",
            "For Step: 30520 recon_loss: 0.18532738089561462 \tdiscriminator_loss: 1.1882741451263428 \tgenerator_loss: 0.8553215265274048\n",
            "For Step: 30530 recon_loss: 0.1752687245607376 \tdiscriminator_loss: 1.3130013942718506 \tgenerator_loss: 0.8154526352882385\n",
            "For Step: 30540 recon_loss: 0.17453892529010773 \tdiscriminator_loss: 1.2563071250915527 \tgenerator_loss: 0.8739557862281799\n",
            "For Step: 30550 recon_loss: 0.17388415336608887 \tdiscriminator_loss: 1.281449556350708 \tgenerator_loss: 0.8682070970535278\n",
            "For Step: 30560 recon_loss: 0.17684553563594818 \tdiscriminator_loss: 1.2059326171875 \tgenerator_loss: 0.9522606134414673\n",
            "For Step: 30570 recon_loss: 0.17260771989822388 \tdiscriminator_loss: 1.257979393005371 \tgenerator_loss: 0.8389779329299927\n",
            "For Step: 30580 recon_loss: 0.1852738857269287 \tdiscriminator_loss: 1.2065761089324951 \tgenerator_loss: 0.8909941911697388\n",
            "For Step: 30590 recon_loss: 0.18409688770771027 \tdiscriminator_loss: 1.2948496341705322 \tgenerator_loss: 0.859266996383667\n",
            "For Step: 30600 recon_loss: 0.18310800194740295 \tdiscriminator_loss: 1.3659915924072266 \tgenerator_loss: 0.867787778377533\n",
            "For Step: 30610 recon_loss: 0.16880135238170624 \tdiscriminator_loss: 1.1586614847183228 \tgenerator_loss: 0.8527652621269226\n",
            "For Step: 30620 recon_loss: 0.17758053541183472 \tdiscriminator_loss: 1.2185254096984863 \tgenerator_loss: 0.8673070669174194\n",
            "For Step: 30630 recon_loss: 0.1752256602048874 \tdiscriminator_loss: 1.353132963180542 \tgenerator_loss: 0.8394509553909302\n",
            "For Step: 30640 recon_loss: 0.16264155507087708 \tdiscriminator_loss: 1.2225582599639893 \tgenerator_loss: 0.9163769483566284\n",
            "For Step: 30650 recon_loss: 0.169752836227417 \tdiscriminator_loss: 1.3241075277328491 \tgenerator_loss: 0.8607147932052612\n",
            "For Step: 30660 recon_loss: 0.17620888352394104 \tdiscriminator_loss: 1.2181869745254517 \tgenerator_loss: 0.8680261373519897\n",
            "For Step: 30670 recon_loss: 0.17664633691310883 \tdiscriminator_loss: 1.3141469955444336 \tgenerator_loss: 0.8451471328735352\n",
            "For Step: 30680 recon_loss: 0.1721179336309433 \tdiscriminator_loss: 1.2105315923690796 \tgenerator_loss: 0.9146925210952759\n",
            "For Step: 30690 recon_loss: 0.18037845194339752 \tdiscriminator_loss: 1.1723780632019043 \tgenerator_loss: 0.977649450302124\n",
            "For Step: 30700 recon_loss: 0.1759672313928604 \tdiscriminator_loss: 1.3167309761047363 \tgenerator_loss: 0.9211595058441162\n",
            "For Step: 30710 recon_loss: 0.17355281114578247 \tdiscriminator_loss: 1.2114641666412354 \tgenerator_loss: 0.9037153720855713\n",
            "For Step: 30720 recon_loss: 0.16990195214748383 \tdiscriminator_loss: 1.1824753284454346 \tgenerator_loss: 0.9160714745521545\n",
            "For Step: 30730 recon_loss: 0.17145021259784698 \tdiscriminator_loss: 1.2047321796417236 \tgenerator_loss: 0.8988527655601501\n",
            "For Step: 30740 recon_loss: 0.16166716814041138 \tdiscriminator_loss: 1.3603267669677734 \tgenerator_loss: 0.8868989944458008\n",
            "For Step: 30750 recon_loss: 0.17628896236419678 \tdiscriminator_loss: 1.2831649780273438 \tgenerator_loss: 0.9069305062294006\n",
            "For Step: 30760 recon_loss: 0.1801941990852356 \tdiscriminator_loss: 1.1489146947860718 \tgenerator_loss: 0.8956772089004517\n",
            "For Step: 30770 recon_loss: 0.15481457114219666 \tdiscriminator_loss: 1.182050108909607 \tgenerator_loss: 0.9010012745857239\n",
            "For Step: 30780 recon_loss: 0.17535144090652466 \tdiscriminator_loss: 1.181644082069397 \tgenerator_loss: 0.8970427513122559\n",
            "For Step: 30790 recon_loss: 0.17473042011260986 \tdiscriminator_loss: 1.2610015869140625 \tgenerator_loss: 0.9363346695899963\n",
            "For Step: 30800 recon_loss: 0.17108836770057678 \tdiscriminator_loss: 1.1477398872375488 \tgenerator_loss: 0.8908231258392334\n",
            "For Step: 30810 recon_loss: 0.17222627997398376 \tdiscriminator_loss: 1.2900869846343994 \tgenerator_loss: 0.8689227104187012\n",
            "For Step: 30820 recon_loss: 0.1719450056552887 \tdiscriminator_loss: 1.3582175970077515 \tgenerator_loss: 0.8768389821052551\n",
            "For Step: 30830 recon_loss: 0.17643818259239197 \tdiscriminator_loss: 1.228623390197754 \tgenerator_loss: 0.8208065032958984\n",
            "For Step: 30840 recon_loss: 0.17617008090019226 \tdiscriminator_loss: 1.1336992979049683 \tgenerator_loss: 0.9298334121704102\n",
            "For Step: 30850 recon_loss: 0.18737034499645233 \tdiscriminator_loss: 1.177954912185669 \tgenerator_loss: 0.8629465103149414\n",
            "For Step: 30860 recon_loss: 0.1863495111465454 \tdiscriminator_loss: 1.2140142917633057 \tgenerator_loss: 0.9319365620613098\n",
            "For Step: 30870 recon_loss: 0.16508057713508606 \tdiscriminator_loss: 1.312671422958374 \tgenerator_loss: 0.8279381394386292\n",
            "For Step: 30880 recon_loss: 0.18551890552043915 \tdiscriminator_loss: 1.2808887958526611 \tgenerator_loss: 0.8949664831161499\n",
            "For Step: 30890 recon_loss: 0.17230521142482758 \tdiscriminator_loss: 1.3066238164901733 \tgenerator_loss: 0.8744404315948486\n",
            "For Step: 30900 recon_loss: 0.17925433814525604 \tdiscriminator_loss: 1.174155831336975 \tgenerator_loss: 0.8561622500419617\n",
            "For Step: 30910 recon_loss: 0.17229396104812622 \tdiscriminator_loss: 1.3324202299118042 \tgenerator_loss: 0.8561071157455444\n",
            "For Step: 30920 recon_loss: 0.18285775184631348 \tdiscriminator_loss: 1.2836930751800537 \tgenerator_loss: 0.8522283434867859\n",
            "For Step: 30930 recon_loss: 0.16528376936912537 \tdiscriminator_loss: 1.2262927293777466 \tgenerator_loss: 0.8850417137145996\n",
            "For Step: 30940 recon_loss: 0.17017103731632233 \tdiscriminator_loss: 1.2556281089782715 \tgenerator_loss: 0.862249493598938\n",
            "For Step: 30950 recon_loss: 0.15828636288642883 \tdiscriminator_loss: 1.2581145763397217 \tgenerator_loss: 0.8259992599487305\n",
            "For Step: 30960 recon_loss: 0.16823890805244446 \tdiscriminator_loss: 1.2824208736419678 \tgenerator_loss: 0.8902397751808167\n",
            "For Step: 30970 recon_loss: 0.17212560772895813 \tdiscriminator_loss: 1.1853249073028564 \tgenerator_loss: 0.8196382522583008\n",
            "For Step: 30980 recon_loss: 0.1722206026315689 \tdiscriminator_loss: 1.293257474899292 \tgenerator_loss: 0.8685084581375122\n",
            "For Step: 30990 recon_loss: 0.16634762287139893 \tdiscriminator_loss: 1.2559826374053955 \tgenerator_loss: 0.8781256675720215\n",
            "For Step: 31000 recon_loss: 0.17024947702884674 \tdiscriminator_loss: 1.217379093170166 \tgenerator_loss: 0.8933429718017578\n",
            "For Step: 31010 recon_loss: 0.17251241207122803 \tdiscriminator_loss: 1.291333794593811 \tgenerator_loss: 0.8267050981521606\n",
            "For Step: 31020 recon_loss: 0.17794111371040344 \tdiscriminator_loss: 1.3055250644683838 \tgenerator_loss: 0.8787775635719299\n",
            "For Step: 31030 recon_loss: 0.18540418148040771 \tdiscriminator_loss: 1.3073586225509644 \tgenerator_loss: 0.9687075614929199\n",
            "For Step: 31040 recon_loss: 0.15888293087482452 \tdiscriminator_loss: 1.2330613136291504 \tgenerator_loss: 0.8749197721481323\n",
            "For Step: 31050 recon_loss: 0.18330644071102142 \tdiscriminator_loss: 1.3069254159927368 \tgenerator_loss: 0.8882638216018677\n",
            "For Step: 31060 recon_loss: 0.18195496499538422 \tdiscriminator_loss: 1.358851432800293 \tgenerator_loss: 0.8290371298789978\n",
            "For Step: 31070 recon_loss: 0.1786854863166809 \tdiscriminator_loss: 1.288884162902832 \tgenerator_loss: 0.8650544881820679\n",
            "For Step: 31080 recon_loss: 0.17688530683517456 \tdiscriminator_loss: 1.164724588394165 \tgenerator_loss: 0.9479249119758606\n",
            "For Step: 31090 recon_loss: 0.1742665022611618 \tdiscriminator_loss: 1.4022811651229858 \tgenerator_loss: 0.8366344571113586\n",
            "For Step: 31100 recon_loss: 0.18475104868412018 \tdiscriminator_loss: 1.404106616973877 \tgenerator_loss: 0.8583099246025085\n",
            "For Step: 31110 recon_loss: 0.18430425226688385 \tdiscriminator_loss: 1.3165063858032227 \tgenerator_loss: 0.8384349346160889\n",
            "For Step: 31120 recon_loss: 0.17289996147155762 \tdiscriminator_loss: 1.328341007232666 \tgenerator_loss: 0.8368678689002991\n",
            "For Step: 31130 recon_loss: 0.1697523295879364 \tdiscriminator_loss: 1.2200543880462646 \tgenerator_loss: 0.9115787744522095\n",
            "For Step: 31140 recon_loss: 0.17171789705753326 \tdiscriminator_loss: 1.197920322418213 \tgenerator_loss: 0.9572749137878418\n",
            "For Step: 31150 recon_loss: 0.18000297248363495 \tdiscriminator_loss: 1.2142201662063599 \tgenerator_loss: 0.8972166776657104\n",
            "For Step: 31160 recon_loss: 0.17400862276554108 \tdiscriminator_loss: 1.3170024156570435 \tgenerator_loss: 0.8828239440917969\n",
            "For Step: 31170 recon_loss: 0.17837758362293243 \tdiscriminator_loss: 1.1791164875030518 \tgenerator_loss: 0.8645309209823608\n",
            "For Step: 31180 recon_loss: 0.16885599493980408 \tdiscriminator_loss: 1.439691185951233 \tgenerator_loss: 0.8590084314346313\n",
            "For Step: 31190 recon_loss: 0.168755903840065 \tdiscriminator_loss: 1.2925442457199097 \tgenerator_loss: 0.8244110345840454\n",
            "For Step: 31200 recon_loss: 0.1727476716041565 \tdiscriminator_loss: 1.1783344745635986 \tgenerator_loss: 0.8789042830467224\n",
            "For Step: 31210 recon_loss: 0.1790158748626709 \tdiscriminator_loss: 1.3385899066925049 \tgenerator_loss: 0.7505266666412354\n",
            "For Step: 31220 recon_loss: 0.17060381174087524 \tdiscriminator_loss: 1.2847771644592285 \tgenerator_loss: 0.7874080538749695\n",
            "For Step: 31230 recon_loss: 0.17104071378707886 \tdiscriminator_loss: 1.267791986465454 \tgenerator_loss: 0.8259800672531128\n",
            "For Step: 31240 recon_loss: 0.16618207097053528 \tdiscriminator_loss: 1.381289005279541 \tgenerator_loss: 0.8345547914505005\n",
            "For Step: 31250 recon_loss: 0.17059040069580078 \tdiscriminator_loss: 1.1834475994110107 \tgenerator_loss: 0.9600617289543152\n",
            "For Step: 31260 recon_loss: 0.17968468368053436 \tdiscriminator_loss: 1.1870810985565186 \tgenerator_loss: 0.9229103326797485\n",
            "For Step: 31270 recon_loss: 0.16932228207588196 \tdiscriminator_loss: 1.2393759489059448 \tgenerator_loss: 0.9180842638015747\n",
            "For Step: 31280 recon_loss: 0.18820840120315552 \tdiscriminator_loss: 1.1910736560821533 \tgenerator_loss: 0.8797273635864258\n",
            "For Step: 31290 recon_loss: 0.17063187062740326 \tdiscriminator_loss: 1.3019649982452393 \tgenerator_loss: 0.8820939660072327\n",
            "For Step: 31300 recon_loss: 0.16421473026275635 \tdiscriminator_loss: 1.2503584623336792 \tgenerator_loss: 0.8908372521400452\n",
            "For Step: 31310 recon_loss: 0.16752365231513977 \tdiscriminator_loss: 1.2212644815444946 \tgenerator_loss: 0.866926372051239\n",
            "For Step: 31320 recon_loss: 0.1775231659412384 \tdiscriminator_loss: 1.1774325370788574 \tgenerator_loss: 0.8833413124084473\n",
            "For Step: 31330 recon_loss: 0.1773144006729126 \tdiscriminator_loss: 1.2642382383346558 \tgenerator_loss: 0.865746021270752\n",
            "For Step: 31340 recon_loss: 0.17803530395030975 \tdiscriminator_loss: 1.3245970010757446 \tgenerator_loss: 0.8409781455993652\n",
            "For Step: 31350 recon_loss: 0.16010572016239166 \tdiscriminator_loss: 1.3100961446762085 \tgenerator_loss: 0.8463050127029419\n",
            "For Step: 31360 recon_loss: 0.17769667506217957 \tdiscriminator_loss: 1.2763164043426514 \tgenerator_loss: 0.8717095851898193\n",
            "For Step: 31370 recon_loss: 0.17248253524303436 \tdiscriminator_loss: 1.23699152469635 \tgenerator_loss: 0.8882939219474792\n",
            "For Step: 31380 recon_loss: 0.17836064100265503 \tdiscriminator_loss: 1.2239296436309814 \tgenerator_loss: 0.907353937625885\n",
            "For Step: 31390 recon_loss: 0.16324500739574432 \tdiscriminator_loss: 1.279470443725586 \tgenerator_loss: 0.8225172162055969\n",
            "For Step: 31400 recon_loss: 0.1705384999513626 \tdiscriminator_loss: 1.2226998805999756 \tgenerator_loss: 0.8548617362976074\n",
            "For Step: 31410 recon_loss: 0.18808770179748535 \tdiscriminator_loss: 1.2103773355484009 \tgenerator_loss: 0.8430896401405334\n",
            "For Step: 31420 recon_loss: 0.18629440665245056 \tdiscriminator_loss: 1.2592899799346924 \tgenerator_loss: 0.8181816339492798\n",
            "For Step: 31430 recon_loss: 0.16315032541751862 \tdiscriminator_loss: 1.2886998653411865 \tgenerator_loss: 0.8672176599502563\n",
            "For Step: 31440 recon_loss: 0.1692487746477127 \tdiscriminator_loss: 1.257453441619873 \tgenerator_loss: 0.94013512134552\n",
            "For Step: 31450 recon_loss: 0.164619579911232 \tdiscriminator_loss: 1.2080466747283936 \tgenerator_loss: 0.8277388215065002\n",
            "For Step: 31460 recon_loss: 0.1756027340888977 \tdiscriminator_loss: 1.2930035591125488 \tgenerator_loss: 0.8487366437911987\n",
            "For Step: 31470 recon_loss: 0.16959598660469055 \tdiscriminator_loss: 1.204891562461853 \tgenerator_loss: 0.8763911724090576\n",
            "For Step: 31480 recon_loss: 0.17246006429195404 \tdiscriminator_loss: 1.3105798959732056 \tgenerator_loss: 0.8554749488830566\n",
            "For Step: 31490 recon_loss: 0.16901378333568573 \tdiscriminator_loss: 1.2006763219833374 \tgenerator_loss: 0.893268346786499\n",
            "For Step: 31500 recon_loss: 0.1797962337732315 \tdiscriminator_loss: 1.2602298259735107 \tgenerator_loss: 0.8543272018432617\n",
            "For Step: 31510 recon_loss: 0.17513321340084076 \tdiscriminator_loss: 1.1348689794540405 \tgenerator_loss: 0.9200053811073303\n",
            "For Step: 31520 recon_loss: 0.17089542746543884 \tdiscriminator_loss: 1.3693530559539795 \tgenerator_loss: 0.8435815572738647\n",
            "For Step: 31530 recon_loss: 0.17263144254684448 \tdiscriminator_loss: 1.2024757862091064 \tgenerator_loss: 0.8770141005516052\n",
            "For Step: 31540 recon_loss: 0.179500013589859 \tdiscriminator_loss: 1.1948349475860596 \tgenerator_loss: 0.9501938819885254\n",
            "For Step: 31550 recon_loss: 0.16434524953365326 \tdiscriminator_loss: 1.3035036325454712 \tgenerator_loss: 0.8551560640335083\n",
            "For Step: 31560 recon_loss: 0.16862063109874725 \tdiscriminator_loss: 1.2159547805786133 \tgenerator_loss: 0.9183986186981201\n",
            "For Step: 31570 recon_loss: 0.1723729521036148 \tdiscriminator_loss: 1.4000914096832275 \tgenerator_loss: 0.7732710838317871\n",
            "For Step: 31580 recon_loss: 0.17038461565971375 \tdiscriminator_loss: 1.2596274614334106 \tgenerator_loss: 0.7878515124320984\n",
            "For Step: 31590 recon_loss: 0.18394698202610016 \tdiscriminator_loss: 1.3518922328948975 \tgenerator_loss: 0.8873921036720276\n",
            "For Step: 31600 recon_loss: 0.16931267082691193 \tdiscriminator_loss: 1.3087049722671509 \tgenerator_loss: 0.889964759349823\n",
            "For Step: 31610 recon_loss: 0.17316082119941711 \tdiscriminator_loss: 1.214590311050415 \tgenerator_loss: 0.8905379176139832\n",
            "For Step: 31620 recon_loss: 0.1562369018793106 \tdiscriminator_loss: 1.2482047080993652 \tgenerator_loss: 0.8916624784469604\n",
            "For Step: 31630 recon_loss: 0.171282559633255 \tdiscriminator_loss: 1.2665174007415771 \tgenerator_loss: 0.8838630318641663\n",
            "For Step: 31640 recon_loss: 0.17188222706317902 \tdiscriminator_loss: 1.1897238492965698 \tgenerator_loss: 0.8344316482543945\n",
            "For Step: 31650 recon_loss: 0.16586636006832123 \tdiscriminator_loss: 1.2164180278778076 \tgenerator_loss: 0.8710223436355591\n",
            "For Step: 31660 recon_loss: 0.17432595789432526 \tdiscriminator_loss: 1.3279192447662354 \tgenerator_loss: 0.8387173414230347\n",
            "For Step: 31670 recon_loss: 0.18585127592086792 \tdiscriminator_loss: 1.1978967189788818 \tgenerator_loss: 0.8851466178894043\n",
            "For Step: 31680 recon_loss: 0.18935292959213257 \tdiscriminator_loss: 1.1947916746139526 \tgenerator_loss: 0.9627559185028076\n",
            "For Step: 31690 recon_loss: 0.17825204133987427 \tdiscriminator_loss: 1.3128604888916016 \tgenerator_loss: 0.8621534705162048\n",
            "For Step: 31700 recon_loss: 0.18678079545497894 \tdiscriminator_loss: 1.3728524446487427 \tgenerator_loss: 0.8494822978973389\n",
            "For Step: 31710 recon_loss: 0.1734909862279892 \tdiscriminator_loss: 1.2586078643798828 \tgenerator_loss: 0.8545304536819458\n",
            "For Step: 31720 recon_loss: 0.17161786556243896 \tdiscriminator_loss: 1.2512767314910889 \tgenerator_loss: 0.869660496711731\n",
            "For Step: 31730 recon_loss: 0.1782788187265396 \tdiscriminator_loss: 1.2234172821044922 \tgenerator_loss: 0.8802766799926758\n",
            "For Step: 31740 recon_loss: 0.17805524170398712 \tdiscriminator_loss: 1.2821476459503174 \tgenerator_loss: 0.8524237871170044\n",
            "For Step: 31750 recon_loss: 0.17243050038814545 \tdiscriminator_loss: 1.3379539251327515 \tgenerator_loss: 0.8337096571922302\n",
            "For Step: 31760 recon_loss: 0.16234765946865082 \tdiscriminator_loss: 1.3193683624267578 \tgenerator_loss: 0.8596321940422058\n",
            "For Step: 31770 recon_loss: 0.1731138974428177 \tdiscriminator_loss: 1.2452853918075562 \tgenerator_loss: 0.8871120810508728\n",
            "For Step: 31780 recon_loss: 0.17748431861400604 \tdiscriminator_loss: 1.2130194902420044 \tgenerator_loss: 0.8880864977836609\n",
            "For Step: 31790 recon_loss: 0.1720081865787506 \tdiscriminator_loss: 1.3404157161712646 \tgenerator_loss: 0.8360711336135864\n",
            "For Step: 31800 recon_loss: 0.17338697612285614 \tdiscriminator_loss: 1.2564759254455566 \tgenerator_loss: 0.8558101654052734\n",
            "For Step: 31810 recon_loss: 0.17716936767101288 \tdiscriminator_loss: 1.3066235780715942 \tgenerator_loss: 0.8808457851409912\n",
            "For Step: 31820 recon_loss: 0.18388640880584717 \tdiscriminator_loss: 1.2883563041687012 \tgenerator_loss: 0.874075174331665\n",
            "For Step: 31830 recon_loss: 0.1763063669204712 \tdiscriminator_loss: 1.2659813165664673 \tgenerator_loss: 0.8434797525405884\n",
            "For Step: 31840 recon_loss: 0.17820146679878235 \tdiscriminator_loss: 1.221320629119873 \tgenerator_loss: 0.937741219997406\n",
            "For Step: 31850 recon_loss: 0.17762793600559235 \tdiscriminator_loss: 1.2184102535247803 \tgenerator_loss: 0.856230616569519\n",
            "For Step: 31860 recon_loss: 0.16570071876049042 \tdiscriminator_loss: 1.3631312847137451 \tgenerator_loss: 0.8737417459487915\n",
            "For Step: 31870 recon_loss: 0.1650409698486328 \tdiscriminator_loss: 1.3215700387954712 \tgenerator_loss: 0.8186094760894775\n",
            "For Step: 31880 recon_loss: 0.17176461219787598 \tdiscriminator_loss: 1.2228915691375732 \tgenerator_loss: 0.8950487375259399\n",
            "For Step: 31890 recon_loss: 0.17519810795783997 \tdiscriminator_loss: 1.302161693572998 \tgenerator_loss: 0.8826237916946411\n",
            "For Step: 31900 recon_loss: 0.16466288268566132 \tdiscriminator_loss: 1.266829252243042 \tgenerator_loss: 0.8461060523986816\n",
            "For Step: 31910 recon_loss: 0.17974451184272766 \tdiscriminator_loss: 1.297684907913208 \tgenerator_loss: 0.8587551116943359\n",
            "For Step: 31920 recon_loss: 0.17964965105056763 \tdiscriminator_loss: 1.2302167415618896 \tgenerator_loss: 0.8752562999725342\n",
            "For Step: 31930 recon_loss: 0.17386527359485626 \tdiscriminator_loss: 1.3030602931976318 \tgenerator_loss: 0.8345227241516113\n",
            "For Step: 31940 recon_loss: 0.16459164023399353 \tdiscriminator_loss: 1.251798391342163 \tgenerator_loss: 0.8906001448631287\n",
            "For Step: 31950 recon_loss: 0.19186821579933167 \tdiscriminator_loss: 1.193037509918213 \tgenerator_loss: 0.9131360054016113\n",
            "For Step: 31960 recon_loss: 0.17719466984272003 \tdiscriminator_loss: 1.2878717184066772 \tgenerator_loss: 0.8476922512054443\n",
            "For Step: 31970 recon_loss: 0.17092794179916382 \tdiscriminator_loss: 1.3247430324554443 \tgenerator_loss: 0.8543981313705444\n",
            "For Step: 31980 recon_loss: 0.17844390869140625 \tdiscriminator_loss: 1.211228370666504 \tgenerator_loss: 0.9277116060256958\n",
            "For Step: 31990 recon_loss: 0.17177636921405792 \tdiscriminator_loss: 1.316537857055664 \tgenerator_loss: 0.8242948651313782\n",
            "For Step: 32000 recon_loss: 0.16407479345798492 \tdiscriminator_loss: 1.2802860736846924 \tgenerator_loss: 0.8567540049552917\n",
            "For Step: 32010 recon_loss: 0.17547081410884857 \tdiscriminator_loss: 1.3362162113189697 \tgenerator_loss: 0.8578067421913147\n",
            "For Step: 32020 recon_loss: 0.1721397489309311 \tdiscriminator_loss: 1.1959526538848877 \tgenerator_loss: 0.8463520407676697\n",
            "For Step: 32030 recon_loss: 0.17208322882652283 \tdiscriminator_loss: 1.2741472721099854 \tgenerator_loss: 0.818447470664978\n",
            "For Step: 32040 recon_loss: 0.17369344830513 \tdiscriminator_loss: 1.1459002494812012 \tgenerator_loss: 0.8630332946777344\n",
            "For Step: 32050 recon_loss: 0.16834807395935059 \tdiscriminator_loss: 1.2148382663726807 \tgenerator_loss: 0.8896886706352234\n",
            "For Step: 32060 recon_loss: 0.1764443814754486 \tdiscriminator_loss: 1.1611664295196533 \tgenerator_loss: 0.9505435228347778\n",
            "For Step: 32070 recon_loss: 0.18267512321472168 \tdiscriminator_loss: 1.269350528717041 \tgenerator_loss: 0.8623351454734802\n",
            "For Step: 32080 recon_loss: 0.16080909967422485 \tdiscriminator_loss: 1.2696332931518555 \tgenerator_loss: 0.8491968512535095\n",
            "For Step: 32090 recon_loss: 0.17267578840255737 \tdiscriminator_loss: 1.29368257522583 \tgenerator_loss: 0.8679080605506897\n",
            "For Step: 32100 recon_loss: 0.17120780050754547 \tdiscriminator_loss: 1.3062829971313477 \tgenerator_loss: 0.9280388355255127\n",
            "For Step: 32110 recon_loss: 0.16498802602291107 \tdiscriminator_loss: 1.2885499000549316 \tgenerator_loss: 0.9128326773643494\n",
            "For Step: 32120 recon_loss: 0.17709241807460785 \tdiscriminator_loss: 1.2240458726882935 \tgenerator_loss: 0.9004008769989014\n",
            "For Step: 32130 recon_loss: 0.17286701500415802 \tdiscriminator_loss: 1.1812556982040405 \tgenerator_loss: 0.908226490020752\n",
            "For Step: 32140 recon_loss: 0.17913024127483368 \tdiscriminator_loss: 1.2794920206069946 \tgenerator_loss: 0.9036601781845093\n",
            "For Step: 32150 recon_loss: 0.15713198482990265 \tdiscriminator_loss: 1.2698523998260498 \tgenerator_loss: 0.8183114528656006\n",
            "For Step: 32160 recon_loss: 0.1831706315279007 \tdiscriminator_loss: 1.2288906574249268 \tgenerator_loss: 0.8635439872741699\n",
            "For Step: 32170 recon_loss: 0.1670600175857544 \tdiscriminator_loss: 1.2121977806091309 \tgenerator_loss: 0.8893673419952393\n",
            "For Step: 32180 recon_loss: 0.17325447499752045 \tdiscriminator_loss: 1.230762243270874 \tgenerator_loss: 0.8936635851860046\n",
            "For Step: 32190 recon_loss: 0.17175596952438354 \tdiscriminator_loss: 1.1335471868515015 \tgenerator_loss: 0.9459108114242554\n",
            "For Step: 32200 recon_loss: 0.16904786229133606 \tdiscriminator_loss: 1.3078787326812744 \tgenerator_loss: 0.8674346208572388\n",
            "For Step: 32210 recon_loss: 0.16686570644378662 \tdiscriminator_loss: 1.3232479095458984 \tgenerator_loss: 0.8545112609863281\n",
            "For Step: 32220 recon_loss: 0.17702771723270416 \tdiscriminator_loss: 1.2767512798309326 \tgenerator_loss: 0.8488782048225403\n",
            "For Step: 32230 recon_loss: 0.1712106168270111 \tdiscriminator_loss: 1.1922096014022827 \tgenerator_loss: 0.8289019465446472\n",
            "For Step: 32240 recon_loss: 0.1901090145111084 \tdiscriminator_loss: 1.2369804382324219 \tgenerator_loss: 0.9056357145309448\n",
            "For Step: 32250 recon_loss: 0.16562260687351227 \tdiscriminator_loss: 1.298635721206665 \tgenerator_loss: 0.8365864157676697\n",
            "For Step: 32260 recon_loss: 0.17300330102443695 \tdiscriminator_loss: 1.2758393287658691 \tgenerator_loss: 0.8693113923072815\n",
            "For Step: 32270 recon_loss: 0.17345112562179565 \tdiscriminator_loss: 1.3306384086608887 \tgenerator_loss: 0.8972693681716919\n",
            "For Step: 32280 recon_loss: 0.17451857030391693 \tdiscriminator_loss: 1.3283740282058716 \tgenerator_loss: 0.8546693921089172\n",
            "For Step: 32290 recon_loss: 0.17829954624176025 \tdiscriminator_loss: 1.1900731325149536 \tgenerator_loss: 0.8729836344718933\n",
            "For Step: 32300 recon_loss: 0.17029671370983124 \tdiscriminator_loss: 1.2480106353759766 \tgenerator_loss: 0.8798751831054688\n",
            "For Step: 32310 recon_loss: 0.16367265582084656 \tdiscriminator_loss: 1.26068115234375 \tgenerator_loss: 0.8160799741744995\n",
            "For Step: 32320 recon_loss: 0.1620674580335617 \tdiscriminator_loss: 1.3089914321899414 \tgenerator_loss: 0.8706673979759216\n",
            "For Step: 32330 recon_loss: 0.17933888733386993 \tdiscriminator_loss: 1.18218994140625 \tgenerator_loss: 0.8748133182525635\n",
            "For Step: 32340 recon_loss: 0.17781026661396027 \tdiscriminator_loss: 1.2213387489318848 \tgenerator_loss: 0.8937892913818359\n",
            "For Step: 32350 recon_loss: 0.16514360904693604 \tdiscriminator_loss: 1.2536168098449707 \tgenerator_loss: 0.8470503091812134\n",
            "For Step: 32360 recon_loss: 0.1684136539697647 \tdiscriminator_loss: 1.3072190284729004 \tgenerator_loss: 0.8750417232513428\n",
            "For Step: 32370 recon_loss: 0.16746392846107483 \tdiscriminator_loss: 1.3189220428466797 \tgenerator_loss: 0.8968031406402588\n",
            "For Step: 32380 recon_loss: 0.17150934040546417 \tdiscriminator_loss: 1.2565619945526123 \tgenerator_loss: 0.8376532793045044\n",
            "For Step: 32390 recon_loss: 0.16902872920036316 \tdiscriminator_loss: 1.2473562955856323 \tgenerator_loss: 0.888106644153595\n",
            "For Step: 32400 recon_loss: 0.16625896096229553 \tdiscriminator_loss: 1.2429826259613037 \tgenerator_loss: 0.850864052772522\n",
            "For Step: 32410 recon_loss: 0.17064696550369263 \tdiscriminator_loss: 1.2375048398971558 \tgenerator_loss: 0.9011917114257812\n",
            "For Step: 32420 recon_loss: 0.17243877053260803 \tdiscriminator_loss: 1.2266167402267456 \tgenerator_loss: 0.9189454913139343\n",
            "For Step: 32430 recon_loss: 0.17629514634609222 \tdiscriminator_loss: 1.3104780912399292 \tgenerator_loss: 0.8317804336547852\n",
            "For Step: 32440 recon_loss: 0.17908011376857758 \tdiscriminator_loss: 1.3226583003997803 \tgenerator_loss: 0.865237832069397\n",
            "For Step: 32450 recon_loss: 0.16080202162265778 \tdiscriminator_loss: 1.3265453577041626 \tgenerator_loss: 0.8184919357299805\n",
            "For Step: 32460 recon_loss: 0.1913718730211258 \tdiscriminator_loss: 1.3147107362747192 \tgenerator_loss: 0.8173294067382812\n",
            "For Step: 32470 recon_loss: 0.17193058133125305 \tdiscriminator_loss: 1.1680107116699219 \tgenerator_loss: 0.9127346873283386\n",
            "For Step: 32480 recon_loss: 0.16911852359771729 \tdiscriminator_loss: 1.2582489252090454 \tgenerator_loss: 0.8811542391777039\n",
            "For Step: 32490 recon_loss: 0.17069274187088013 \tdiscriminator_loss: 1.2313590049743652 \tgenerator_loss: 0.8474348187446594\n",
            "For Step: 32500 recon_loss: 0.18178842961788177 \tdiscriminator_loss: 1.1792187690734863 \tgenerator_loss: 0.8599923849105835\n",
            "For Step: 32510 recon_loss: 0.16780567169189453 \tdiscriminator_loss: 1.2647185325622559 \tgenerator_loss: 0.8196564316749573\n",
            "For Step: 32520 recon_loss: 0.16982592642307281 \tdiscriminator_loss: 1.2637135982513428 \tgenerator_loss: 0.8905047178268433\n",
            "For Step: 32530 recon_loss: 0.17689348757266998 \tdiscriminator_loss: 1.1671435832977295 \tgenerator_loss: 0.8629263043403625\n",
            "For Step: 32540 recon_loss: 0.17228414118289948 \tdiscriminator_loss: 1.2967132329940796 \tgenerator_loss: 0.8595994710922241\n",
            "For Step: 32550 recon_loss: 0.17118073999881744 \tdiscriminator_loss: 1.0826140642166138 \tgenerator_loss: 0.8731489181518555\n",
            "For Step: 32560 recon_loss: 0.1667986512184143 \tdiscriminator_loss: 1.3854570388793945 \tgenerator_loss: 0.8380754590034485\n",
            "For Step: 32570 recon_loss: 0.16637101769447327 \tdiscriminator_loss: 1.298468828201294 \tgenerator_loss: 0.8602436780929565\n",
            "For Step: 32580 recon_loss: 0.1706131398677826 \tdiscriminator_loss: 1.2483952045440674 \tgenerator_loss: 0.8793754577636719\n",
            "For Step: 32590 recon_loss: 0.18190987408161163 \tdiscriminator_loss: 1.2292556762695312 \tgenerator_loss: 0.9009732007980347\n",
            "For Step: 32600 recon_loss: 0.173311248421669 \tdiscriminator_loss: 1.1776243448257446 \tgenerator_loss: 0.853600025177002\n",
            "For Step: 32610 recon_loss: 0.18430715799331665 \tdiscriminator_loss: 1.1867581605911255 \tgenerator_loss: 0.9116150140762329\n",
            "For Step: 32620 recon_loss: 0.17644336819648743 \tdiscriminator_loss: 1.2320432662963867 \tgenerator_loss: 0.9232349395751953\n",
            "For Step: 32630 recon_loss: 0.18815632164478302 \tdiscriminator_loss: 1.2439804077148438 \tgenerator_loss: 0.8808354139328003\n",
            "For Step: 32640 recon_loss: 0.16663359105587006 \tdiscriminator_loss: 1.1958887577056885 \tgenerator_loss: 0.8923791646957397\n",
            "For Step: 32650 recon_loss: 0.18042698502540588 \tdiscriminator_loss: 1.3109195232391357 \tgenerator_loss: 0.818449854850769\n",
            "For Step: 32660 recon_loss: 0.17948824167251587 \tdiscriminator_loss: 1.2218174934387207 \tgenerator_loss: 0.8802783489227295\n",
            "For Step: 32670 recon_loss: 0.1714041382074356 \tdiscriminator_loss: 1.2872722148895264 \tgenerator_loss: 0.8611905574798584\n",
            "For Step: 32680 recon_loss: 0.16593056917190552 \tdiscriminator_loss: 1.1573413610458374 \tgenerator_loss: 0.868773341178894\n",
            "For Step: 32690 recon_loss: 0.17277929186820984 \tdiscriminator_loss: 1.3304946422576904 \tgenerator_loss: 0.819110095500946\n",
            "For Step: 32700 recon_loss: 0.15703068673610687 \tdiscriminator_loss: 1.4195438623428345 \tgenerator_loss: 0.8193310499191284\n",
            "For Step: 32710 recon_loss: 0.16322553157806396 \tdiscriminator_loss: 1.2714532613754272 \tgenerator_loss: 0.8095170855522156\n",
            "For Step: 32720 recon_loss: 0.1618133783340454 \tdiscriminator_loss: 1.2203686237335205 \tgenerator_loss: 0.9046920537948608\n",
            "For Step: 32730 recon_loss: 0.1660822033882141 \tdiscriminator_loss: 1.3161910772323608 \tgenerator_loss: 0.8262075185775757\n",
            "For Step: 32740 recon_loss: 0.18067201972007751 \tdiscriminator_loss: 1.3188542127609253 \tgenerator_loss: 0.8190454244613647\n",
            "For Step: 32750 recon_loss: 0.1644686609506607 \tdiscriminator_loss: 1.179024338722229 \tgenerator_loss: 0.8552303314208984\n",
            "For Step: 32760 recon_loss: 0.18311099708080292 \tdiscriminator_loss: 1.2731263637542725 \tgenerator_loss: 0.8838587403297424\n",
            "For Step: 32770 recon_loss: 0.17097364366054535 \tdiscriminator_loss: 1.2756874561309814 \tgenerator_loss: 0.7931405305862427\n",
            "For Step: 32780 recon_loss: 0.17377012968063354 \tdiscriminator_loss: 1.250112771987915 \tgenerator_loss: 0.8829326629638672\n",
            "For Step: 32790 recon_loss: 0.17614810168743134 \tdiscriminator_loss: 1.2445998191833496 \tgenerator_loss: 0.8475772142410278\n",
            "For Step: 32800 recon_loss: 0.17431870102882385 \tdiscriminator_loss: 1.28104829788208 \tgenerator_loss: 0.9058982133865356\n",
            "For Step: 32810 recon_loss: 0.166287362575531 \tdiscriminator_loss: 1.2510501146316528 \tgenerator_loss: 0.82648766040802\n",
            "For Step: 32820 recon_loss: 0.17580170929431915 \tdiscriminator_loss: 1.156895637512207 \tgenerator_loss: 0.9514696598052979\n",
            "For Step: 32830 recon_loss: 0.16220131516456604 \tdiscriminator_loss: 1.283636212348938 \tgenerator_loss: 0.8295843601226807\n",
            "For Step: 32840 recon_loss: 0.1770150363445282 \tdiscriminator_loss: 1.1873633861541748 \tgenerator_loss: 0.9089757204055786\n",
            "For Step: 32850 recon_loss: 0.16996102035045624 \tdiscriminator_loss: 1.2835643291473389 \tgenerator_loss: 0.8484729528427124\n",
            "For Step: 32860 recon_loss: 0.17476513981819153 \tdiscriminator_loss: 1.222074270248413 \tgenerator_loss: 0.897040605545044\n",
            "For Step: 32870 recon_loss: 0.17576415836811066 \tdiscriminator_loss: 1.2732981443405151 \tgenerator_loss: 0.841907262802124\n",
            "For Step: 32880 recon_loss: 0.1689383089542389 \tdiscriminator_loss: 1.2615010738372803 \tgenerator_loss: 0.8223379254341125\n",
            "For Step: 32890 recon_loss: 0.17619286477565765 \tdiscriminator_loss: 1.285262107849121 \tgenerator_loss: 0.850037693977356\n",
            "For Step: 32900 recon_loss: 0.1864379197359085 \tdiscriminator_loss: 1.2995915412902832 \tgenerator_loss: 0.8293189406394958\n",
            "For Step: 32910 recon_loss: 0.19129067659378052 \tdiscriminator_loss: 1.2814881801605225 \tgenerator_loss: 0.8187524080276489\n",
            "For Step: 32920 recon_loss: 0.16471442580223083 \tdiscriminator_loss: 1.3592175245285034 \tgenerator_loss: 0.7741000652313232\n",
            "For Step: 32930 recon_loss: 0.17063434422016144 \tdiscriminator_loss: 1.3121552467346191 \tgenerator_loss: 0.8046474456787109\n",
            "For Step: 32940 recon_loss: 0.17919711768627167 \tdiscriminator_loss: 1.2556005716323853 \tgenerator_loss: 0.8492077589035034\n",
            "For Step: 32950 recon_loss: 0.17197512090206146 \tdiscriminator_loss: 1.16313636302948 \tgenerator_loss: 0.9122167825698853\n",
            "For Step: 32960 recon_loss: 0.18673202395439148 \tdiscriminator_loss: 1.2680456638336182 \tgenerator_loss: 0.8478055000305176\n",
            "For Step: 32970 recon_loss: 0.17626042664051056 \tdiscriminator_loss: 1.2732937335968018 \tgenerator_loss: 0.8596439361572266\n",
            "For Step: 32980 recon_loss: 0.17339648306369781 \tdiscriminator_loss: 1.3210928440093994 \tgenerator_loss: 0.8184556365013123\n",
            "For Step: 32990 recon_loss: 0.18208527565002441 \tdiscriminator_loss: 1.3433767557144165 \tgenerator_loss: 0.8425938487052917\n",
            "For Step: 33000 recon_loss: 0.18581436574459076 \tdiscriminator_loss: 1.3581302165985107 \tgenerator_loss: 0.9152059555053711\n",
            "For Step: 33010 recon_loss: 0.1767810583114624 \tdiscriminator_loss: 1.232630729675293 \tgenerator_loss: 0.8675245642662048\n",
            "For Step: 33020 recon_loss: 0.17479822039604187 \tdiscriminator_loss: 1.298621654510498 \tgenerator_loss: 0.8165608644485474\n",
            "For Step: 33030 recon_loss: 0.18558309972286224 \tdiscriminator_loss: 1.3271839618682861 \tgenerator_loss: 0.877143144607544\n",
            "For Step: 33040 recon_loss: 0.18355244398117065 \tdiscriminator_loss: 1.2827606201171875 \tgenerator_loss: 0.8655635118484497\n",
            "For Step: 33050 recon_loss: 0.16899846494197845 \tdiscriminator_loss: 1.386173963546753 \tgenerator_loss: 0.8232824802398682\n",
            "For Step: 33060 recon_loss: 0.16549931466579437 \tdiscriminator_loss: 1.2710652351379395 \tgenerator_loss: 0.8808075785636902\n",
            "For Step: 33070 recon_loss: 0.1742737889289856 \tdiscriminator_loss: 1.1939802169799805 \tgenerator_loss: 0.9013708829879761\n",
            "For Step: 33080 recon_loss: 0.17151443660259247 \tdiscriminator_loss: 1.2679270505905151 \tgenerator_loss: 0.8880395293235779\n",
            "For Step: 33090 recon_loss: 0.1697087585926056 \tdiscriminator_loss: 1.3410180807113647 \tgenerator_loss: 0.8464832305908203\n",
            "For Step: 33100 recon_loss: 0.1715950071811676 \tdiscriminator_loss: 1.26509690284729 \tgenerator_loss: 0.8643489480018616\n",
            "For Step: 33110 recon_loss: 0.17622707784175873 \tdiscriminator_loss: 1.2335269451141357 \tgenerator_loss: 0.8238564729690552\n",
            "For Step: 33120 recon_loss: 0.17799557745456696 \tdiscriminator_loss: 1.292954921722412 \tgenerator_loss: 0.8908891677856445\n",
            "For Step: 33130 recon_loss: 0.17406395077705383 \tdiscriminator_loss: 1.2956570386886597 \tgenerator_loss: 0.9013350009918213\n",
            "For Step: 33140 recon_loss: 0.17431986331939697 \tdiscriminator_loss: 1.1678656339645386 \tgenerator_loss: 0.887731671333313\n",
            "For Step: 33150 recon_loss: 0.17606806755065918 \tdiscriminator_loss: 1.213200330734253 \tgenerator_loss: 0.8850312829017639\n",
            "For Step: 33160 recon_loss: 0.16711799800395966 \tdiscriminator_loss: 1.172873854637146 \tgenerator_loss: 0.8695159554481506\n",
            "For Step: 33170 recon_loss: 0.16739267110824585 \tdiscriminator_loss: 1.4310462474822998 \tgenerator_loss: 0.7899101972579956\n",
            "For Step: 33180 recon_loss: 0.160841166973114 \tdiscriminator_loss: 1.25054931640625 \tgenerator_loss: 0.8734264969825745\n",
            "For Step: 33190 recon_loss: 0.17471082508563995 \tdiscriminator_loss: 1.2324292659759521 \tgenerator_loss: 0.9081001281738281\n",
            "For Step: 33200 recon_loss: 0.17537491023540497 \tdiscriminator_loss: 1.3307201862335205 \tgenerator_loss: 0.8879547119140625\n",
            "For Step: 33210 recon_loss: 0.17869633436203003 \tdiscriminator_loss: 1.2587780952453613 \tgenerator_loss: 0.8433154821395874\n",
            "For Step: 33220 recon_loss: 0.17685438692569733 \tdiscriminator_loss: 1.2010380029678345 \tgenerator_loss: 0.8720195293426514\n",
            "For Step: 33230 recon_loss: 0.18745678663253784 \tdiscriminator_loss: 1.2416889667510986 \tgenerator_loss: 0.827678918838501\n",
            "For Step: 33240 recon_loss: 0.17464104294776917 \tdiscriminator_loss: 1.2435154914855957 \tgenerator_loss: 0.8371243476867676\n",
            "For Step: 33250 recon_loss: 0.17355181276798248 \tdiscriminator_loss: 1.3364439010620117 \tgenerator_loss: 0.8440859317779541\n",
            "For Step: 33260 recon_loss: 0.17339789867401123 \tdiscriminator_loss: 1.2896844148635864 \tgenerator_loss: 0.875286340713501\n",
            "For Step: 33270 recon_loss: 0.16406495869159698 \tdiscriminator_loss: 1.3699902296066284 \tgenerator_loss: 0.8484336137771606\n",
            "For Step: 33280 recon_loss: 0.1749432384967804 \tdiscriminator_loss: 1.3391004800796509 \tgenerator_loss: 0.9014405012130737\n",
            "For Step: 33290 recon_loss: 0.17501571774482727 \tdiscriminator_loss: 1.3189337253570557 \tgenerator_loss: 0.8519930839538574\n",
            "For Step: 33300 recon_loss: 0.1739986538887024 \tdiscriminator_loss: 1.3304766416549683 \tgenerator_loss: 0.8401220440864563\n",
            "For Step: 33310 recon_loss: 0.16903987526893616 \tdiscriminator_loss: 1.299839735031128 \tgenerator_loss: 0.8824586272239685\n",
            "For Step: 33320 recon_loss: 0.1753283590078354 \tdiscriminator_loss: 1.2430568933486938 \tgenerator_loss: 0.8464873433113098\n",
            "For Step: 33330 recon_loss: 0.17332898080348969 \tdiscriminator_loss: 1.1627447605133057 \tgenerator_loss: 0.8602405786514282\n",
            "For Step: 33340 recon_loss: 0.17547647655010223 \tdiscriminator_loss: 1.2437790632247925 \tgenerator_loss: 0.8485933542251587\n",
            "For Step: 33350 recon_loss: 0.17603136599063873 \tdiscriminator_loss: 1.2014133930206299 \tgenerator_loss: 0.903052568435669\n",
            "For Step: 33360 recon_loss: 0.1695815920829773 \tdiscriminator_loss: 1.2446163892745972 \tgenerator_loss: 0.8894846439361572\n",
            "For Step: 33370 recon_loss: 0.16817480325698853 \tdiscriminator_loss: 1.3155418634414673 \tgenerator_loss: 0.8854793310165405\n",
            "For Step: 33380 recon_loss: 0.1851719617843628 \tdiscriminator_loss: 1.267075777053833 \tgenerator_loss: 0.809798538684845\n",
            "For Step: 33390 recon_loss: 0.17624258995056152 \tdiscriminator_loss: 1.2419500350952148 \tgenerator_loss: 0.8970198631286621\n",
            "For Step: 33400 recon_loss: 0.17862099409103394 \tdiscriminator_loss: 1.2011220455169678 \tgenerator_loss: 0.8581346273422241\n",
            "For Step: 33410 recon_loss: 0.1638152003288269 \tdiscriminator_loss: 1.2551324367523193 \tgenerator_loss: 0.8127905130386353\n",
            "For Step: 33420 recon_loss: 0.17595016956329346 \tdiscriminator_loss: 1.257904291152954 \tgenerator_loss: 0.9162200093269348\n",
            "For Step: 33430 recon_loss: 0.1878240704536438 \tdiscriminator_loss: 1.2792408466339111 \tgenerator_loss: 0.8594944477081299\n",
            "For Step: 33440 recon_loss: 0.17635752260684967 \tdiscriminator_loss: 1.2540555000305176 \tgenerator_loss: 0.846896231174469\n",
            "For Step: 33450 recon_loss: 0.1823309361934662 \tdiscriminator_loss: 1.2954548597335815 \tgenerator_loss: 0.8963775634765625\n",
            "For Step: 33460 recon_loss: 0.172442227602005 \tdiscriminator_loss: 1.2285537719726562 \tgenerator_loss: 0.8665989637374878\n",
            "For Step: 33470 recon_loss: 0.17773184180259705 \tdiscriminator_loss: 1.3071601390838623 \tgenerator_loss: 0.8017591238021851\n",
            "For Step: 33480 recon_loss: 0.1880665421485901 \tdiscriminator_loss: 1.2687275409698486 \tgenerator_loss: 0.8558837175369263\n",
            "For Step: 33490 recon_loss: 0.1699613630771637 \tdiscriminator_loss: 1.2315623760223389 \tgenerator_loss: 0.8218528032302856\n",
            "For Step: 33500 recon_loss: 0.18253692984580994 \tdiscriminator_loss: 1.223150610923767 \tgenerator_loss: 0.8531432151794434\n",
            "For Step: 33510 recon_loss: 0.18000221252441406 \tdiscriminator_loss: 1.2411483526229858 \tgenerator_loss: 0.8385907411575317\n",
            "For Step: 33520 recon_loss: 0.17062009871006012 \tdiscriminator_loss: 1.284949779510498 \tgenerator_loss: 0.859843373298645\n",
            "For Step: 33530 recon_loss: 0.16964399814605713 \tdiscriminator_loss: 1.23652982711792 \tgenerator_loss: 0.9331135749816895\n",
            "For Step: 33540 recon_loss: 0.17976392805576324 \tdiscriminator_loss: 1.2410670518875122 \tgenerator_loss: 0.90157151222229\n",
            "For Step: 33550 recon_loss: 0.1756013035774231 \tdiscriminator_loss: 1.3847824335098267 \tgenerator_loss: 0.8712578415870667\n",
            "For Step: 33560 recon_loss: 0.17855244874954224 \tdiscriminator_loss: 1.3453863859176636 \tgenerator_loss: 0.8195217847824097\n",
            "For Step: 33570 recon_loss: 0.18768516182899475 \tdiscriminator_loss: 1.260347843170166 \tgenerator_loss: 0.8481044769287109\n",
            "For Step: 33580 recon_loss: 0.17761701345443726 \tdiscriminator_loss: 1.2181007862091064 \tgenerator_loss: 0.8418827652931213\n",
            "For Step: 33590 recon_loss: 0.17526596784591675 \tdiscriminator_loss: 1.2770365476608276 \tgenerator_loss: 0.8363968133926392\n",
            "For Step: 33600 recon_loss: 0.1845197081565857 \tdiscriminator_loss: 1.2113683223724365 \tgenerator_loss: 0.8491668701171875\n",
            "For Step: 33610 recon_loss: 0.17956578731536865 \tdiscriminator_loss: 1.1849253177642822 \tgenerator_loss: 0.8496928811073303\n",
            "For Step: 33620 recon_loss: 0.1734686940908432 \tdiscriminator_loss: 1.212611436843872 \tgenerator_loss: 0.8690941333770752\n",
            "For Step: 33630 recon_loss: 0.17022903263568878 \tdiscriminator_loss: 1.2182807922363281 \tgenerator_loss: 0.8515042066574097\n",
            "For Step: 33640 recon_loss: 0.16731847822666168 \tdiscriminator_loss: 1.3217146396636963 \tgenerator_loss: 0.83150315284729\n",
            "For Step: 33650 recon_loss: 0.17799867689609528 \tdiscriminator_loss: 1.3375861644744873 \tgenerator_loss: 0.8429381847381592\n",
            "For Step: 33660 recon_loss: 0.17128252983093262 \tdiscriminator_loss: 1.2426857948303223 \tgenerator_loss: 0.833690881729126\n",
            "For Step: 33670 recon_loss: 0.17641879618167877 \tdiscriminator_loss: 1.231809139251709 \tgenerator_loss: 0.9060744047164917\n",
            "For Step: 33680 recon_loss: 0.17599451541900635 \tdiscriminator_loss: 1.2127130031585693 \tgenerator_loss: 0.8883213996887207\n",
            "For Step: 33690 recon_loss: 0.19459745287895203 \tdiscriminator_loss: 1.2370357513427734 \tgenerator_loss: 0.8655936121940613\n",
            "For Step: 33700 recon_loss: 0.17818622291088104 \tdiscriminator_loss: 1.3099031448364258 \tgenerator_loss: 0.83701491355896\n",
            "For Step: 33710 recon_loss: 0.18092456459999084 \tdiscriminator_loss: 1.2401258945465088 \tgenerator_loss: 0.825888991355896\n",
            "For Step: 33720 recon_loss: 0.1679755598306656 \tdiscriminator_loss: 1.291459321975708 \tgenerator_loss: 0.8797323703765869\n",
            "For Step: 33730 recon_loss: 0.16547831892967224 \tdiscriminator_loss: 1.142686128616333 \tgenerator_loss: 0.8889120817184448\n",
            "For Step: 33740 recon_loss: 0.17524532973766327 \tdiscriminator_loss: 1.3218557834625244 \tgenerator_loss: 0.8652732372283936\n",
            "For Step: 33750 recon_loss: 0.1885070651769638 \tdiscriminator_loss: 1.244680643081665 \tgenerator_loss: 0.8949832916259766\n",
            "For Step: 33760 recon_loss: 0.1662689447402954 \tdiscriminator_loss: 1.2737915515899658 \tgenerator_loss: 0.8354547023773193\n",
            "For Step: 33770 recon_loss: 0.1759921908378601 \tdiscriminator_loss: 1.264910101890564 \tgenerator_loss: 0.8725622296333313\n",
            "For Step: 33780 recon_loss: 0.18073499202728271 \tdiscriminator_loss: 1.3077648878097534 \tgenerator_loss: 0.8504348993301392\n",
            "For Step: 33790 recon_loss: 0.17018923163414001 \tdiscriminator_loss: 1.345228672027588 \tgenerator_loss: 0.8698296546936035\n",
            "For Step: 33800 recon_loss: 0.17112457752227783 \tdiscriminator_loss: 1.3022127151489258 \tgenerator_loss: 0.8632643222808838\n",
            "For Step: 33810 recon_loss: 0.17257921397686005 \tdiscriminator_loss: 1.315384864807129 \tgenerator_loss: 0.8425317406654358\n",
            "For Step: 33820 recon_loss: 0.17273667454719543 \tdiscriminator_loss: 1.3126485347747803 \tgenerator_loss: 0.8743677139282227\n",
            "For Step: 33830 recon_loss: 0.17468313872814178 \tdiscriminator_loss: 1.157670259475708 \tgenerator_loss: 0.917254626750946\n",
            "For Step: 33840 recon_loss: 0.16360454261302948 \tdiscriminator_loss: 1.282959222793579 \tgenerator_loss: 0.8516648411750793\n",
            "For Step: 33850 recon_loss: 0.16487230360507965 \tdiscriminator_loss: 1.3224655389785767 \tgenerator_loss: 0.8505553007125854\n",
            "For Step: 33860 recon_loss: 0.16660892963409424 \tdiscriminator_loss: 1.392454981803894 \tgenerator_loss: 0.8254538178443909\n",
            "For Step: 33870 recon_loss: 0.16946710646152496 \tdiscriminator_loss: 1.2203372716903687 \tgenerator_loss: 0.8978910446166992\n",
            "For Step: 33880 recon_loss: 0.17294646799564362 \tdiscriminator_loss: 1.2452197074890137 \tgenerator_loss: 0.8768601417541504\n",
            "For Step: 33890 recon_loss: 0.16355429589748383 \tdiscriminator_loss: 1.2582261562347412 \tgenerator_loss: 0.887550413608551\n",
            "For Step: 33900 recon_loss: 0.17337830364704132 \tdiscriminator_loss: 1.3531020879745483 \tgenerator_loss: 0.8213710784912109\n",
            "For Step: 33910 recon_loss: 0.17350471019744873 \tdiscriminator_loss: 1.2025814056396484 \tgenerator_loss: 0.8852784633636475\n",
            "For Step: 33920 recon_loss: 0.1729695200920105 \tdiscriminator_loss: 1.2524738311767578 \tgenerator_loss: 0.8609368205070496\n",
            "For Step: 33930 recon_loss: 0.17865072190761566 \tdiscriminator_loss: 1.2256933450698853 \tgenerator_loss: 0.8456816673278809\n",
            "For Step: 33940 recon_loss: 0.17479577660560608 \tdiscriminator_loss: 1.2299144268035889 \tgenerator_loss: 0.8588079810142517\n",
            "For Step: 33950 recon_loss: 0.17707893252372742 \tdiscriminator_loss: 1.212453007698059 \tgenerator_loss: 0.850926399230957\n",
            "For Step: 33960 recon_loss: 0.16945233941078186 \tdiscriminator_loss: 1.297869086265564 \tgenerator_loss: 0.8826017379760742\n",
            "For Step: 33970 recon_loss: 0.1693098098039627 \tdiscriminator_loss: 1.3181496858596802 \tgenerator_loss: 0.8410539627075195\n",
            "For Step: 33980 recon_loss: 0.16822674870491028 \tdiscriminator_loss: 1.188478946685791 \tgenerator_loss: 0.8760233521461487\n",
            "For Step: 33990 recon_loss: 0.17161864042282104 \tdiscriminator_loss: 1.3060553073883057 \tgenerator_loss: 0.8849198818206787\n",
            "For Step: 34000 recon_loss: 0.15695451200008392 \tdiscriminator_loss: 1.2199103832244873 \tgenerator_loss: 0.8569005131721497\n",
            "For Step: 34010 recon_loss: 0.17166823148727417 \tdiscriminator_loss: 1.2288718223571777 \tgenerator_loss: 0.8790169954299927\n",
            "For Step: 34020 recon_loss: 0.1653556376695633 \tdiscriminator_loss: 1.3375656604766846 \tgenerator_loss: 0.8032855987548828\n",
            "For Step: 34030 recon_loss: 0.17372073233127594 \tdiscriminator_loss: 1.273085594177246 \tgenerator_loss: 0.810580313205719\n",
            "For Step: 34040 recon_loss: 0.16396991908550262 \tdiscriminator_loss: 1.177246332168579 \tgenerator_loss: 0.9131231307983398\n",
            "For Step: 34050 recon_loss: 0.16187554597854614 \tdiscriminator_loss: 1.2312977313995361 \tgenerator_loss: 0.7997870445251465\n",
            "For Step: 34060 recon_loss: 0.1634473353624344 \tdiscriminator_loss: 1.2579275369644165 \tgenerator_loss: 0.8537284135818481\n",
            "For Step: 34070 recon_loss: 0.17314565181732178 \tdiscriminator_loss: 1.2415168285369873 \tgenerator_loss: 0.7971683740615845\n",
            "For Step: 34080 recon_loss: 0.17527474462985992 \tdiscriminator_loss: 1.2680972814559937 \tgenerator_loss: 0.861465334892273\n",
            "For Step: 34090 recon_loss: 0.16269107162952423 \tdiscriminator_loss: 1.2987715005874634 \tgenerator_loss: 0.8572696447372437\n",
            "For Step: 34100 recon_loss: 0.18117442727088928 \tdiscriminator_loss: 1.3164443969726562 \tgenerator_loss: 0.8701040744781494\n",
            "For Step: 34110 recon_loss: 0.18037235736846924 \tdiscriminator_loss: 1.2236671447753906 \tgenerator_loss: 0.8008770942687988\n",
            "For Step: 34120 recon_loss: 0.17085935175418854 \tdiscriminator_loss: 1.326348900794983 \tgenerator_loss: 0.8556146621704102\n",
            "For Step: 34130 recon_loss: 0.16812516748905182 \tdiscriminator_loss: 1.2690956592559814 \tgenerator_loss: 0.8369700908660889\n",
            "For Step: 34140 recon_loss: 0.1655569076538086 \tdiscriminator_loss: 1.2304407358169556 \tgenerator_loss: 0.8985608816146851\n",
            "For Step: 34150 recon_loss: 0.16752110421657562 \tdiscriminator_loss: 1.2558314800262451 \tgenerator_loss: 0.8579393625259399\n",
            "For Step: 34160 recon_loss: 0.1775374710559845 \tdiscriminator_loss: 1.2649867534637451 \tgenerator_loss: 0.8949326872825623\n",
            "For Step: 34170 recon_loss: 0.17388927936553955 \tdiscriminator_loss: 1.2722432613372803 \tgenerator_loss: 0.9009789228439331\n",
            "For Step: 34180 recon_loss: 0.17143893241882324 \tdiscriminator_loss: 1.2212213277816772 \tgenerator_loss: 0.8746298551559448\n",
            "For Step: 34190 recon_loss: 0.1700930893421173 \tdiscriminator_loss: 1.2282912731170654 \tgenerator_loss: 0.9004385471343994\n",
            "For Step: 34200 recon_loss: 0.17395414412021637 \tdiscriminator_loss: 1.2703320980072021 \tgenerator_loss: 0.8638805747032166\n",
            "For Step: 34210 recon_loss: 0.18075765669345856 \tdiscriminator_loss: 1.233445167541504 \tgenerator_loss: 0.8481121063232422\n",
            "For Step: 34220 recon_loss: 0.1589333862066269 \tdiscriminator_loss: 1.2866926193237305 \tgenerator_loss: 0.8348754048347473\n",
            "For Step: 34230 recon_loss: 0.16415323317050934 \tdiscriminator_loss: 1.2302541732788086 \tgenerator_loss: 0.877648115158081\n",
            "For Step: 34240 recon_loss: 0.1767951250076294 \tdiscriminator_loss: 1.219118356704712 \tgenerator_loss: 0.8130192756652832\n",
            "For Step: 34250 recon_loss: 0.16799238324165344 \tdiscriminator_loss: 1.2649216651916504 \tgenerator_loss: 0.8305056691169739\n",
            "For Step: 34260 recon_loss: 0.17181703448295593 \tdiscriminator_loss: 1.263230323791504 \tgenerator_loss: 0.8318192958831787\n",
            "For Step: 34270 recon_loss: 0.16629093885421753 \tdiscriminator_loss: 1.4002702236175537 \tgenerator_loss: 0.8381874561309814\n",
            "For Step: 34280 recon_loss: 0.15841782093048096 \tdiscriminator_loss: 1.2094051837921143 \tgenerator_loss: 0.9085281491279602\n",
            "For Step: 34290 recon_loss: 0.17175261676311493 \tdiscriminator_loss: 1.3107538223266602 \tgenerator_loss: 0.9121457934379578\n",
            "For Step: 34300 recon_loss: 0.1725205034017563 \tdiscriminator_loss: 1.3220915794372559 \tgenerator_loss: 0.8621611595153809\n",
            "For Step: 34310 recon_loss: 0.18524570763111115 \tdiscriminator_loss: 1.2654154300689697 \tgenerator_loss: 0.8600120544433594\n",
            "For Step: 34320 recon_loss: 0.183578222990036 \tdiscriminator_loss: 1.1938023567199707 \tgenerator_loss: 0.855758786201477\n",
            "For Step: 34330 recon_loss: 0.16471095383167267 \tdiscriminator_loss: 1.3159533739089966 \tgenerator_loss: 0.8525038957595825\n",
            "For Step: 34340 recon_loss: 0.17424094676971436 \tdiscriminator_loss: 1.2655354738235474 \tgenerator_loss: 0.8225895762443542\n",
            "For Step: 34350 recon_loss: 0.18484485149383545 \tdiscriminator_loss: 1.2071717977523804 \tgenerator_loss: 0.8771025538444519\n",
            "For Step: 34360 recon_loss: 0.16073407232761383 \tdiscriminator_loss: 1.3072655200958252 \tgenerator_loss: 0.8094684481620789\n",
            "For Step: 34370 recon_loss: 0.1771947294473648 \tdiscriminator_loss: 1.3090074062347412 \tgenerator_loss: 0.8528220057487488\n",
            "For Step: 34380 recon_loss: 0.16701994836330414 \tdiscriminator_loss: 1.3839597702026367 \tgenerator_loss: 0.8728352785110474\n",
            "For Step: 34390 recon_loss: 0.1635025590658188 \tdiscriminator_loss: 1.2506837844848633 \tgenerator_loss: 0.8898144960403442\n",
            "For Step: 34400 recon_loss: 0.17792290449142456 \tdiscriminator_loss: 1.2625985145568848 \tgenerator_loss: 0.8429858684539795\n",
            "For Step: 34410 recon_loss: 0.17854773998260498 \tdiscriminator_loss: 1.2782928943634033 \tgenerator_loss: 0.8040944337844849\n",
            "For Step: 34420 recon_loss: 0.1685752421617508 \tdiscriminator_loss: 1.3114745616912842 \tgenerator_loss: 0.8377822041511536\n",
            "For Step: 34430 recon_loss: 0.1745157688856125 \tdiscriminator_loss: 1.2849855422973633 \tgenerator_loss: 0.8725119829177856\n",
            "For Step: 34440 recon_loss: 0.17465293407440186 \tdiscriminator_loss: 1.2210347652435303 \tgenerator_loss: 0.8484146595001221\n",
            "For Step: 34450 recon_loss: 0.17792408168315887 \tdiscriminator_loss: 1.3413410186767578 \tgenerator_loss: 0.8691279888153076\n",
            "For Step: 34460 recon_loss: 0.18199515342712402 \tdiscriminator_loss: 1.2390280961990356 \tgenerator_loss: 0.9012938737869263\n",
            "For Step: 34470 recon_loss: 0.16261450946331024 \tdiscriminator_loss: 1.1693381071090698 \tgenerator_loss: 0.8117060661315918\n",
            "For Step: 34480 recon_loss: 0.17298537492752075 \tdiscriminator_loss: 1.3010504245758057 \tgenerator_loss: 0.8456811904907227\n",
            "For Step: 34490 recon_loss: 0.17630977928638458 \tdiscriminator_loss: 1.3657273054122925 \tgenerator_loss: 0.8434063196182251\n",
            "For Step: 34500 recon_loss: 0.18056024610996246 \tdiscriminator_loss: 1.2990636825561523 \tgenerator_loss: 0.8371363282203674\n",
            "For Step: 34510 recon_loss: 0.17425525188446045 \tdiscriminator_loss: 1.3040707111358643 \tgenerator_loss: 0.7954069375991821\n",
            "For Step: 34520 recon_loss: 0.18165171146392822 \tdiscriminator_loss: 1.2152230739593506 \tgenerator_loss: 0.8699353337287903\n",
            "For Step: 34530 recon_loss: 0.16913102567195892 \tdiscriminator_loss: 1.2607274055480957 \tgenerator_loss: 0.9111143350601196\n",
            "For Step: 34540 recon_loss: 0.179635688662529 \tdiscriminator_loss: 1.3249046802520752 \tgenerator_loss: 0.7994840145111084\n",
            "For Step: 34550 recon_loss: 0.167277991771698 \tdiscriminator_loss: 1.3626315593719482 \tgenerator_loss: 0.8324926495552063\n",
            "For Step: 34560 recon_loss: 0.19013860821723938 \tdiscriminator_loss: 1.2049431800842285 \tgenerator_loss: 0.9240829348564148\n",
            "For Step: 34570 recon_loss: 0.17455914616584778 \tdiscriminator_loss: 1.3618345260620117 \tgenerator_loss: 0.8434092402458191\n",
            "For Step: 34580 recon_loss: 0.17357800900936127 \tdiscriminator_loss: 1.3159936666488647 \tgenerator_loss: 0.8260104656219482\n",
            "For Step: 34590 recon_loss: 0.1688307225704193 \tdiscriminator_loss: 1.3008167743682861 \tgenerator_loss: 0.7998222708702087\n",
            "For Step: 34600 recon_loss: 0.16490845382213593 \tdiscriminator_loss: 1.2463443279266357 \tgenerator_loss: 0.8400620222091675\n",
            "For Step: 34610 recon_loss: 0.17064715921878815 \tdiscriminator_loss: 1.25984525680542 \tgenerator_loss: 0.8363527059555054\n",
            "For Step: 34620 recon_loss: 0.18670903146266937 \tdiscriminator_loss: 1.350270390510559 \tgenerator_loss: 0.7864151000976562\n",
            "For Step: 34630 recon_loss: 0.17317135632038116 \tdiscriminator_loss: 1.2233806848526 \tgenerator_loss: 0.8987728357315063\n",
            "For Step: 34640 recon_loss: 0.1689041256904602 \tdiscriminator_loss: 1.2346205711364746 \tgenerator_loss: 0.8687214255332947\n",
            "For Step: 34650 recon_loss: 0.15813623368740082 \tdiscriminator_loss: 1.2667909860610962 \tgenerator_loss: 0.8904183506965637\n",
            "For Step: 34660 recon_loss: 0.17686247825622559 \tdiscriminator_loss: 1.1930601596832275 \tgenerator_loss: 0.8628661632537842\n",
            "For Step: 34670 recon_loss: 0.16678760945796967 \tdiscriminator_loss: 1.3053202629089355 \tgenerator_loss: 0.8091492652893066\n",
            "For Step: 34680 recon_loss: 0.1794462352991104 \tdiscriminator_loss: 1.3017940521240234 \tgenerator_loss: 0.8547399640083313\n",
            "For Step: 34690 recon_loss: 0.1765030324459076 \tdiscriminator_loss: 1.299262523651123 \tgenerator_loss: 0.8722306489944458\n",
            "For Step: 34700 recon_loss: 0.17717614769935608 \tdiscriminator_loss: 1.3132030963897705 \tgenerator_loss: 0.843094527721405\n",
            "For Step: 34710 recon_loss: 0.17473076283931732 \tdiscriminator_loss: 1.167076826095581 \tgenerator_loss: 0.9100735187530518\n",
            "For Step: 34720 recon_loss: 0.17807650566101074 \tdiscriminator_loss: 1.1542212963104248 \tgenerator_loss: 0.8873004913330078\n",
            "For Step: 34730 recon_loss: 0.17751744389533997 \tdiscriminator_loss: 1.2000436782836914 \tgenerator_loss: 0.935064435005188\n",
            "For Step: 34740 recon_loss: 0.1703147292137146 \tdiscriminator_loss: 1.244875192642212 \tgenerator_loss: 0.8916758298873901\n",
            "For Step: 34750 recon_loss: 0.17887893319129944 \tdiscriminator_loss: 1.220231056213379 \tgenerator_loss: 0.882408857345581\n",
            "For Step: 34760 recon_loss: 0.1757119596004486 \tdiscriminator_loss: 1.3660881519317627 \tgenerator_loss: 0.7980636954307556\n",
            "For Step: 34770 recon_loss: 0.15695247054100037 \tdiscriminator_loss: 1.2183575630187988 \tgenerator_loss: 0.8425711989402771\n",
            "For Step: 34780 recon_loss: 0.17368417978286743 \tdiscriminator_loss: 1.3092862367630005 \tgenerator_loss: 0.7941005229949951\n",
            "For Step: 34790 recon_loss: 0.1837097406387329 \tdiscriminator_loss: 1.2822332382202148 \tgenerator_loss: 0.8679205179214478\n",
            "For Step: 34800 recon_loss: 0.1732240915298462 \tdiscriminator_loss: 1.1808550357818604 \tgenerator_loss: 0.9047384262084961\n",
            "For Step: 34810 recon_loss: 0.17565403878688812 \tdiscriminator_loss: 1.2122379541397095 \tgenerator_loss: 0.8447058200836182\n",
            "For Step: 34820 recon_loss: 0.17246216535568237 \tdiscriminator_loss: 1.3514795303344727 \tgenerator_loss: 0.8157461881637573\n",
            "For Step: 34830 recon_loss: 0.18492887914180756 \tdiscriminator_loss: 1.303438663482666 \tgenerator_loss: 0.807549238204956\n",
            "For Step: 34840 recon_loss: 0.1743444949388504 \tdiscriminator_loss: 1.2625908851623535 \tgenerator_loss: 0.8541889190673828\n",
            "For Step: 34850 recon_loss: 0.17155785858631134 \tdiscriminator_loss: 1.2707672119140625 \tgenerator_loss: 0.8600980043411255\n",
            "For Step: 34860 recon_loss: 0.17350344359874725 \tdiscriminator_loss: 1.217458963394165 \tgenerator_loss: 0.8841129541397095\n",
            "For Step: 34870 recon_loss: 0.18094950914382935 \tdiscriminator_loss: 1.2636590003967285 \tgenerator_loss: 0.8616412281990051\n",
            "For Step: 34880 recon_loss: 0.17065680027008057 \tdiscriminator_loss: 1.3338698148727417 \tgenerator_loss: 0.7950602769851685\n",
            "For Step: 34890 recon_loss: 0.17142684757709503 \tdiscriminator_loss: 1.2501649856567383 \tgenerator_loss: 0.8543496131896973\n",
            "For Step: 34900 recon_loss: 0.18386070430278778 \tdiscriminator_loss: 1.208961009979248 \tgenerator_loss: 0.9192187786102295\n",
            "For Step: 34910 recon_loss: 0.16867125034332275 \tdiscriminator_loss: 1.2573009729385376 \tgenerator_loss: 0.8456637859344482\n",
            "For Step: 34920 recon_loss: 0.17513789236545563 \tdiscriminator_loss: 1.2323977947235107 \tgenerator_loss: 0.8261199593544006\n",
            "For Step: 34930 recon_loss: 0.1681092083454132 \tdiscriminator_loss: 1.2289729118347168 \tgenerator_loss: 0.8647414445877075\n",
            "For Step: 34940 recon_loss: 0.177042156457901 \tdiscriminator_loss: 1.1984755992889404 \tgenerator_loss: 0.9172329902648926\n",
            "For Step: 34950 recon_loss: 0.17014718055725098 \tdiscriminator_loss: 1.263265609741211 \tgenerator_loss: 0.9022065997123718\n",
            "For Step: 34960 recon_loss: 0.17305327951908112 \tdiscriminator_loss: 1.2825465202331543 \tgenerator_loss: 0.8763399720191956\n",
            "For Step: 34970 recon_loss: 0.17504018545150757 \tdiscriminator_loss: 1.1994184255599976 \tgenerator_loss: 0.9113147258758545\n",
            "For Step: 34980 recon_loss: 0.1815793216228485 \tdiscriminator_loss: 1.263078212738037 \tgenerator_loss: 0.8250889182090759\n",
            "For Step: 34990 recon_loss: 0.1883351057767868 \tdiscriminator_loss: 1.1658228635787964 \tgenerator_loss: 0.9287046194076538\n",
            "For Step: 35000 recon_loss: 0.16955573856830597 \tdiscriminator_loss: 1.2707808017730713 \tgenerator_loss: 0.8171792030334473\n",
            "For Step: 35010 recon_loss: 0.17419305443763733 \tdiscriminator_loss: 1.2855581045150757 \tgenerator_loss: 0.8743144273757935\n",
            "For Step: 35020 recon_loss: 0.16667206585407257 \tdiscriminator_loss: 1.3018503189086914 \tgenerator_loss: 0.8205440044403076\n",
            "For Step: 35030 recon_loss: 0.18278981745243073 \tdiscriminator_loss: 1.3149316310882568 \tgenerator_loss: 0.8428215980529785\n",
            "For Step: 35040 recon_loss: 0.16508333384990692 \tdiscriminator_loss: 1.2565662860870361 \tgenerator_loss: 0.8357784748077393\n",
            "For Step: 35050 recon_loss: 0.1660022884607315 \tdiscriminator_loss: 1.2826237678527832 \tgenerator_loss: 0.8368232846260071\n",
            "For Step: 35060 recon_loss: 0.17386628687381744 \tdiscriminator_loss: 1.2865865230560303 \tgenerator_loss: 0.8377299904823303\n",
            "For Step: 35070 recon_loss: 0.17170141637325287 \tdiscriminator_loss: 1.3439918756484985 \tgenerator_loss: 0.873006284236908\n",
            "For Step: 35080 recon_loss: 0.1770526021718979 \tdiscriminator_loss: 1.1774711608886719 \tgenerator_loss: 0.8983874320983887\n",
            "For Step: 35090 recon_loss: 0.1612391173839569 \tdiscriminator_loss: 1.2473026514053345 \tgenerator_loss: 0.8064267039299011\n",
            "For Step: 35100 recon_loss: 0.17399029433727264 \tdiscriminator_loss: 1.3126858472824097 \tgenerator_loss: 0.8579531908035278\n",
            "For Step: 35110 recon_loss: 0.1745048612356186 \tdiscriminator_loss: 1.3124101161956787 \tgenerator_loss: 0.8697503805160522\n",
            "For Step: 35120 recon_loss: 0.17086148262023926 \tdiscriminator_loss: 1.2812443971633911 \tgenerator_loss: 0.8276679515838623\n",
            "For Step: 35130 recon_loss: 0.17701010406017303 \tdiscriminator_loss: 1.3056731224060059 \tgenerator_loss: 0.8363595604896545\n",
            "For Step: 35140 recon_loss: 0.17229816317558289 \tdiscriminator_loss: 1.282583236694336 \tgenerator_loss: 0.8309299945831299\n",
            "For Step: 35150 recon_loss: 0.1689387708902359 \tdiscriminator_loss: 1.2561641931533813 \tgenerator_loss: 0.876806914806366\n",
            "For Step: 35160 recon_loss: 0.17379894852638245 \tdiscriminator_loss: 1.2751977443695068 \tgenerator_loss: 0.843992292881012\n",
            "For Step: 35170 recon_loss: 0.17574843764305115 \tdiscriminator_loss: 1.2334200143814087 \tgenerator_loss: 0.911122739315033\n",
            "For Step: 35180 recon_loss: 0.17577658593654633 \tdiscriminator_loss: 1.280190348625183 \tgenerator_loss: 0.8335603475570679\n",
            "For Step: 35190 recon_loss: 0.16211654245853424 \tdiscriminator_loss: 1.4086570739746094 \tgenerator_loss: 0.7466414570808411\n",
            "For Step: 35200 recon_loss: 0.16697369515895844 \tdiscriminator_loss: 1.295823335647583 \tgenerator_loss: 0.8549484610557556\n",
            "For Step: 35210 recon_loss: 0.1858104020357132 \tdiscriminator_loss: 1.3381011486053467 \tgenerator_loss: 0.8291239738464355\n",
            "For Step: 35220 recon_loss: 0.17904426157474518 \tdiscriminator_loss: 1.2457760572433472 \tgenerator_loss: 0.874711275100708\n",
            "For Step: 35230 recon_loss: 0.17839846014976501 \tdiscriminator_loss: 1.2415034770965576 \tgenerator_loss: 0.8906563520431519\n",
            "For Step: 35240 recon_loss: 0.1707460582256317 \tdiscriminator_loss: 1.3307195901870728 \tgenerator_loss: 0.8060141205787659\n",
            "For Step: 35250 recon_loss: 0.16375702619552612 \tdiscriminator_loss: 1.3208973407745361 \tgenerator_loss: 0.8421267867088318\n",
            "For Step: 35260 recon_loss: 0.16219137609004974 \tdiscriminator_loss: 1.38970947265625 \tgenerator_loss: 0.8492059707641602\n",
            "For Step: 35270 recon_loss: 0.1821233630180359 \tdiscriminator_loss: 1.2541229724884033 \tgenerator_loss: 0.9133429527282715\n",
            "For Step: 35280 recon_loss: 0.17532765865325928 \tdiscriminator_loss: 1.2485936880111694 \tgenerator_loss: 0.886818528175354\n",
            "For Step: 35290 recon_loss: 0.1675814390182495 \tdiscriminator_loss: 1.265042781829834 \tgenerator_loss: 0.8615319728851318\n",
            "For Step: 35300 recon_loss: 0.16747520864009857 \tdiscriminator_loss: 1.2512049674987793 \tgenerator_loss: 0.811578631401062\n",
            "For Step: 35310 recon_loss: 0.18762393295764923 \tdiscriminator_loss: 1.3703694343566895 \tgenerator_loss: 0.8263938426971436\n",
            "For Step: 35320 recon_loss: 0.1817714273929596 \tdiscriminator_loss: 1.2589764595031738 \tgenerator_loss: 0.8535459041595459\n",
            "For Step: 35330 recon_loss: 0.17635796964168549 \tdiscriminator_loss: 1.362260103225708 \tgenerator_loss: 0.7766493558883667\n",
            "For Step: 35340 recon_loss: 0.16367678344249725 \tdiscriminator_loss: 1.2533299922943115 \tgenerator_loss: 0.8610784411430359\n",
            "For Step: 35350 recon_loss: 0.16335082054138184 \tdiscriminator_loss: 1.288461446762085 \tgenerator_loss: 0.7974544763565063\n",
            "For Step: 35360 recon_loss: 0.16664358973503113 \tdiscriminator_loss: 1.3333606719970703 \tgenerator_loss: 0.7834261655807495\n",
            "For Step: 35370 recon_loss: 0.1664571911096573 \tdiscriminator_loss: 1.2921264171600342 \tgenerator_loss: 0.8273816108703613\n",
            "For Step: 35380 recon_loss: 0.16170942783355713 \tdiscriminator_loss: 1.2405545711517334 \tgenerator_loss: 0.8751734495162964\n",
            "For Step: 35390 recon_loss: 0.17404231429100037 \tdiscriminator_loss: 1.24170982837677 \tgenerator_loss: 0.878196120262146\n",
            "For Step: 35400 recon_loss: 0.1695331335067749 \tdiscriminator_loss: 1.3307042121887207 \tgenerator_loss: 0.8464186191558838\n",
            "For Step: 35410 recon_loss: 0.16340240836143494 \tdiscriminator_loss: 1.3311684131622314 \tgenerator_loss: 0.8228969573974609\n",
            "For Step: 35420 recon_loss: 0.1637556552886963 \tdiscriminator_loss: 1.1666693687438965 \tgenerator_loss: 0.9108645915985107\n",
            "For Step: 35430 recon_loss: 0.1573222279548645 \tdiscriminator_loss: 1.3015894889831543 \tgenerator_loss: 0.8613132238388062\n",
            "For Step: 35440 recon_loss: 0.15806925296783447 \tdiscriminator_loss: 1.2125349044799805 \tgenerator_loss: 0.8647638559341431\n",
            "For Step: 35450 recon_loss: 0.17369139194488525 \tdiscriminator_loss: 1.2044353485107422 \tgenerator_loss: 0.8865662217140198\n",
            "For Step: 35460 recon_loss: 0.1754865050315857 \tdiscriminator_loss: 1.220282793045044 \tgenerator_loss: 0.7990093231201172\n",
            "For Step: 35470 recon_loss: 0.1786837875843048 \tdiscriminator_loss: 1.227664589881897 \tgenerator_loss: 0.7878497242927551\n",
            "For Step: 35480 recon_loss: 0.17454257607460022 \tdiscriminator_loss: 1.259089708328247 \tgenerator_loss: 0.8967109322547913\n",
            "For Step: 35490 recon_loss: 0.17717662453651428 \tdiscriminator_loss: 1.2415053844451904 \tgenerator_loss: 0.9024198055267334\n",
            "For Step: 35500 recon_loss: 0.16847892105579376 \tdiscriminator_loss: 1.2909213304519653 \tgenerator_loss: 0.8415664434432983\n",
            "For Step: 35510 recon_loss: 0.18181243538856506 \tdiscriminator_loss: 1.2561869621276855 \tgenerator_loss: 0.8134764432907104\n",
            "For Step: 35520 recon_loss: 0.171688511967659 \tdiscriminator_loss: 1.373399257659912 \tgenerator_loss: 0.8265125751495361\n",
            "For Step: 35530 recon_loss: 0.16423669457435608 \tdiscriminator_loss: 1.3464319705963135 \tgenerator_loss: 0.8369330763816833\n",
            "For Step: 35540 recon_loss: 0.160475492477417 \tdiscriminator_loss: 1.261826515197754 \tgenerator_loss: 0.8363485336303711\n",
            "For Step: 35550 recon_loss: 0.17093370854854584 \tdiscriminator_loss: 1.2526757717132568 \tgenerator_loss: 0.9102603197097778\n",
            "For Step: 35560 recon_loss: 0.1704089343547821 \tdiscriminator_loss: 1.1280473470687866 \tgenerator_loss: 0.8611587285995483\n",
            "For Step: 35570 recon_loss: 0.16595272719860077 \tdiscriminator_loss: 1.2028570175170898 \tgenerator_loss: 0.8741565346717834\n",
            "For Step: 35580 recon_loss: 0.16113072633743286 \tdiscriminator_loss: 1.299256682395935 \tgenerator_loss: 0.851581871509552\n",
            "For Step: 35590 recon_loss: 0.1742924153804779 \tdiscriminator_loss: 1.2865688800811768 \tgenerator_loss: 0.8838109970092773\n",
            "For Step: 35600 recon_loss: 0.16212989389896393 \tdiscriminator_loss: 1.2067348957061768 \tgenerator_loss: 0.8311859965324402\n",
            "For Step: 35610 recon_loss: 0.1729428619146347 \tdiscriminator_loss: 1.2803242206573486 \tgenerator_loss: 0.8544495105743408\n",
            "For Step: 35620 recon_loss: 0.16446244716644287 \tdiscriminator_loss: 1.3466778993606567 \tgenerator_loss: 0.8017123937606812\n",
            "For Step: 35630 recon_loss: 0.1751018762588501 \tdiscriminator_loss: 1.3342411518096924 \tgenerator_loss: 0.8254350423812866\n",
            "For Step: 35640 recon_loss: 0.16562917828559875 \tdiscriminator_loss: 1.2200543880462646 \tgenerator_loss: 0.8423699736595154\n",
            "For Step: 35650 recon_loss: 0.16955101490020752 \tdiscriminator_loss: 1.2663559913635254 \tgenerator_loss: 0.8725767135620117\n",
            "For Step: 35660 recon_loss: 0.17094892263412476 \tdiscriminator_loss: 1.2688772678375244 \tgenerator_loss: 0.8259925246238708\n",
            "For Step: 35670 recon_loss: 0.1730400025844574 \tdiscriminator_loss: 1.273618221282959 \tgenerator_loss: 0.8552285432815552\n",
            "For Step: 35680 recon_loss: 0.1735697239637375 \tdiscriminator_loss: 1.213209629058838 \tgenerator_loss: 0.8976340293884277\n",
            "For Step: 35690 recon_loss: 0.1684814989566803 \tdiscriminator_loss: 1.2177060842514038 \tgenerator_loss: 0.8580949306488037\n",
            "For Step: 35700 recon_loss: 0.1698271483182907 \tdiscriminator_loss: 1.312134027481079 \tgenerator_loss: 0.8173750638961792\n",
            "For Step: 35710 recon_loss: 0.16402971744537354 \tdiscriminator_loss: 1.3171334266662598 \tgenerator_loss: 0.8216027617454529\n",
            "For Step: 35720 recon_loss: 0.17272429168224335 \tdiscriminator_loss: 1.3575820922851562 \tgenerator_loss: 0.8460371494293213\n",
            "For Step: 35730 recon_loss: 0.17530345916748047 \tdiscriminator_loss: 1.3508405685424805 \tgenerator_loss: 0.8283416032791138\n",
            "For Step: 35740 recon_loss: 0.17192000150680542 \tdiscriminator_loss: 1.3384805917739868 \tgenerator_loss: 0.8053070306777954\n",
            "For Step: 35750 recon_loss: 0.1794160008430481 \tdiscriminator_loss: 1.2486541271209717 \tgenerator_loss: 0.858312726020813\n",
            "For Step: 35760 recon_loss: 0.17197231948375702 \tdiscriminator_loss: 1.2267570495605469 \tgenerator_loss: 0.8648384809494019\n",
            "For Step: 35770 recon_loss: 0.16918082535266876 \tdiscriminator_loss: 1.3438698053359985 \tgenerator_loss: 0.8311068415641785\n",
            "For Step: 35780 recon_loss: 0.1817166656255722 \tdiscriminator_loss: 1.269036054611206 \tgenerator_loss: 0.9150526523590088\n",
            "For Step: 35790 recon_loss: 0.16555951535701752 \tdiscriminator_loss: 1.3350870609283447 \tgenerator_loss: 0.8254823088645935\n",
            "For Step: 35800 recon_loss: 0.1724187731742859 \tdiscriminator_loss: 1.3908896446228027 \tgenerator_loss: 0.8473012447357178\n",
            "For Step: 35810 recon_loss: 0.17780852317810059 \tdiscriminator_loss: 1.2286021709442139 \tgenerator_loss: 0.8327158093452454\n",
            "For Step: 35820 recon_loss: 0.16573360562324524 \tdiscriminator_loss: 1.1888177394866943 \tgenerator_loss: 0.891261875629425\n",
            "For Step: 35830 recon_loss: 0.17412832379341125 \tdiscriminator_loss: 1.2787224054336548 \tgenerator_loss: 0.8189228773117065\n",
            "For Step: 35840 recon_loss: 0.16878736019134521 \tdiscriminator_loss: 1.2450443506240845 \tgenerator_loss: 0.8631407618522644\n",
            "For Step: 35850 recon_loss: 0.17003391683101654 \tdiscriminator_loss: 1.321794033050537 \tgenerator_loss: 0.8087373971939087\n",
            "For Step: 35860 recon_loss: 0.17402608692646027 \tdiscriminator_loss: 1.1956660747528076 \tgenerator_loss: 0.8759341239929199\n",
            "For Step: 35870 recon_loss: 0.16855104267597198 \tdiscriminator_loss: 1.2928104400634766 \tgenerator_loss: 0.8350284695625305\n",
            "For Step: 35880 recon_loss: 0.17702266573905945 \tdiscriminator_loss: 1.2308399677276611 \tgenerator_loss: 0.9114643335342407\n",
            "For Step: 35890 recon_loss: 0.18130119144916534 \tdiscriminator_loss: 1.350128173828125 \tgenerator_loss: 0.8172218203544617\n",
            "For Step: 35900 recon_loss: 0.1684005707502365 \tdiscriminator_loss: 1.2825560569763184 \tgenerator_loss: 0.8432713747024536\n",
            "For Step: 35910 recon_loss: 0.1789962202310562 \tdiscriminator_loss: 1.2839678525924683 \tgenerator_loss: 0.8432300090789795\n",
            "For Step: 35920 recon_loss: 0.17001184821128845 \tdiscriminator_loss: 1.3168938159942627 \tgenerator_loss: 0.8444852232933044\n",
            "For Step: 35930 recon_loss: 0.17469246685504913 \tdiscriminator_loss: 1.3227837085723877 \tgenerator_loss: 0.8449556827545166\n",
            "For Step: 35940 recon_loss: 0.18385660648345947 \tdiscriminator_loss: 1.2316869497299194 \tgenerator_loss: 0.8454470634460449\n",
            "For Step: 35950 recon_loss: 0.15364913642406464 \tdiscriminator_loss: 1.2358607053756714 \tgenerator_loss: 0.8358814120292664\n",
            "For Step: 35960 recon_loss: 0.1754866987466812 \tdiscriminator_loss: 1.3304650783538818 \tgenerator_loss: 0.836699366569519\n",
            "For Step: 35970 recon_loss: 0.17228896915912628 \tdiscriminator_loss: 1.2878913879394531 \tgenerator_loss: 0.8076601028442383\n",
            "For Step: 35980 recon_loss: 0.16303665935993195 \tdiscriminator_loss: 1.3030860424041748 \tgenerator_loss: 0.8396017551422119\n",
            "For Step: 35990 recon_loss: 0.17549684643745422 \tdiscriminator_loss: 1.3381285667419434 \tgenerator_loss: 0.8152703642845154\n",
            "For Step: 36000 recon_loss: 0.180185005068779 \tdiscriminator_loss: 1.2528274059295654 \tgenerator_loss: 0.9092777371406555\n",
            "For Step: 36010 recon_loss: 0.17937685549259186 \tdiscriminator_loss: 1.3454856872558594 \tgenerator_loss: 0.820357620716095\n",
            "For Step: 36020 recon_loss: 0.1739746779203415 \tdiscriminator_loss: 1.2192986011505127 \tgenerator_loss: 0.8849203586578369\n",
            "For Step: 36030 recon_loss: 0.17141400277614594 \tdiscriminator_loss: 1.3132076263427734 \tgenerator_loss: 0.8210546374320984\n",
            "For Step: 36040 recon_loss: 0.17284518480300903 \tdiscriminator_loss: 1.241332769393921 \tgenerator_loss: 0.8883809447288513\n",
            "For Step: 36050 recon_loss: 0.19369250535964966 \tdiscriminator_loss: 1.2932283878326416 \tgenerator_loss: 0.8654564023017883\n",
            "For Step: 36060 recon_loss: 0.17959320545196533 \tdiscriminator_loss: 1.230146884918213 \tgenerator_loss: 0.8693436980247498\n",
            "For Step: 36070 recon_loss: 0.17746831476688385 \tdiscriminator_loss: 1.2835278511047363 \tgenerator_loss: 0.8363816738128662\n",
            "For Step: 36080 recon_loss: 0.1835874617099762 \tdiscriminator_loss: 1.229456901550293 \tgenerator_loss: 0.8477143049240112\n",
            "For Step: 36090 recon_loss: 0.17443358898162842 \tdiscriminator_loss: 1.1967108249664307 \tgenerator_loss: 0.902180552482605\n",
            "For Step: 36100 recon_loss: 0.1738021820783615 \tdiscriminator_loss: 1.1735132932662964 \tgenerator_loss: 0.8873266577720642\n",
            "For Step: 36110 recon_loss: 0.1622651219367981 \tdiscriminator_loss: 1.3749403953552246 \tgenerator_loss: 0.7987700700759888\n",
            "For Step: 36120 recon_loss: 0.18439790606498718 \tdiscriminator_loss: 1.1708641052246094 \tgenerator_loss: 0.8549132347106934\n",
            "For Step: 36130 recon_loss: 0.16909578442573547 \tdiscriminator_loss: 1.260633945465088 \tgenerator_loss: 0.8733839988708496\n",
            "For Step: 36140 recon_loss: 0.18532583117485046 \tdiscriminator_loss: 1.2362453937530518 \tgenerator_loss: 0.8998615741729736\n",
            "For Step: 36150 recon_loss: 0.18059366941452026 \tdiscriminator_loss: 1.2326607704162598 \tgenerator_loss: 0.8755717277526855\n",
            "For Step: 36160 recon_loss: 0.17966395616531372 \tdiscriminator_loss: 1.227524995803833 \tgenerator_loss: 0.878374457359314\n",
            "For Step: 36170 recon_loss: 0.17445896565914154 \tdiscriminator_loss: 1.3391913175582886 \tgenerator_loss: 0.8523450493812561\n",
            "For Step: 36180 recon_loss: 0.17471803724765778 \tdiscriminator_loss: 1.202423095703125 \tgenerator_loss: 0.9231027364730835\n",
            "For Step: 36190 recon_loss: 0.17615237832069397 \tdiscriminator_loss: 1.2674407958984375 \tgenerator_loss: 0.8388516902923584\n",
            "For Step: 36200 recon_loss: 0.17289979755878448 \tdiscriminator_loss: 1.2225502729415894 \tgenerator_loss: 0.8734657764434814\n",
            "For Step: 36210 recon_loss: 0.17378799617290497 \tdiscriminator_loss: 1.244881510734558 \tgenerator_loss: 0.8107026219367981\n",
            "For Step: 36220 recon_loss: 0.1771169751882553 \tdiscriminator_loss: 1.314993143081665 \tgenerator_loss: 0.8679904341697693\n",
            "For Step: 36230 recon_loss: 0.1831514835357666 \tdiscriminator_loss: 1.2874170541763306 \tgenerator_loss: 0.8681089282035828\n",
            "For Step: 36240 recon_loss: 0.16971132159233093 \tdiscriminator_loss: 1.311580777168274 \tgenerator_loss: 0.8654322624206543\n",
            "For Step: 36250 recon_loss: 0.17857126891613007 \tdiscriminator_loss: 1.2593388557434082 \tgenerator_loss: 0.8110077381134033\n",
            "For Step: 36260 recon_loss: 0.17021766304969788 \tdiscriminator_loss: 1.201453685760498 \tgenerator_loss: 0.8698107600212097\n",
            "For Step: 36270 recon_loss: 0.17041020095348358 \tdiscriminator_loss: 1.298453688621521 \tgenerator_loss: 0.8482897877693176\n",
            "For Step: 36280 recon_loss: 0.16494780778884888 \tdiscriminator_loss: 1.2634559869766235 \tgenerator_loss: 0.8703343868255615\n",
            "For Step: 36290 recon_loss: 0.16716650128364563 \tdiscriminator_loss: 1.3297210931777954 \tgenerator_loss: 0.8834477066993713\n",
            "For Step: 36300 recon_loss: 0.15180253982543945 \tdiscriminator_loss: 1.2522473335266113 \tgenerator_loss: 0.8326632976531982\n",
            "For Step: 36310 recon_loss: 0.1709846705198288 \tdiscriminator_loss: 1.313569188117981 \tgenerator_loss: 0.8539965748786926\n",
            "For Step: 36320 recon_loss: 0.16657264530658722 \tdiscriminator_loss: 1.247535228729248 \tgenerator_loss: 0.8577903509140015\n",
            "For Step: 36330 recon_loss: 0.1770474761724472 \tdiscriminator_loss: 1.2937471866607666 \tgenerator_loss: 0.8249905109405518\n",
            "For Step: 36340 recon_loss: 0.1589800864458084 \tdiscriminator_loss: 1.2999181747436523 \tgenerator_loss: 0.8124862909317017\n",
            "For Step: 36350 recon_loss: 0.17136307060718536 \tdiscriminator_loss: 1.16206955909729 \tgenerator_loss: 0.8285634517669678\n",
            "For Step: 36360 recon_loss: 0.1638914942741394 \tdiscriminator_loss: 1.2897871732711792 \tgenerator_loss: 0.8624781966209412\n",
            "For Step: 36370 recon_loss: 0.185494065284729 \tdiscriminator_loss: 1.3071675300598145 \tgenerator_loss: 0.8105575442314148\n",
            "For Step: 36380 recon_loss: 0.17158691585063934 \tdiscriminator_loss: 1.2393932342529297 \tgenerator_loss: 0.8747508525848389\n",
            "For Step: 36390 recon_loss: 0.171426460146904 \tdiscriminator_loss: 1.3097310066223145 \tgenerator_loss: 0.8229807615280151\n",
            "For Step: 36400 recon_loss: 0.17022106051445007 \tdiscriminator_loss: 1.2952523231506348 \tgenerator_loss: 0.844878077507019\n",
            "For Step: 36410 recon_loss: 0.17015185952186584 \tdiscriminator_loss: 1.3597452640533447 \tgenerator_loss: 0.8459337949752808\n",
            "For Step: 36420 recon_loss: 0.17833296954631805 \tdiscriminator_loss: 1.2652463912963867 \tgenerator_loss: 0.8726121187210083\n",
            "For Step: 36430 recon_loss: 0.18338310718536377 \tdiscriminator_loss: 1.2669916152954102 \tgenerator_loss: 0.9310309886932373\n",
            "For Step: 36440 recon_loss: 0.17186979949474335 \tdiscriminator_loss: 1.3216965198516846 \tgenerator_loss: 0.821605920791626\n",
            "For Step: 36450 recon_loss: 0.1710309535264969 \tdiscriminator_loss: 1.3029500246047974 \tgenerator_loss: 0.8317348957061768\n",
            "For Step: 36460 recon_loss: 0.18922992050647736 \tdiscriminator_loss: 1.2562549114227295 \tgenerator_loss: 0.888500452041626\n",
            "For Step: 36470 recon_loss: 0.180024653673172 \tdiscriminator_loss: 1.380521535873413 \tgenerator_loss: 0.8311673998832703\n",
            "For Step: 36480 recon_loss: 0.17509259283542633 \tdiscriminator_loss: 1.370615839958191 \tgenerator_loss: 0.8080655336380005\n",
            "For Step: 36490 recon_loss: 0.16148827970027924 \tdiscriminator_loss: 1.2237522602081299 \tgenerator_loss: 0.8561023473739624\n",
            "For Step: 36500 recon_loss: 0.17110902070999146 \tdiscriminator_loss: 1.1897040605545044 \tgenerator_loss: 0.9383805394172668\n",
            "For Step: 36510 recon_loss: 0.162301167845726 \tdiscriminator_loss: 1.2708660364151 \tgenerator_loss: 0.8461945652961731\n",
            "For Step: 36520 recon_loss: 0.18554605543613434 \tdiscriminator_loss: 1.3013315200805664 \tgenerator_loss: 0.8360309600830078\n",
            "For Step: 36530 recon_loss: 0.17585071921348572 \tdiscriminator_loss: 1.217156171798706 \tgenerator_loss: 0.900989294052124\n",
            "For Step: 36540 recon_loss: 0.1738605946302414 \tdiscriminator_loss: 1.2975733280181885 \tgenerator_loss: 0.8102163076400757\n",
            "For Step: 36550 recon_loss: 0.1812727451324463 \tdiscriminator_loss: 1.2658404111862183 \tgenerator_loss: 0.873916745185852\n",
            "For Step: 36560 recon_loss: 0.18751871585845947 \tdiscriminator_loss: 1.2675914764404297 \tgenerator_loss: 0.8770360946655273\n",
            "For Step: 36570 recon_loss: 0.1644885390996933 \tdiscriminator_loss: 1.192678689956665 \tgenerator_loss: 0.855911374092102\n",
            "For Step: 36580 recon_loss: 0.1741739809513092 \tdiscriminator_loss: 1.3876206874847412 \tgenerator_loss: 0.7866389751434326\n",
            "For Step: 36590 recon_loss: 0.1808132380247116 \tdiscriminator_loss: 1.251096248626709 \tgenerator_loss: 0.8671665787696838\n",
            "For Step: 36600 recon_loss: 0.17631787061691284 \tdiscriminator_loss: 1.305051565170288 \tgenerator_loss: 0.7949426770210266\n",
            "For Step: 36610 recon_loss: 0.17530226707458496 \tdiscriminator_loss: 1.3655767440795898 \tgenerator_loss: 0.8264394998550415\n",
            "For Step: 36620 recon_loss: 0.15586578845977783 \tdiscriminator_loss: 1.2172272205352783 \tgenerator_loss: 0.8583358526229858\n",
            "For Step: 36630 recon_loss: 0.17115861177444458 \tdiscriminator_loss: 1.3568739891052246 \tgenerator_loss: 0.7603043913841248\n",
            "For Step: 36640 recon_loss: 0.1862410306930542 \tdiscriminator_loss: 1.2833476066589355 \tgenerator_loss: 0.8122057318687439\n",
            "For Step: 36650 recon_loss: 0.1711597591638565 \tdiscriminator_loss: 1.3205130100250244 \tgenerator_loss: 0.8176887035369873\n",
            "For Step: 36660 recon_loss: 0.16565026342868805 \tdiscriminator_loss: 1.335196614265442 \tgenerator_loss: 0.8550686836242676\n",
            "For Step: 36670 recon_loss: 0.16072574257850647 \tdiscriminator_loss: 1.2817906141281128 \tgenerator_loss: 0.8500721454620361\n",
            "For Step: 36680 recon_loss: 0.1670323610305786 \tdiscriminator_loss: 1.233805775642395 \tgenerator_loss: 0.8641422390937805\n",
            "For Step: 36690 recon_loss: 0.17392608523368835 \tdiscriminator_loss: 1.2057905197143555 \tgenerator_loss: 0.9073296785354614\n",
            "For Step: 36700 recon_loss: 0.15893058478832245 \tdiscriminator_loss: 1.310278296470642 \tgenerator_loss: 0.8290112614631653\n",
            "For Step: 36710 recon_loss: 0.16662593185901642 \tdiscriminator_loss: 1.3011419773101807 \tgenerator_loss: 0.8186373114585876\n",
            "For Step: 36720 recon_loss: 0.16913098096847534 \tdiscriminator_loss: 1.222104787826538 \tgenerator_loss: 0.8851433992385864\n",
            "For Step: 36730 recon_loss: 0.17821305990219116 \tdiscriminator_loss: 1.2076764106750488 \tgenerator_loss: 0.7999627590179443\n",
            "For Step: 36740 recon_loss: 0.163290336728096 \tdiscriminator_loss: 1.21851646900177 \tgenerator_loss: 0.8341728448867798\n",
            "For Step: 36750 recon_loss: 0.17705503106117249 \tdiscriminator_loss: 1.2294477224349976 \tgenerator_loss: 0.8586372137069702\n",
            "For Step: 36760 recon_loss: 0.16184180974960327 \tdiscriminator_loss: 1.2707796096801758 \tgenerator_loss: 0.8164291381835938\n",
            "For Step: 36770 recon_loss: 0.18073770403862 \tdiscriminator_loss: 1.2706786394119263 \tgenerator_loss: 0.8596035242080688\n",
            "For Step: 36780 recon_loss: 0.16731031239032745 \tdiscriminator_loss: 1.2926292419433594 \tgenerator_loss: 0.7918471097946167\n",
            "For Step: 36790 recon_loss: 0.18069131672382355 \tdiscriminator_loss: 1.3100509643554688 \tgenerator_loss: 0.8302045464515686\n",
            "For Step: 36800 recon_loss: 0.17058271169662476 \tdiscriminator_loss: 1.3181859254837036 \tgenerator_loss: 0.8045512437820435\n",
            "For Step: 36810 recon_loss: 0.18062478303909302 \tdiscriminator_loss: 1.2736351490020752 \tgenerator_loss: 0.8734934329986572\n",
            "For Step: 36820 recon_loss: 0.176835834980011 \tdiscriminator_loss: 1.2247464656829834 \tgenerator_loss: 0.8436907529830933\n",
            "For Step: 36830 recon_loss: 0.16635361313819885 \tdiscriminator_loss: 1.2919344902038574 \tgenerator_loss: 0.8376543521881104\n",
            "For Step: 36840 recon_loss: 0.17351971566677094 \tdiscriminator_loss: 1.1521519422531128 \tgenerator_loss: 0.8618594408035278\n",
            "For Step: 36850 recon_loss: 0.177493155002594 \tdiscriminator_loss: 1.3337030410766602 \tgenerator_loss: 0.8306174278259277\n",
            "For Step: 36860 recon_loss: 0.18208174407482147 \tdiscriminator_loss: 1.2537590265274048 \tgenerator_loss: 0.8945403695106506\n",
            "For Step: 36870 recon_loss: 0.1720878630876541 \tdiscriminator_loss: 1.2497624158859253 \tgenerator_loss: 0.8463266491889954\n",
            "For Step: 36880 recon_loss: 0.17108096182346344 \tdiscriminator_loss: 1.3234355449676514 \tgenerator_loss: 0.8178461790084839\n",
            "For Step: 36890 recon_loss: 0.1706114411354065 \tdiscriminator_loss: 1.2914533615112305 \tgenerator_loss: 0.817136824131012\n",
            "For Step: 36900 recon_loss: 0.16557078063488007 \tdiscriminator_loss: 1.257045030593872 \tgenerator_loss: 0.8458088636398315\n",
            "For Step: 36910 recon_loss: 0.16648361086845398 \tdiscriminator_loss: 1.1884081363677979 \tgenerator_loss: 0.8489900827407837\n",
            "For Step: 36920 recon_loss: 0.16835245490074158 \tdiscriminator_loss: 1.3352464437484741 \tgenerator_loss: 0.7963197231292725\n",
            "For Step: 36930 recon_loss: 0.18286430835723877 \tdiscriminator_loss: 1.3404390811920166 \tgenerator_loss: 0.842767059803009\n",
            "For Step: 36940 recon_loss: 0.16816385090351105 \tdiscriminator_loss: 1.2390403747558594 \tgenerator_loss: 0.8407601714134216\n",
            "For Step: 36950 recon_loss: 0.17523817718029022 \tdiscriminator_loss: 1.2984673976898193 \tgenerator_loss: 0.83670574426651\n",
            "For Step: 36960 recon_loss: 0.17163683474063873 \tdiscriminator_loss: 1.3547720909118652 \tgenerator_loss: 0.8417413830757141\n",
            "For Step: 36970 recon_loss: 0.16850771009922028 \tdiscriminator_loss: 1.2953848838806152 \tgenerator_loss: 0.8393588662147522\n",
            "For Step: 36980 recon_loss: 0.16464018821716309 \tdiscriminator_loss: 1.2708981037139893 \tgenerator_loss: 0.8174697160720825\n",
            "For Step: 36990 recon_loss: 0.16694019734859467 \tdiscriminator_loss: 1.3052098751068115 \tgenerator_loss: 0.8400332927703857\n",
            "For Step: 37000 recon_loss: 0.16854864358901978 \tdiscriminator_loss: 1.3753238916397095 \tgenerator_loss: 0.8129492998123169\n",
            "For Step: 37010 recon_loss: 0.18213596940040588 \tdiscriminator_loss: 1.2933857440948486 \tgenerator_loss: 0.8200490474700928\n",
            "For Step: 37020 recon_loss: 0.1680750995874405 \tdiscriminator_loss: 1.2180161476135254 \tgenerator_loss: 0.8710153102874756\n",
            "For Step: 37030 recon_loss: 0.1733705997467041 \tdiscriminator_loss: 1.3716247081756592 \tgenerator_loss: 0.8366479873657227\n",
            "For Step: 37040 recon_loss: 0.17229099571704865 \tdiscriminator_loss: 1.2436436414718628 \tgenerator_loss: 0.8641687035560608\n",
            "For Step: 37050 recon_loss: 0.1871410310268402 \tdiscriminator_loss: 1.2522387504577637 \tgenerator_loss: 0.8926020264625549\n",
            "For Step: 37060 recon_loss: 0.16547629237174988 \tdiscriminator_loss: 1.2329474687576294 \tgenerator_loss: 0.882754921913147\n",
            "For Step: 37070 recon_loss: 0.18718306720256805 \tdiscriminator_loss: 1.2296706438064575 \tgenerator_loss: 0.8989390134811401\n",
            "For Step: 37080 recon_loss: 0.17282681167125702 \tdiscriminator_loss: 1.3066258430480957 \tgenerator_loss: 0.878043532371521\n",
            "For Step: 37090 recon_loss: 0.17624430358409882 \tdiscriminator_loss: 1.264449119567871 \tgenerator_loss: 0.8443275094032288\n",
            "For Step: 37100 recon_loss: 0.16653354465961456 \tdiscriminator_loss: 1.3037735223770142 \tgenerator_loss: 0.8735479712486267\n",
            "For Step: 37110 recon_loss: 0.17946258187294006 \tdiscriminator_loss: 1.3226350545883179 \tgenerator_loss: 0.8372241854667664\n",
            "For Step: 37120 recon_loss: 0.17495790123939514 \tdiscriminator_loss: 1.3733446598052979 \tgenerator_loss: 0.81379234790802\n",
            "For Step: 37130 recon_loss: 0.17462258040905 \tdiscriminator_loss: 1.256058931350708 \tgenerator_loss: 0.8766363859176636\n",
            "For Step: 37140 recon_loss: 0.16478216648101807 \tdiscriminator_loss: 1.2077159881591797 \tgenerator_loss: 0.8399824500083923\n",
            "For Step: 37150 recon_loss: 0.17573122680187225 \tdiscriminator_loss: 1.4236791133880615 \tgenerator_loss: 0.761374294757843\n",
            "For Step: 37160 recon_loss: 0.18030644953250885 \tdiscriminator_loss: 1.3171806335449219 \tgenerator_loss: 0.8434597253799438\n",
            "For Step: 37170 recon_loss: 0.1691114455461502 \tdiscriminator_loss: 1.2675520181655884 \tgenerator_loss: 0.8514982461929321\n",
            "For Step: 37180 recon_loss: 0.1805875301361084 \tdiscriminator_loss: 1.2685441970825195 \tgenerator_loss: 0.854012131690979\n",
            "For Step: 37190 recon_loss: 0.1775297075510025 \tdiscriminator_loss: 1.2987515926361084 \tgenerator_loss: 0.8295199275016785\n",
            "For Step: 37200 recon_loss: 0.16947665810585022 \tdiscriminator_loss: 1.3856253623962402 \tgenerator_loss: 0.805342435836792\n",
            "For Step: 37210 recon_loss: 0.1821419894695282 \tdiscriminator_loss: 1.267364501953125 \tgenerator_loss: 0.8660370111465454\n",
            "For Step: 37220 recon_loss: 0.18327409029006958 \tdiscriminator_loss: 1.3146686553955078 \tgenerator_loss: 0.8788758516311646\n",
            "For Step: 37230 recon_loss: 0.1677895188331604 \tdiscriminator_loss: 1.2718312740325928 \tgenerator_loss: 0.8238489627838135\n",
            "For Step: 37240 recon_loss: 0.1660556197166443 \tdiscriminator_loss: 1.233797550201416 \tgenerator_loss: 0.8401492834091187\n",
            "For Step: 37250 recon_loss: 0.17758095264434814 \tdiscriminator_loss: 1.195604681968689 \tgenerator_loss: 0.852587103843689\n",
            "For Step: 37260 recon_loss: 0.1740153431892395 \tdiscriminator_loss: 1.3101449012756348 \tgenerator_loss: 0.8514695167541504\n",
            "For Step: 37270 recon_loss: 0.1700073480606079 \tdiscriminator_loss: 1.2324769496917725 \tgenerator_loss: 0.8589779138565063\n",
            "For Step: 37280 recon_loss: 0.17813155055046082 \tdiscriminator_loss: 1.3350350856781006 \tgenerator_loss: 0.8285914063453674\n",
            "For Step: 37290 recon_loss: 0.1563095897436142 \tdiscriminator_loss: 1.2775263786315918 \tgenerator_loss: 0.8746241331100464\n",
            "For Step: 37300 recon_loss: 0.17599117755889893 \tdiscriminator_loss: 1.2358512878417969 \tgenerator_loss: 0.8098725080490112\n",
            "For Step: 37310 recon_loss: 0.16948704421520233 \tdiscriminator_loss: 1.3419814109802246 \tgenerator_loss: 0.8107905387878418\n",
            "For Step: 37320 recon_loss: 0.16373133659362793 \tdiscriminator_loss: 1.3201477527618408 \tgenerator_loss: 0.8459941744804382\n",
            "For Step: 37330 recon_loss: 0.1653822362422943 \tdiscriminator_loss: 1.3167335987091064 \tgenerator_loss: 0.8089994192123413\n",
            "For Step: 37340 recon_loss: 0.17654551565647125 \tdiscriminator_loss: 1.2575125694274902 \tgenerator_loss: 0.8639923334121704\n",
            "For Step: 37350 recon_loss: 0.1643809825181961 \tdiscriminator_loss: 1.2126359939575195 \tgenerator_loss: 0.8116188049316406\n",
            "For Step: 37360 recon_loss: 0.17999565601348877 \tdiscriminator_loss: 1.383504867553711 \tgenerator_loss: 0.7856029272079468\n",
            "For Step: 37370 recon_loss: 0.19004802405834198 \tdiscriminator_loss: 1.3184592723846436 \tgenerator_loss: 0.8345245122909546\n",
            "For Step: 37380 recon_loss: 0.17933234572410583 \tdiscriminator_loss: 1.2581851482391357 \tgenerator_loss: 0.8721950054168701\n",
            "For Step: 37390 recon_loss: 0.17247545719146729 \tdiscriminator_loss: 1.2835642099380493 \tgenerator_loss: 0.8683990240097046\n",
            "For Step: 37400 recon_loss: 0.16261525452136993 \tdiscriminator_loss: 1.271787405014038 \tgenerator_loss: 0.7838951945304871\n",
            "For Step: 37410 recon_loss: 0.1690642237663269 \tdiscriminator_loss: 1.3356993198394775 \tgenerator_loss: 0.8195536732673645\n",
            "For Step: 37420 recon_loss: 0.18401837348937988 \tdiscriminator_loss: 1.2493488788604736 \tgenerator_loss: 0.863662838935852\n",
            "For Step: 37430 recon_loss: 0.16743126511573792 \tdiscriminator_loss: 1.2736499309539795 \tgenerator_loss: 0.8207096457481384\n",
            "For Step: 37440 recon_loss: 0.17591038346290588 \tdiscriminator_loss: 1.3224457502365112 \tgenerator_loss: 0.8279606103897095\n",
            "For Step: 37450 recon_loss: 0.17221318185329437 \tdiscriminator_loss: 1.2845830917358398 \tgenerator_loss: 0.8436693549156189\n",
            "For Step: 37460 recon_loss: 0.17618350684642792 \tdiscriminator_loss: 1.2522034645080566 \tgenerator_loss: 0.8439353704452515\n",
            "For Step: 37470 recon_loss: 0.17704741656780243 \tdiscriminator_loss: 1.2848782539367676 \tgenerator_loss: 0.8460506200790405\n",
            "For Step: 37480 recon_loss: 0.16817963123321533 \tdiscriminator_loss: 1.2758376598358154 \tgenerator_loss: 0.8153219223022461\n",
            "For Step: 37490 recon_loss: 0.17443999648094177 \tdiscriminator_loss: 1.2143903970718384 \tgenerator_loss: 0.8887888789176941\n",
            "For Step: 37500 recon_loss: 0.1608094573020935 \tdiscriminator_loss: 1.389279842376709 \tgenerator_loss: 0.792672872543335\n",
            "For Step: 37510 recon_loss: 0.17149244248867035 \tdiscriminator_loss: 1.318922758102417 \tgenerator_loss: 0.8026769161224365\n",
            "For Step: 37520 recon_loss: 0.17339231073856354 \tdiscriminator_loss: 1.2706398963928223 \tgenerator_loss: 0.8200551271438599\n",
            "For Step: 37530 recon_loss: 0.17580993473529816 \tdiscriminator_loss: 1.3446216583251953 \tgenerator_loss: 0.7998428344726562\n",
            "For Step: 37540 recon_loss: 0.15626101195812225 \tdiscriminator_loss: 1.2215850353240967 \tgenerator_loss: 0.8751832246780396\n",
            "For Step: 37550 recon_loss: 0.17489752173423767 \tdiscriminator_loss: 1.2861484289169312 \tgenerator_loss: 0.8153290748596191\n",
            "For Step: 37560 recon_loss: 0.16493745148181915 \tdiscriminator_loss: 1.2693160772323608 \tgenerator_loss: 0.8106215000152588\n",
            "For Step: 37570 recon_loss: 0.17777329683303833 \tdiscriminator_loss: 1.2178415060043335 \tgenerator_loss: 0.8484180569648743\n",
            "For Step: 37580 recon_loss: 0.17414994537830353 \tdiscriminator_loss: 1.137966275215149 \tgenerator_loss: 0.8686681389808655\n",
            "For Step: 37590 recon_loss: 0.1725233942270279 \tdiscriminator_loss: 1.3157365322113037 \tgenerator_loss: 0.8123639822006226\n",
            "For Step: 37600 recon_loss: 0.18247267603874207 \tdiscriminator_loss: 1.3335835933685303 \tgenerator_loss: 0.7931652069091797\n",
            "For Step: 37610 recon_loss: 0.1758359670639038 \tdiscriminator_loss: 1.2659995555877686 \tgenerator_loss: 0.8455438017845154\n",
            "For Step: 37620 recon_loss: 0.1680997759103775 \tdiscriminator_loss: 1.4494386911392212 \tgenerator_loss: 0.8187023997306824\n",
            "For Step: 37630 recon_loss: 0.17042994499206543 \tdiscriminator_loss: 1.3778696060180664 \tgenerator_loss: 0.8463459610939026\n",
            "For Step: 37640 recon_loss: 0.17543812096118927 \tdiscriminator_loss: 1.243019938468933 \tgenerator_loss: 0.8357254266738892\n",
            "For Step: 37650 recon_loss: 0.16670164465904236 \tdiscriminator_loss: 1.2486828565597534 \tgenerator_loss: 0.8624172210693359\n",
            "For Step: 37660 recon_loss: 0.17190100252628326 \tdiscriminator_loss: 1.3207322359085083 \tgenerator_loss: 0.8440829515457153\n",
            "For Step: 37670 recon_loss: 0.1657051146030426 \tdiscriminator_loss: 1.249542474746704 \tgenerator_loss: 0.8073223233222961\n",
            "For Step: 37680 recon_loss: 0.17632795870304108 \tdiscriminator_loss: 1.250290870666504 \tgenerator_loss: 0.8541423082351685\n",
            "For Step: 37690 recon_loss: 0.17103511095046997 \tdiscriminator_loss: 1.2258929014205933 \tgenerator_loss: 0.8372880816459656\n",
            "For Step: 37700 recon_loss: 0.16544818878173828 \tdiscriminator_loss: 1.334478497505188 \tgenerator_loss: 0.8391290307044983\n",
            "For Step: 37710 recon_loss: 0.17521946132183075 \tdiscriminator_loss: 1.3498109579086304 \tgenerator_loss: 0.8552274107933044\n",
            "For Step: 37720 recon_loss: 0.1724323332309723 \tdiscriminator_loss: 1.2042622566223145 \tgenerator_loss: 0.8827017545700073\n",
            "For Step: 37730 recon_loss: 0.15934136509895325 \tdiscriminator_loss: 1.2612812519073486 \tgenerator_loss: 0.8640023469924927\n",
            "For Step: 37740 recon_loss: 0.1749786138534546 \tdiscriminator_loss: 1.2182838916778564 \tgenerator_loss: 0.8742870092391968\n",
            "For Step: 37750 recon_loss: 0.17950651049613953 \tdiscriminator_loss: 1.2251482009887695 \tgenerator_loss: 0.8620951771736145\n",
            "For Step: 37760 recon_loss: 0.1794809103012085 \tdiscriminator_loss: 1.328769564628601 \tgenerator_loss: 0.8151031732559204\n",
            "For Step: 37770 recon_loss: 0.17018476128578186 \tdiscriminator_loss: 1.3908021450042725 \tgenerator_loss: 0.7942336201667786\n",
            "For Step: 37780 recon_loss: 0.16316662728786469 \tdiscriminator_loss: 1.2731904983520508 \tgenerator_loss: 0.8559516668319702\n",
            "For Step: 37790 recon_loss: 0.16155484318733215 \tdiscriminator_loss: 1.2946947813034058 \tgenerator_loss: 0.8621758818626404\n",
            "For Step: 37800 recon_loss: 0.18146374821662903 \tdiscriminator_loss: 1.294570803642273 \tgenerator_loss: 0.8587536215782166\n",
            "For Step: 37810 recon_loss: 0.16656838357448578 \tdiscriminator_loss: 1.2813990116119385 \tgenerator_loss: 0.8252797722816467\n",
            "For Step: 37820 recon_loss: 0.18018373847007751 \tdiscriminator_loss: 1.2490025758743286 \tgenerator_loss: 0.8395202159881592\n",
            "For Step: 37830 recon_loss: 0.1824633777141571 \tdiscriminator_loss: 1.1891241073608398 \tgenerator_loss: 0.8823044300079346\n",
            "For Step: 37840 recon_loss: 0.17536965012550354 \tdiscriminator_loss: 1.3272733688354492 \tgenerator_loss: 0.8329152464866638\n",
            "For Step: 37850 recon_loss: 0.16984519362449646 \tdiscriminator_loss: 1.199192762374878 \tgenerator_loss: 0.8758544325828552\n",
            "For Step: 37860 recon_loss: 0.1742977350950241 \tdiscriminator_loss: 1.2465336322784424 \tgenerator_loss: 0.877153217792511\n",
            "For Step: 37870 recon_loss: 0.16769687831401825 \tdiscriminator_loss: 1.2035815715789795 \tgenerator_loss: 0.8361420631408691\n",
            "For Step: 37880 recon_loss: 0.1779877245426178 \tdiscriminator_loss: 1.3835506439208984 \tgenerator_loss: 0.8231966495513916\n",
            "For Step: 37890 recon_loss: 0.1570507287979126 \tdiscriminator_loss: 1.2649317979812622 \tgenerator_loss: 0.8276644349098206\n",
            "For Step: 37900 recon_loss: 0.17415466904640198 \tdiscriminator_loss: 1.209526538848877 \tgenerator_loss: 0.8353455662727356\n",
            "For Step: 37910 recon_loss: 0.1825259029865265 \tdiscriminator_loss: 1.2303544282913208 \tgenerator_loss: 0.8761492967605591\n",
            "For Step: 37920 recon_loss: 0.17410977184772491 \tdiscriminator_loss: 1.2143301963806152 \tgenerator_loss: 0.8431991338729858\n",
            "For Step: 37930 recon_loss: 0.1780894696712494 \tdiscriminator_loss: 1.3636674880981445 \tgenerator_loss: 0.8361537456512451\n",
            "For Step: 37940 recon_loss: 0.1777903437614441 \tdiscriminator_loss: 1.302030324935913 \tgenerator_loss: 0.7809038162231445\n",
            "For Step: 37950 recon_loss: 0.16288529336452484 \tdiscriminator_loss: 1.2663825750350952 \tgenerator_loss: 0.785291314125061\n",
            "For Step: 37960 recon_loss: 0.1740827113389969 \tdiscriminator_loss: 1.27080237865448 \tgenerator_loss: 0.8097431063652039\n",
            "For Step: 37970 recon_loss: 0.1726093292236328 \tdiscriminator_loss: 1.3261805772781372 \tgenerator_loss: 0.8188310861587524\n",
            "For Step: 37980 recon_loss: 0.17667002975940704 \tdiscriminator_loss: 1.305294156074524 \tgenerator_loss: 0.8571718335151672\n",
            "For Step: 37990 recon_loss: 0.1753419190645218 \tdiscriminator_loss: 1.27505624294281 \tgenerator_loss: 0.858493983745575\n",
            "For Step: 38000 recon_loss: 0.18026286363601685 \tdiscriminator_loss: 1.2900936603546143 \tgenerator_loss: 0.8638674020767212\n",
            "For Step: 38010 recon_loss: 0.16315364837646484 \tdiscriminator_loss: 1.279976487159729 \tgenerator_loss: 0.8261667490005493\n",
            "For Step: 38020 recon_loss: 0.18336805701255798 \tdiscriminator_loss: 1.2473533153533936 \tgenerator_loss: 0.8437262773513794\n",
            "For Step: 38030 recon_loss: 0.1628587543964386 \tdiscriminator_loss: 1.3162269592285156 \tgenerator_loss: 0.8224829435348511\n",
            "For Step: 38040 recon_loss: 0.16666971147060394 \tdiscriminator_loss: 1.3844181299209595 \tgenerator_loss: 0.8027477860450745\n",
            "For Step: 38050 recon_loss: 0.1618945300579071 \tdiscriminator_loss: 1.2602354288101196 \tgenerator_loss: 0.8927576541900635\n",
            "For Step: 38060 recon_loss: 0.16962935030460358 \tdiscriminator_loss: 1.356550693511963 \tgenerator_loss: 0.8470964431762695\n",
            "For Step: 38070 recon_loss: 0.17426425218582153 \tdiscriminator_loss: 1.245741367340088 \tgenerator_loss: 0.8373063802719116\n",
            "For Step: 38080 recon_loss: 0.16652829945087433 \tdiscriminator_loss: 1.2961513996124268 \tgenerator_loss: 0.8866689205169678\n",
            "For Step: 38090 recon_loss: 0.1746785044670105 \tdiscriminator_loss: 1.2679476737976074 \tgenerator_loss: 0.8332417011260986\n",
            "For Step: 38100 recon_loss: 0.16939228773117065 \tdiscriminator_loss: 1.4166196584701538 \tgenerator_loss: 0.7864614129066467\n",
            "For Step: 38110 recon_loss: 0.1749967336654663 \tdiscriminator_loss: 1.215079665184021 \tgenerator_loss: 0.867770791053772\n",
            "For Step: 38120 recon_loss: 0.16533678770065308 \tdiscriminator_loss: 1.3335686922073364 \tgenerator_loss: 0.8634023070335388\n",
            "For Step: 38130 recon_loss: 0.18228089809417725 \tdiscriminator_loss: 1.3284108638763428 \tgenerator_loss: 0.8510408997535706\n",
            "For Step: 38140 recon_loss: 0.1640530377626419 \tdiscriminator_loss: 1.3460789918899536 \tgenerator_loss: 0.8281701803207397\n",
            "For Step: 38150 recon_loss: 0.1780545562505722 \tdiscriminator_loss: 1.2530486583709717 \tgenerator_loss: 0.82489013671875\n",
            "For Step: 38160 recon_loss: 0.16620180010795593 \tdiscriminator_loss: 1.3308467864990234 \tgenerator_loss: 0.8228705525398254\n",
            "For Step: 38170 recon_loss: 0.17723311483860016 \tdiscriminator_loss: 1.3110934495925903 \tgenerator_loss: 0.8371020555496216\n",
            "For Step: 38180 recon_loss: 0.1713617891073227 \tdiscriminator_loss: 1.2828381061553955 \tgenerator_loss: 0.8390469551086426\n",
            "For Step: 38190 recon_loss: 0.17001692950725555 \tdiscriminator_loss: 1.2280232906341553 \tgenerator_loss: 0.8409895896911621\n",
            "For Step: 38200 recon_loss: 0.1806732416152954 \tdiscriminator_loss: 1.2912800312042236 \tgenerator_loss: 0.8396021127700806\n",
            "For Step: 38210 recon_loss: 0.1711498349905014 \tdiscriminator_loss: 1.2534892559051514 \tgenerator_loss: 0.8527308702468872\n",
            "For Step: 38220 recon_loss: 0.17082513868808746 \tdiscriminator_loss: 1.3987243175506592 \tgenerator_loss: 0.7983588576316833\n",
            "For Step: 38230 recon_loss: 0.1760634183883667 \tdiscriminator_loss: 1.236726999282837 \tgenerator_loss: 0.826136589050293\n",
            "For Step: 38240 recon_loss: 0.1726824939250946 \tdiscriminator_loss: 1.2327969074249268 \tgenerator_loss: 0.8099585771560669\n",
            "For Step: 38250 recon_loss: 0.17266394197940826 \tdiscriminator_loss: 1.2728071212768555 \tgenerator_loss: 0.7799708843231201\n",
            "For Step: 38260 recon_loss: 0.17919489741325378 \tdiscriminator_loss: 1.204878330230713 \tgenerator_loss: 0.8139706254005432\n",
            "For Step: 38270 recon_loss: 0.18629080057144165 \tdiscriminator_loss: 1.4173600673675537 \tgenerator_loss: 0.8014622926712036\n",
            "For Step: 38280 recon_loss: 0.17541912198066711 \tdiscriminator_loss: 1.2738325595855713 \tgenerator_loss: 0.902646541595459\n",
            "For Step: 38290 recon_loss: 0.17335689067840576 \tdiscriminator_loss: 1.305149793624878 \tgenerator_loss: 0.8179284334182739\n",
            "For Step: 38300 recon_loss: 0.180467426776886 \tdiscriminator_loss: 1.2583444118499756 \tgenerator_loss: 0.8526123762130737\n",
            "For Step: 38310 recon_loss: 0.1623796671628952 \tdiscriminator_loss: 1.2888057231903076 \tgenerator_loss: 0.7844690084457397\n",
            "For Step: 38320 recon_loss: 0.17266368865966797 \tdiscriminator_loss: 1.2559064626693726 \tgenerator_loss: 0.8107037544250488\n",
            "For Step: 38330 recon_loss: 0.1653662472963333 \tdiscriminator_loss: 1.3130559921264648 \tgenerator_loss: 0.8286951780319214\n",
            "For Step: 38340 recon_loss: 0.18667730689048767 \tdiscriminator_loss: 1.3429863452911377 \tgenerator_loss: 0.7801591157913208\n",
            "For Step: 38350 recon_loss: 0.165034219622612 \tdiscriminator_loss: 1.2602823972702026 \tgenerator_loss: 0.8662748336791992\n",
            "For Step: 38360 recon_loss: 0.16518181562423706 \tdiscriminator_loss: 1.2475651502609253 \tgenerator_loss: 0.8302443027496338\n",
            "For Step: 38370 recon_loss: 0.17234942317008972 \tdiscriminator_loss: 1.2754157781600952 \tgenerator_loss: 0.8640240430831909\n",
            "For Step: 38380 recon_loss: 0.17303232848644257 \tdiscriminator_loss: 1.1930230855941772 \tgenerator_loss: 0.914592981338501\n",
            "For Step: 38390 recon_loss: 0.18521669507026672 \tdiscriminator_loss: 1.3406823873519897 \tgenerator_loss: 0.7885230183601379\n",
            "For Step: 38400 recon_loss: 0.1712578386068344 \tdiscriminator_loss: 1.2258741855621338 \tgenerator_loss: 0.8862038850784302\n",
            "For Step: 38410 recon_loss: 0.18014469742774963 \tdiscriminator_loss: 1.297013282775879 \tgenerator_loss: 0.8884382843971252\n",
            "For Step: 38420 recon_loss: 0.15781845152378082 \tdiscriminator_loss: 1.3766448497772217 \tgenerator_loss: 0.7871589660644531\n",
            "For Step: 38430 recon_loss: 0.1812707781791687 \tdiscriminator_loss: 1.3547797203063965 \tgenerator_loss: 0.7989866733551025\n",
            "For Step: 38440 recon_loss: 0.16731849312782288 \tdiscriminator_loss: 1.2769290208816528 \tgenerator_loss: 0.8230448961257935\n",
            "For Step: 38450 recon_loss: 0.16400296986103058 \tdiscriminator_loss: 1.4227478504180908 \tgenerator_loss: 0.7868666648864746\n",
            "For Step: 38460 recon_loss: 0.17080914974212646 \tdiscriminator_loss: 1.2928154468536377 \tgenerator_loss: 0.8696261644363403\n",
            "For Step: 38470 recon_loss: 0.17268601059913635 \tdiscriminator_loss: 1.2315195798873901 \tgenerator_loss: 0.8820325136184692\n",
            "For Step: 38480 recon_loss: 0.17280779778957367 \tdiscriminator_loss: 1.219818115234375 \tgenerator_loss: 0.9292709231376648\n",
            "For Step: 38490 recon_loss: 0.17286904156208038 \tdiscriminator_loss: 1.1529598236083984 \tgenerator_loss: 0.8513196110725403\n",
            "For Step: 38500 recon_loss: 0.16361306607723236 \tdiscriminator_loss: 1.2873280048370361 \tgenerator_loss: 0.8550902605056763\n",
            "For Step: 38510 recon_loss: 0.1643466353416443 \tdiscriminator_loss: 1.3540568351745605 \tgenerator_loss: 0.75770103931427\n",
            "For Step: 38520 recon_loss: 0.16690044105052948 \tdiscriminator_loss: 1.2731664180755615 \tgenerator_loss: 0.853950560092926\n",
            "For Step: 38530 recon_loss: 0.17437787353992462 \tdiscriminator_loss: 1.2194337844848633 \tgenerator_loss: 0.837161123752594\n",
            "For Step: 38540 recon_loss: 0.16825932264328003 \tdiscriminator_loss: 1.3979606628417969 \tgenerator_loss: 0.7952063679695129\n",
            "For Step: 38550 recon_loss: 0.1731865555047989 \tdiscriminator_loss: 1.2752833366394043 \tgenerator_loss: 0.8347340822219849\n",
            "For Step: 38560 recon_loss: 0.1613570749759674 \tdiscriminator_loss: 1.4070314168930054 \tgenerator_loss: 0.8173246383666992\n",
            "For Step: 38570 recon_loss: 0.17823335528373718 \tdiscriminator_loss: 1.2275738716125488 \tgenerator_loss: 0.8765413761138916\n",
            "For Step: 38580 recon_loss: 0.17884807288646698 \tdiscriminator_loss: 1.1955077648162842 \tgenerator_loss: 0.8348508477210999\n",
            "For Step: 38590 recon_loss: 0.17278355360031128 \tdiscriminator_loss: 1.306758999824524 \tgenerator_loss: 0.8531555533409119\n",
            "For Step: 38600 recon_loss: 0.17097704112529755 \tdiscriminator_loss: 1.3116328716278076 \tgenerator_loss: 0.8235095739364624\n",
            "For Step: 38610 recon_loss: 0.17380434274673462 \tdiscriminator_loss: 1.2579751014709473 \tgenerator_loss: 0.8651360273361206\n",
            "For Step: 38620 recon_loss: 0.17680378258228302 \tdiscriminator_loss: 1.3941991329193115 \tgenerator_loss: 0.8175983428955078\n",
            "For Step: 38630 recon_loss: 0.17664006352424622 \tdiscriminator_loss: 1.2347757816314697 \tgenerator_loss: 0.870059609413147\n",
            "For Step: 38640 recon_loss: 0.16747257113456726 \tdiscriminator_loss: 1.2568563222885132 \tgenerator_loss: 0.8784297108650208\n",
            "For Step: 38650 recon_loss: 0.1608775407075882 \tdiscriminator_loss: 1.4010343551635742 \tgenerator_loss: 0.7728021144866943\n",
            "For Step: 38660 recon_loss: 0.1681046038866043 \tdiscriminator_loss: 1.314633846282959 \tgenerator_loss: 0.7950679063796997\n",
            "For Step: 38670 recon_loss: 0.16710740327835083 \tdiscriminator_loss: 1.2399203777313232 \tgenerator_loss: 0.8276783227920532\n",
            "For Step: 38680 recon_loss: 0.17423401772975922 \tdiscriminator_loss: 1.2533105611801147 \tgenerator_loss: 0.8163235187530518\n",
            "For Step: 38690 recon_loss: 0.17966428399085999 \tdiscriminator_loss: 1.2367630004882812 \tgenerator_loss: 0.8367772102355957\n",
            "For Step: 38700 recon_loss: 0.16173134744167328 \tdiscriminator_loss: 1.305034875869751 \tgenerator_loss: 0.8279249668121338\n",
            "For Step: 38710 recon_loss: 0.1695765107870102 \tdiscriminator_loss: 1.2875341176986694 \tgenerator_loss: 0.8839290738105774\n",
            "For Step: 38720 recon_loss: 0.17223070561885834 \tdiscriminator_loss: 1.266906499862671 \tgenerator_loss: 0.876660168170929\n",
            "For Step: 38730 recon_loss: 0.1815410554409027 \tdiscriminator_loss: 1.362516164779663 \tgenerator_loss: 0.8098079562187195\n",
            "For Step: 38740 recon_loss: 0.16822069883346558 \tdiscriminator_loss: 1.3102500438690186 \tgenerator_loss: 0.8269306421279907\n",
            "For Step: 38750 recon_loss: 0.17291279137134552 \tdiscriminator_loss: 1.245879888534546 \tgenerator_loss: 0.8590049743652344\n",
            "For Step: 38760 recon_loss: 0.16866518557071686 \tdiscriminator_loss: 1.2300992012023926 \tgenerator_loss: 0.891148567199707\n",
            "For Step: 38770 recon_loss: 0.1647988110780716 \tdiscriminator_loss: 1.2870105504989624 \tgenerator_loss: 0.8763861060142517\n",
            "For Step: 38780 recon_loss: 0.16655635833740234 \tdiscriminator_loss: 1.3410364389419556 \tgenerator_loss: 0.8289262652397156\n",
            "For Step: 38790 recon_loss: 0.1810988187789917 \tdiscriminator_loss: 1.2456568479537964 \tgenerator_loss: 0.8878549933433533\n",
            "For Step: 38800 recon_loss: 0.1692928522825241 \tdiscriminator_loss: 1.2602477073669434 \tgenerator_loss: 0.8235172629356384\n",
            "For Step: 38810 recon_loss: 0.16903641819953918 \tdiscriminator_loss: 1.2939131259918213 \tgenerator_loss: 0.8363189697265625\n",
            "For Step: 38820 recon_loss: 0.16224755346775055 \tdiscriminator_loss: 1.221630334854126 \tgenerator_loss: 0.8229148387908936\n",
            "For Step: 38830 recon_loss: 0.16764037311077118 \tdiscriminator_loss: 1.2918622493743896 \tgenerator_loss: 0.8350825309753418\n",
            "For Step: 38840 recon_loss: 0.18577735126018524 \tdiscriminator_loss: 1.3212648630142212 \tgenerator_loss: 0.8319316506385803\n",
            "For Step: 38850 recon_loss: 0.16953709721565247 \tdiscriminator_loss: 1.3044334650039673 \tgenerator_loss: 0.8421396017074585\n",
            "For Step: 38860 recon_loss: 0.1744755506515503 \tdiscriminator_loss: 1.2920856475830078 \tgenerator_loss: 0.8016278743743896\n",
            "For Step: 38870 recon_loss: 0.17432668805122375 \tdiscriminator_loss: 1.3208870887756348 \tgenerator_loss: 0.8418790102005005\n",
            "For Step: 38880 recon_loss: 0.18774579465389252 \tdiscriminator_loss: 1.2222380638122559 \tgenerator_loss: 0.8676621913909912\n",
            "For Step: 38890 recon_loss: 0.17316356301307678 \tdiscriminator_loss: 1.4012457132339478 \tgenerator_loss: 0.8426740169525146\n",
            "For Step: 38900 recon_loss: 0.16375942528247833 \tdiscriminator_loss: 1.3001384735107422 \tgenerator_loss: 0.8251607418060303\n",
            "For Step: 38910 recon_loss: 0.17517508566379547 \tdiscriminator_loss: 1.256218671798706 \tgenerator_loss: 0.8433024287223816\n",
            "For Step: 38920 recon_loss: 0.17364685237407684 \tdiscriminator_loss: 1.3210041522979736 \tgenerator_loss: 0.7853456735610962\n",
            "For Step: 38930 recon_loss: 0.17504800856113434 \tdiscriminator_loss: 1.3383140563964844 \tgenerator_loss: 0.8802763223648071\n",
            "For Step: 38940 recon_loss: 0.18384486436843872 \tdiscriminator_loss: 1.3387463092803955 \tgenerator_loss: 0.8333396911621094\n",
            "For Step: 38950 recon_loss: 0.1686743199825287 \tdiscriminator_loss: 1.2712427377700806 \tgenerator_loss: 0.8353696465492249\n",
            "For Step: 38960 recon_loss: 0.18066862225532532 \tdiscriminator_loss: 1.2802832126617432 \tgenerator_loss: 0.7958204746246338\n",
            "For Step: 38970 recon_loss: 0.15900000929832458 \tdiscriminator_loss: 1.3751617670059204 \tgenerator_loss: 0.8012356758117676\n",
            "For Step: 38980 recon_loss: 0.17846646904945374 \tdiscriminator_loss: 1.2558019161224365 \tgenerator_loss: 0.8836469650268555\n",
            "For Step: 38990 recon_loss: 0.18154393136501312 \tdiscriminator_loss: 1.3315315246582031 \tgenerator_loss: 0.86179119348526\n",
            "For Step: 39000 recon_loss: 0.17450784146785736 \tdiscriminator_loss: 1.25730299949646 \tgenerator_loss: 0.8158260583877563\n",
            "For Step: 39010 recon_loss: 0.17023086547851562 \tdiscriminator_loss: 1.2671705484390259 \tgenerator_loss: 0.8189094066619873\n",
            "For Step: 39020 recon_loss: 0.18730226159095764 \tdiscriminator_loss: 1.2775609493255615 \tgenerator_loss: 0.8003560304641724\n",
            "For Step: 39030 recon_loss: 0.17731496691703796 \tdiscriminator_loss: 1.2510513067245483 \tgenerator_loss: 0.8571664094924927\n",
            "For Step: 39040 recon_loss: 0.17301885783672333 \tdiscriminator_loss: 1.2796987295150757 \tgenerator_loss: 0.863499104976654\n",
            "For Step: 39050 recon_loss: 0.17596034705638885 \tdiscriminator_loss: 1.218277096748352 \tgenerator_loss: 0.8555586934089661\n",
            "For Step: 39060 recon_loss: 0.17538109421730042 \tdiscriminator_loss: 1.2609927654266357 \tgenerator_loss: 0.8143104314804077\n",
            "For Step: 39070 recon_loss: 0.17289668321609497 \tdiscriminator_loss: 1.348465085029602 \tgenerator_loss: 0.8339775204658508\n",
            "For Step: 39080 recon_loss: 0.1804533153772354 \tdiscriminator_loss: 1.2747504711151123 \tgenerator_loss: 0.8513424396514893\n",
            "For Step: 39090 recon_loss: 0.1686997264623642 \tdiscriminator_loss: 1.3097554445266724 \tgenerator_loss: 0.8362792730331421\n",
            "For Step: 39100 recon_loss: 0.17047813534736633 \tdiscriminator_loss: 1.3286385536193848 \tgenerator_loss: 0.8359723687171936\n",
            "For Step: 39110 recon_loss: 0.17663486301898956 \tdiscriminator_loss: 1.2464494705200195 \tgenerator_loss: 0.8433871865272522\n",
            "For Step: 39120 recon_loss: 0.17217783629894257 \tdiscriminator_loss: 1.2879694700241089 \tgenerator_loss: 0.8678016662597656\n",
            "For Step: 39130 recon_loss: 0.16598938405513763 \tdiscriminator_loss: 1.3090252876281738 \tgenerator_loss: 0.8040461540222168\n",
            "For Step: 39140 recon_loss: 0.1672774702310562 \tdiscriminator_loss: 1.2992274761199951 \tgenerator_loss: 0.8165427446365356\n",
            "For Step: 39150 recon_loss: 0.17421290278434753 \tdiscriminator_loss: 1.3024873733520508 \tgenerator_loss: 0.8381991386413574\n",
            "For Step: 39160 recon_loss: 0.16611391305923462 \tdiscriminator_loss: 1.2865251302719116 \tgenerator_loss: 0.8386887907981873\n",
            "For Step: 39170 recon_loss: 0.1644243746995926 \tdiscriminator_loss: 1.357975721359253 \tgenerator_loss: 0.8180869817733765\n",
            "For Step: 39180 recon_loss: 0.1614689826965332 \tdiscriminator_loss: 1.227909803390503 \tgenerator_loss: 0.8560518622398376\n",
            "For Step: 39190 recon_loss: 0.1620558351278305 \tdiscriminator_loss: 1.2866650819778442 \tgenerator_loss: 0.7788317799568176\n",
            "For Step: 39200 recon_loss: 0.17509084939956665 \tdiscriminator_loss: 1.2702984809875488 \tgenerator_loss: 0.8186702728271484\n",
            "For Step: 39210 recon_loss: 0.164977565407753 \tdiscriminator_loss: 1.2620501518249512 \tgenerator_loss: 0.8413195610046387\n",
            "For Step: 39220 recon_loss: 0.1797499805688858 \tdiscriminator_loss: 1.2160999774932861 \tgenerator_loss: 0.8633872866630554\n",
            "For Step: 39230 recon_loss: 0.17137081921100616 \tdiscriminator_loss: 1.3131372928619385 \tgenerator_loss: 0.82181715965271\n",
            "For Step: 39240 recon_loss: 0.15646818280220032 \tdiscriminator_loss: 1.3488792181015015 \tgenerator_loss: 0.7952972054481506\n",
            "For Step: 39250 recon_loss: 0.16481690108776093 \tdiscriminator_loss: 1.3128901720046997 \tgenerator_loss: 0.8232394456863403\n",
            "For Step: 39260 recon_loss: 0.17696279287338257 \tdiscriminator_loss: 1.3527063131332397 \tgenerator_loss: 0.7972252368927002\n",
            "For Step: 39270 recon_loss: 0.17773768305778503 \tdiscriminator_loss: 1.3314578533172607 \tgenerator_loss: 0.8083343505859375\n",
            "For Step: 39280 recon_loss: 0.1757139265537262 \tdiscriminator_loss: 1.3056902885437012 \tgenerator_loss: 0.8504437208175659\n",
            "For Step: 39290 recon_loss: 0.16510838270187378 \tdiscriminator_loss: 1.257946252822876 \tgenerator_loss: 0.8494672179222107\n",
            "For Step: 39300 recon_loss: 0.16830305755138397 \tdiscriminator_loss: 1.3324214220046997 \tgenerator_loss: 0.8014539480209351\n",
            "For Step: 39310 recon_loss: 0.1667693853378296 \tdiscriminator_loss: 1.247937798500061 \tgenerator_loss: 0.8915603160858154\n",
            "For Step: 39320 recon_loss: 0.17361676692962646 \tdiscriminator_loss: 1.2999988794326782 \tgenerator_loss: 0.840363085269928\n",
            "For Step: 39330 recon_loss: 0.1806231141090393 \tdiscriminator_loss: 1.2883516550064087 \tgenerator_loss: 0.8373479843139648\n",
            "For Step: 39340 recon_loss: 0.1825941801071167 \tdiscriminator_loss: 1.2526746988296509 \tgenerator_loss: 0.8163838386535645\n",
            "For Step: 39350 recon_loss: 0.18633021414279938 \tdiscriminator_loss: 1.2585976123809814 \tgenerator_loss: 0.8558231592178345\n",
            "For Step: 39360 recon_loss: 0.17368926107883453 \tdiscriminator_loss: 1.3071378469467163 \tgenerator_loss: 0.7739765644073486\n",
            "For Step: 39370 recon_loss: 0.1796274036169052 \tdiscriminator_loss: 1.2793126106262207 \tgenerator_loss: 0.8083800673484802\n",
            "For Step: 39380 recon_loss: 0.1812213957309723 \tdiscriminator_loss: 1.344323992729187 \tgenerator_loss: 0.8552567362785339\n",
            "For Step: 39390 recon_loss: 0.17430052161216736 \tdiscriminator_loss: 1.341545581817627 \tgenerator_loss: 0.7811469435691833\n",
            "For Step: 39400 recon_loss: 0.17064951360225677 \tdiscriminator_loss: 1.2089385986328125 \tgenerator_loss: 0.8564554452896118\n",
            "For Step: 39410 recon_loss: 0.17289774119853973 \tdiscriminator_loss: 1.247096300125122 \tgenerator_loss: 0.878105103969574\n",
            "For Step: 39420 recon_loss: 0.16196350753307343 \tdiscriminator_loss: 1.30184006690979 \tgenerator_loss: 0.8424139022827148\n",
            "For Step: 39430 recon_loss: 0.16376376152038574 \tdiscriminator_loss: 1.2865194082260132 \tgenerator_loss: 0.8665505051612854\n",
            "For Step: 39440 recon_loss: 0.17199546098709106 \tdiscriminator_loss: 1.303422451019287 \tgenerator_loss: 0.7992607355117798\n",
            "For Step: 39450 recon_loss: 0.18490691483020782 \tdiscriminator_loss: 1.2853420972824097 \tgenerator_loss: 0.8512632846832275\n",
            "For Step: 39460 recon_loss: 0.16855847835540771 \tdiscriminator_loss: 1.244079351425171 \tgenerator_loss: 0.8427177667617798\n",
            "For Step: 39470 recon_loss: 0.1720663160085678 \tdiscriminator_loss: 1.2691643238067627 \tgenerator_loss: 0.7975456714630127\n",
            "For Step: 39480 recon_loss: 0.17608128488063812 \tdiscriminator_loss: 1.2894923686981201 \tgenerator_loss: 0.8256173729896545\n",
            "For Step: 39490 recon_loss: 0.180154949426651 \tdiscriminator_loss: 1.1514421701431274 \tgenerator_loss: 0.8580025434494019\n",
            "For Step: 39500 recon_loss: 0.17370298504829407 \tdiscriminator_loss: 1.2552127838134766 \tgenerator_loss: 0.8054166436195374\n",
            "For Step: 39510 recon_loss: 0.1728419065475464 \tdiscriminator_loss: 1.3492940664291382 \tgenerator_loss: 0.7891994714736938\n",
            "For Step: 39520 recon_loss: 0.17838306725025177 \tdiscriminator_loss: 1.264176368713379 \tgenerator_loss: 0.8643531799316406\n",
            "For Step: 39530 recon_loss: 0.17404498159885406 \tdiscriminator_loss: 1.3295130729675293 \tgenerator_loss: 0.7939697504043579\n",
            "For Step: 39540 recon_loss: 0.16999147832393646 \tdiscriminator_loss: 1.3785021305084229 \tgenerator_loss: 0.8497625589370728\n",
            "For Step: 39550 recon_loss: 0.1820179522037506 \tdiscriminator_loss: 1.2920551300048828 \tgenerator_loss: 0.8624576330184937\n",
            "For Step: 39560 recon_loss: 0.1662672609090805 \tdiscriminator_loss: 1.250187873840332 \tgenerator_loss: 0.9115426540374756\n",
            "For Step: 39570 recon_loss: 0.17752645909786224 \tdiscriminator_loss: 1.3016177415847778 \tgenerator_loss: 0.8550925254821777\n",
            "For Step: 39580 recon_loss: 0.16292256116867065 \tdiscriminator_loss: 1.3143751621246338 \tgenerator_loss: 0.8568711280822754\n",
            "For Step: 39590 recon_loss: 0.17773988842964172 \tdiscriminator_loss: 1.315689206123352 \tgenerator_loss: 0.82057785987854\n",
            "For Step: 39600 recon_loss: 0.18309442698955536 \tdiscriminator_loss: 1.2864958047866821 \tgenerator_loss: 0.8660615086555481\n",
            "For Step: 39610 recon_loss: 0.17715950310230255 \tdiscriminator_loss: 1.2729768753051758 \tgenerator_loss: 0.7983565330505371\n",
            "For Step: 39620 recon_loss: 0.17768768966197968 \tdiscriminator_loss: 1.2674741744995117 \tgenerator_loss: 0.8505660891532898\n",
            "For Step: 39630 recon_loss: 0.18101586401462555 \tdiscriminator_loss: 1.3013856410980225 \tgenerator_loss: 0.8338401317596436\n",
            "For Step: 39640 recon_loss: 0.17505048215389252 \tdiscriminator_loss: 1.2706847190856934 \tgenerator_loss: 0.8609256744384766\n",
            "For Step: 39650 recon_loss: 0.17201443016529083 \tdiscriminator_loss: 1.2192935943603516 \tgenerator_loss: 0.8386009931564331\n",
            "For Step: 39660 recon_loss: 0.17195093631744385 \tdiscriminator_loss: 1.346036434173584 \tgenerator_loss: 0.8951117992401123\n",
            "For Step: 39670 recon_loss: 0.1664675921201706 \tdiscriminator_loss: 1.2336711883544922 \tgenerator_loss: 0.8659003973007202\n",
            "For Step: 39680 recon_loss: 0.17782250046730042 \tdiscriminator_loss: 1.3306739330291748 \tgenerator_loss: 0.7886460423469543\n",
            "For Step: 39690 recon_loss: 0.18043844401836395 \tdiscriminator_loss: 1.2810499668121338 \tgenerator_loss: 0.8348782062530518\n",
            "For Step: 39700 recon_loss: 0.1790810376405716 \tdiscriminator_loss: 1.308184027671814 \tgenerator_loss: 0.8189519643783569\n",
            "For Step: 39710 recon_loss: 0.1909269541501999 \tdiscriminator_loss: 1.3525279760360718 \tgenerator_loss: 0.814401388168335\n",
            "For Step: 39720 recon_loss: 0.17694811522960663 \tdiscriminator_loss: 1.2321513891220093 \tgenerator_loss: 0.8623842000961304\n",
            "For Step: 39730 recon_loss: 0.16875913739204407 \tdiscriminator_loss: 1.2642807960510254 \tgenerator_loss: 0.8284776210784912\n",
            "For Step: 39740 recon_loss: 0.18782711029052734 \tdiscriminator_loss: 1.2216747999191284 \tgenerator_loss: 0.817004919052124\n",
            "For Step: 39750 recon_loss: 0.16823206841945648 \tdiscriminator_loss: 1.2547883987426758 \tgenerator_loss: 0.8307104110717773\n",
            "For Step: 39760 recon_loss: 0.1650821715593338 \tdiscriminator_loss: 1.2772873640060425 \tgenerator_loss: 0.8488755226135254\n",
            "For Step: 39770 recon_loss: 0.17115160822868347 \tdiscriminator_loss: 1.1833574771881104 \tgenerator_loss: 0.9060677886009216\n",
            "For Step: 39780 recon_loss: 0.18127577006816864 \tdiscriminator_loss: 1.2413268089294434 \tgenerator_loss: 0.8650680780410767\n",
            "For Step: 39790 recon_loss: 0.17655061185359955 \tdiscriminator_loss: 1.3026878833770752 \tgenerator_loss: 0.8237848281860352\n",
            "For Step: 39800 recon_loss: 0.16876322031021118 \tdiscriminator_loss: 1.3300087451934814 \tgenerator_loss: 0.7984775304794312\n",
            "For Step: 39810 recon_loss: 0.16908441483974457 \tdiscriminator_loss: 1.2850301265716553 \tgenerator_loss: 0.8305550813674927\n",
            "For Step: 39820 recon_loss: 0.17436674237251282 \tdiscriminator_loss: 1.243476390838623 \tgenerator_loss: 0.8392068147659302\n",
            "For Step: 39830 recon_loss: 0.18802712857723236 \tdiscriminator_loss: 1.2452361583709717 \tgenerator_loss: 0.8599948883056641\n",
            "For Step: 39840 recon_loss: 0.17115794122219086 \tdiscriminator_loss: 1.2338483333587646 \tgenerator_loss: 0.8588302731513977\n",
            "For Step: 39850 recon_loss: 0.16981658339500427 \tdiscriminator_loss: 1.288088321685791 \tgenerator_loss: 0.8257596492767334\n",
            "For Step: 39860 recon_loss: 0.16829806566238403 \tdiscriminator_loss: 1.3492484092712402 \tgenerator_loss: 0.7978312969207764\n",
            "For Step: 39870 recon_loss: 0.175955131649971 \tdiscriminator_loss: 1.2584283351898193 \tgenerator_loss: 0.8653331995010376\n",
            "For Step: 39880 recon_loss: 0.15835490822792053 \tdiscriminator_loss: 1.2563133239746094 \tgenerator_loss: 0.903965413570404\n",
            "For Step: 39890 recon_loss: 0.17550599575042725 \tdiscriminator_loss: 1.3130964040756226 \tgenerator_loss: 0.8417085409164429\n",
            "For Step: 39900 recon_loss: 0.1747056245803833 \tdiscriminator_loss: 1.1988756656646729 \tgenerator_loss: 0.8627914190292358\n",
            "For Step: 39910 recon_loss: 0.17223362624645233 \tdiscriminator_loss: 1.2682814598083496 \tgenerator_loss: 0.8264859914779663\n",
            "For Step: 39920 recon_loss: 0.1791904866695404 \tdiscriminator_loss: 1.302623987197876 \tgenerator_loss: 0.7882876992225647\n",
            "For Step: 39930 recon_loss: 0.17653387784957886 \tdiscriminator_loss: 1.3324947357177734 \tgenerator_loss: 0.8211976289749146\n",
            "For Step: 39940 recon_loss: 0.16756585240364075 \tdiscriminator_loss: 1.2742900848388672 \tgenerator_loss: 0.8179079294204712\n",
            "For Step: 39950 recon_loss: 0.16118499636650085 \tdiscriminator_loss: 1.2696646451950073 \tgenerator_loss: 0.8007522821426392\n",
            "For Step: 39960 recon_loss: 0.17824484407901764 \tdiscriminator_loss: 1.3193624019622803 \tgenerator_loss: 0.8632376194000244\n",
            "For Step: 39970 recon_loss: 0.1630881428718567 \tdiscriminator_loss: 1.3012197017669678 \tgenerator_loss: 0.8130220174789429\n",
            "For Step: 39980 recon_loss: 0.16886448860168457 \tdiscriminator_loss: 1.2694061994552612 \tgenerator_loss: 0.8204083442687988\n",
            "For Step: 39990 recon_loss: 0.16853493452072144 \tdiscriminator_loss: 1.2102625370025635 \tgenerator_loss: 0.8378438949584961\n",
            "For Step: 40000 recon_loss: 0.1715613156557083 \tdiscriminator_loss: 1.2941375970840454 \tgenerator_loss: 0.8322176933288574\n",
            "For Step: 40010 recon_loss: 0.1663060486316681 \tdiscriminator_loss: 1.3287436962127686 \tgenerator_loss: 0.8018035888671875\n",
            "For Step: 40020 recon_loss: 0.16997471451759338 \tdiscriminator_loss: 1.3098931312561035 \tgenerator_loss: 0.8002136945724487\n",
            "For Step: 40030 recon_loss: 0.18640530109405518 \tdiscriminator_loss: 1.3120026588439941 \tgenerator_loss: 0.8159819841384888\n",
            "For Step: 40040 recon_loss: 0.16843710839748383 \tdiscriminator_loss: 1.2973501682281494 \tgenerator_loss: 0.8020825386047363\n",
            "For Step: 40050 recon_loss: 0.18584893643856049 \tdiscriminator_loss: 1.262704610824585 \tgenerator_loss: 0.827433705329895\n",
            "For Step: 40060 recon_loss: 0.1781007945537567 \tdiscriminator_loss: 1.130157470703125 \tgenerator_loss: 0.8739870190620422\n",
            "For Step: 40070 recon_loss: 0.16160112619400024 \tdiscriminator_loss: 1.373673677444458 \tgenerator_loss: 0.8364171981811523\n",
            "For Step: 40080 recon_loss: 0.18255938589572906 \tdiscriminator_loss: 1.3090153932571411 \tgenerator_loss: 0.8256237506866455\n",
            "For Step: 40090 recon_loss: 0.15677882730960846 \tdiscriminator_loss: 1.3174971342086792 \tgenerator_loss: 0.8427475690841675\n",
            "For Step: 40100 recon_loss: 0.18341064453125 \tdiscriminator_loss: 1.283292293548584 \tgenerator_loss: 0.8553423881530762\n",
            "For Step: 40110 recon_loss: 0.1680317521095276 \tdiscriminator_loss: 1.3725069761276245 \tgenerator_loss: 0.8048437833786011\n",
            "For Step: 40120 recon_loss: 0.1839234083890915 \tdiscriminator_loss: 1.3176199197769165 \tgenerator_loss: 0.8202494382858276\n",
            "For Step: 40130 recon_loss: 0.16822032630443573 \tdiscriminator_loss: 1.2574445009231567 \tgenerator_loss: 0.8301721811294556\n",
            "For Step: 40140 recon_loss: 0.17082665860652924 \tdiscriminator_loss: 1.3381379842758179 \tgenerator_loss: 0.807178258895874\n",
            "For Step: 40150 recon_loss: 0.17248904705047607 \tdiscriminator_loss: 1.269402027130127 \tgenerator_loss: 0.8829982876777649\n",
            "For Step: 40160 recon_loss: 0.1781618595123291 \tdiscriminator_loss: 1.3078022003173828 \tgenerator_loss: 0.8373970985412598\n",
            "For Step: 40170 recon_loss: 0.17553533613681793 \tdiscriminator_loss: 1.2981117963790894 \tgenerator_loss: 0.8646982908248901\n",
            "For Step: 40180 recon_loss: 0.16624462604522705 \tdiscriminator_loss: 1.3237229585647583 \tgenerator_loss: 0.8545243740081787\n",
            "For Step: 40190 recon_loss: 0.17322617769241333 \tdiscriminator_loss: 1.2477208375930786 \tgenerator_loss: 0.8850890398025513\n",
            "For Step: 40200 recon_loss: 0.15369528532028198 \tdiscriminator_loss: 1.3186681270599365 \tgenerator_loss: 0.7565909624099731\n",
            "For Step: 40210 recon_loss: 0.1762990802526474 \tdiscriminator_loss: 1.1874679327011108 \tgenerator_loss: 0.8167879581451416\n",
            "For Step: 40220 recon_loss: 0.18884709477424622 \tdiscriminator_loss: 1.3289800882339478 \tgenerator_loss: 0.7968220710754395\n",
            "For Step: 40230 recon_loss: 0.1626909226179123 \tdiscriminator_loss: 1.280268907546997 \tgenerator_loss: 0.788667619228363\n",
            "For Step: 40240 recon_loss: 0.1571754664182663 \tdiscriminator_loss: 1.2543774843215942 \tgenerator_loss: 0.8386628031730652\n",
            "For Step: 40250 recon_loss: 0.18435116112232208 \tdiscriminator_loss: 1.3324482440948486 \tgenerator_loss: 0.8278299570083618\n",
            "For Step: 40260 recon_loss: 0.1697893887758255 \tdiscriminator_loss: 1.1803964376449585 \tgenerator_loss: 0.8120564222335815\n",
            "For Step: 40270 recon_loss: 0.17851309478282928 \tdiscriminator_loss: 1.3740410804748535 \tgenerator_loss: 0.783255398273468\n",
            "For Step: 40280 recon_loss: 0.17465567588806152 \tdiscriminator_loss: 1.291847825050354 \tgenerator_loss: 0.7930643558502197\n",
            "For Step: 40290 recon_loss: 0.167785182595253 \tdiscriminator_loss: 1.2815196514129639 \tgenerator_loss: 0.8351598978042603\n",
            "For Step: 40300 recon_loss: 0.1753576099872589 \tdiscriminator_loss: 1.32228422164917 \tgenerator_loss: 0.7979685068130493\n",
            "For Step: 40310 recon_loss: 0.16757969558238983 \tdiscriminator_loss: 1.379051685333252 \tgenerator_loss: 0.8266583681106567\n",
            "For Step: 40320 recon_loss: 0.17763876914978027 \tdiscriminator_loss: 1.3592134714126587 \tgenerator_loss: 0.8018638491630554\n",
            "For Step: 40330 recon_loss: 0.16904519498348236 \tdiscriminator_loss: 1.2195148468017578 \tgenerator_loss: 0.846416711807251\n",
            "For Step: 40340 recon_loss: 0.172805517911911 \tdiscriminator_loss: 1.353149175643921 \tgenerator_loss: 0.8126925230026245\n",
            "For Step: 40350 recon_loss: 0.1652795821428299 \tdiscriminator_loss: 1.2962278127670288 \tgenerator_loss: 0.828900933265686\n",
            "For Step: 40360 recon_loss: 0.16796784102916718 \tdiscriminator_loss: 1.2648241519927979 \tgenerator_loss: 0.865879476070404\n",
            "For Step: 40370 recon_loss: 0.17013080418109894 \tdiscriminator_loss: 1.2967404127120972 \tgenerator_loss: 0.8026686906814575\n",
            "For Step: 40380 recon_loss: 0.16642168164253235 \tdiscriminator_loss: 1.284674882888794 \tgenerator_loss: 0.854372501373291\n",
            "For Step: 40390 recon_loss: 0.1733691394329071 \tdiscriminator_loss: 1.2411879301071167 \tgenerator_loss: 0.8495569229125977\n",
            "For Step: 40400 recon_loss: 0.1610250622034073 \tdiscriminator_loss: 1.4077770709991455 \tgenerator_loss: 0.8298285007476807\n",
            "For Step: 40410 recon_loss: 0.16563433408737183 \tdiscriminator_loss: 1.3222339153289795 \tgenerator_loss: 0.831312894821167\n",
            "For Step: 40420 recon_loss: 0.17186139523983002 \tdiscriminator_loss: 1.214784860610962 \tgenerator_loss: 0.8177784085273743\n",
            "For Step: 40430 recon_loss: 0.17513397336006165 \tdiscriminator_loss: 1.388334035873413 \tgenerator_loss: 0.812886118888855\n",
            "For Step: 40440 recon_loss: 0.1840343028306961 \tdiscriminator_loss: 1.406945824623108 \tgenerator_loss: 0.7912519574165344\n",
            "For Step: 40450 recon_loss: 0.17360424995422363 \tdiscriminator_loss: 1.262200117111206 \tgenerator_loss: 0.8494037389755249\n",
            "For Step: 40460 recon_loss: 0.1667543202638626 \tdiscriminator_loss: 1.3625741004943848 \tgenerator_loss: 0.7909817099571228\n",
            "For Step: 40470 recon_loss: 0.17551752924919128 \tdiscriminator_loss: 1.2585246562957764 \tgenerator_loss: 0.8087128400802612\n",
            "For Step: 40480 recon_loss: 0.17073401808738708 \tdiscriminator_loss: 1.3155150413513184 \tgenerator_loss: 0.7950907945632935\n",
            "For Step: 40490 recon_loss: 0.17845329642295837 \tdiscriminator_loss: 1.2764419317245483 \tgenerator_loss: 0.8349557518959045\n",
            "For Step: 40500 recon_loss: 0.1652134656906128 \tdiscriminator_loss: 1.2825281620025635 \tgenerator_loss: 0.8650938272476196\n",
            "For Step: 40510 recon_loss: 0.18069559335708618 \tdiscriminator_loss: 1.2989392280578613 \tgenerator_loss: 0.812053918838501\n",
            "For Step: 40520 recon_loss: 0.1907622516155243 \tdiscriminator_loss: 1.3317773342132568 \tgenerator_loss: 0.7707528471946716\n",
            "For Step: 40530 recon_loss: 0.18424949049949646 \tdiscriminator_loss: 1.2466304302215576 \tgenerator_loss: 0.8306564688682556\n",
            "For Step: 40540 recon_loss: 0.17669151723384857 \tdiscriminator_loss: 1.2004289627075195 \tgenerator_loss: 0.8898879289627075\n",
            "For Step: 40550 recon_loss: 0.18328431248664856 \tdiscriminator_loss: 1.2486478090286255 \tgenerator_loss: 0.8786756992340088\n",
            "For Step: 40560 recon_loss: 0.1718757599592209 \tdiscriminator_loss: 1.3253856897354126 \tgenerator_loss: 0.7921197414398193\n",
            "For Step: 40570 recon_loss: 0.16944624483585358 \tdiscriminator_loss: 1.2673959732055664 \tgenerator_loss: 0.8225838541984558\n",
            "For Step: 40580 recon_loss: 0.16898314654827118 \tdiscriminator_loss: 1.311737298965454 \tgenerator_loss: 0.8286415338516235\n",
            "For Step: 40590 recon_loss: 0.17163148522377014 \tdiscriminator_loss: 1.2586053609848022 \tgenerator_loss: 0.8303492069244385\n",
            "For Step: 40600 recon_loss: 0.1692543625831604 \tdiscriminator_loss: 1.2296569347381592 \tgenerator_loss: 0.8689297437667847\n",
            "For Step: 40610 recon_loss: 0.16382558643817902 \tdiscriminator_loss: 1.3286755084991455 \tgenerator_loss: 0.7983114719390869\n",
            "For Step: 40620 recon_loss: 0.17094162106513977 \tdiscriminator_loss: 1.2253460884094238 \tgenerator_loss: 0.8565756678581238\n",
            "For Step: 40630 recon_loss: 0.1674480438232422 \tdiscriminator_loss: 1.2339885234832764 \tgenerator_loss: 0.8667178153991699\n",
            "For Step: 40640 recon_loss: 0.17683374881744385 \tdiscriminator_loss: 1.3272366523742676 \tgenerator_loss: 0.8153051137924194\n",
            "For Step: 40650 recon_loss: 0.17971038818359375 \tdiscriminator_loss: 1.328865647315979 \tgenerator_loss: 0.853718638420105\n",
            "For Step: 40660 recon_loss: 0.1740453690290451 \tdiscriminator_loss: 1.3574209213256836 \tgenerator_loss: 0.7617192268371582\n",
            "For Step: 40670 recon_loss: 0.19019974768161774 \tdiscriminator_loss: 1.2863291501998901 \tgenerator_loss: 0.8466532826423645\n",
            "For Step: 40680 recon_loss: 0.166229248046875 \tdiscriminator_loss: 1.2469847202301025 \tgenerator_loss: 0.9107214212417603\n",
            "For Step: 40690 recon_loss: 0.18526268005371094 \tdiscriminator_loss: 1.265602707862854 \tgenerator_loss: 0.7829734683036804\n",
            "For Step: 40700 recon_loss: 0.18229177594184875 \tdiscriminator_loss: 1.2337521314620972 \tgenerator_loss: 0.8121789693832397\n",
            "For Step: 40710 recon_loss: 0.17078334093093872 \tdiscriminator_loss: 1.3582102060317993 \tgenerator_loss: 0.7791567444801331\n",
            "For Step: 40720 recon_loss: 0.16925665736198425 \tdiscriminator_loss: 1.2477765083312988 \tgenerator_loss: 0.825770914554596\n",
            "For Step: 40730 recon_loss: 0.17060424387454987 \tdiscriminator_loss: 1.2580955028533936 \tgenerator_loss: 0.8200305104255676\n",
            "For Step: 40740 recon_loss: 0.17105673253536224 \tdiscriminator_loss: 1.2140721082687378 \tgenerator_loss: 0.844200849533081\n",
            "For Step: 40750 recon_loss: 0.1791815310716629 \tdiscriminator_loss: 1.3305368423461914 \tgenerator_loss: 0.8296184539794922\n",
            "For Step: 40760 recon_loss: 0.16341082751750946 \tdiscriminator_loss: 1.2260205745697021 \tgenerator_loss: 0.8445099592208862\n",
            "For Step: 40770 recon_loss: 0.1846434473991394 \tdiscriminator_loss: 1.3629478216171265 \tgenerator_loss: 0.8286829590797424\n",
            "For Step: 40780 recon_loss: 0.17776788771152496 \tdiscriminator_loss: 1.3013641834259033 \tgenerator_loss: 0.810188889503479\n",
            "For Step: 40790 recon_loss: 0.17567549645900726 \tdiscriminator_loss: 1.2974971532821655 \tgenerator_loss: 0.8424245715141296\n",
            "For Step: 40800 recon_loss: 0.1756228655576706 \tdiscriminator_loss: 1.2276740074157715 \tgenerator_loss: 0.8617007732391357\n",
            "For Step: 40810 recon_loss: 0.17414554953575134 \tdiscriminator_loss: 1.383774757385254 \tgenerator_loss: 0.7739417552947998\n",
            "For Step: 40820 recon_loss: 0.16660277545452118 \tdiscriminator_loss: 1.3066413402557373 \tgenerator_loss: 0.7855561971664429\n",
            "For Step: 40830 recon_loss: 0.17639607191085815 \tdiscriminator_loss: 1.2241710424423218 \tgenerator_loss: 0.8053625226020813\n",
            "For Step: 40840 recon_loss: 0.18281948566436768 \tdiscriminator_loss: 1.2825899124145508 \tgenerator_loss: 0.8423117399215698\n",
            "For Step: 40850 recon_loss: 0.16662052273750305 \tdiscriminator_loss: 1.2653512954711914 \tgenerator_loss: 0.8084743022918701\n",
            "For Step: 40860 recon_loss: 0.17779457569122314 \tdiscriminator_loss: 1.3865175247192383 \tgenerator_loss: 0.7846521735191345\n",
            "For Step: 40870 recon_loss: 0.17232181131839752 \tdiscriminator_loss: 1.2562429904937744 \tgenerator_loss: 0.8182452917098999\n",
            "For Step: 40880 recon_loss: 0.1683645397424698 \tdiscriminator_loss: 1.369577407836914 \tgenerator_loss: 0.7505353689193726\n",
            "For Step: 40890 recon_loss: 0.1831917017698288 \tdiscriminator_loss: 1.2325493097305298 \tgenerator_loss: 0.8709087371826172\n",
            "For Step: 40900 recon_loss: 0.17581972479820251 \tdiscriminator_loss: 1.3138154745101929 \tgenerator_loss: 0.7993007302284241\n",
            "For Step: 40910 recon_loss: 0.17012493312358856 \tdiscriminator_loss: 1.3488309383392334 \tgenerator_loss: 0.7905565500259399\n",
            "For Step: 40920 recon_loss: 0.1671735793352127 \tdiscriminator_loss: 1.3646165132522583 \tgenerator_loss: 0.7665308117866516\n",
            "For Step: 40930 recon_loss: 0.1677793562412262 \tdiscriminator_loss: 1.305741548538208 \tgenerator_loss: 0.8169641494750977\n",
            "For Step: 40940 recon_loss: 0.17205937206745148 \tdiscriminator_loss: 1.2778968811035156 \tgenerator_loss: 0.8081682324409485\n",
            "For Step: 40950 recon_loss: 0.181417778134346 \tdiscriminator_loss: 1.2485284805297852 \tgenerator_loss: 0.8438026905059814\n",
            "For Step: 40960 recon_loss: 0.1772492229938507 \tdiscriminator_loss: 1.3172918558120728 \tgenerator_loss: 0.8499482274055481\n",
            "For Step: 40970 recon_loss: 0.17692691087722778 \tdiscriminator_loss: 1.2381539344787598 \tgenerator_loss: 0.838606595993042\n",
            "For Step: 40980 recon_loss: 0.16639256477355957 \tdiscriminator_loss: 1.296766757965088 \tgenerator_loss: 0.8136008977890015\n",
            "For Step: 40990 recon_loss: 0.1854812502861023 \tdiscriminator_loss: 1.3526973724365234 \tgenerator_loss: 0.8171240091323853\n",
            "For Step: 41000 recon_loss: 0.1681826263666153 \tdiscriminator_loss: 1.2384908199310303 \tgenerator_loss: 0.8730752468109131\n",
            "For Step: 41010 recon_loss: 0.18336570262908936 \tdiscriminator_loss: 1.325837254524231 \tgenerator_loss: 0.837527871131897\n",
            "For Step: 41020 recon_loss: 0.1703425496816635 \tdiscriminator_loss: 1.323883295059204 \tgenerator_loss: 0.7960764765739441\n",
            "For Step: 41030 recon_loss: 0.17641448974609375 \tdiscriminator_loss: 1.345583438873291 \tgenerator_loss: 0.7832736968994141\n",
            "For Step: 41040 recon_loss: 0.16784259676933289 \tdiscriminator_loss: 1.1879222393035889 \tgenerator_loss: 0.8499280214309692\n",
            "For Step: 41050 recon_loss: 0.16662421822547913 \tdiscriminator_loss: 1.2548458576202393 \tgenerator_loss: 0.8608632683753967\n",
            "For Step: 41060 recon_loss: 0.17219792306423187 \tdiscriminator_loss: 1.3038609027862549 \tgenerator_loss: 0.8501193523406982\n",
            "For Step: 41070 recon_loss: 0.16396276652812958 \tdiscriminator_loss: 1.2698554992675781 \tgenerator_loss: 0.8367030620574951\n",
            "For Step: 41080 recon_loss: 0.16614803671836853 \tdiscriminator_loss: 1.3725640773773193 \tgenerator_loss: 0.8584916591644287\n",
            "For Step: 41090 recon_loss: 0.17270596325397491 \tdiscriminator_loss: 1.2582862377166748 \tgenerator_loss: 0.8413674831390381\n",
            "For Step: 41100 recon_loss: 0.17846742272377014 \tdiscriminator_loss: 1.2556977272033691 \tgenerator_loss: 0.8107872009277344\n",
            "For Step: 41110 recon_loss: 0.1735152006149292 \tdiscriminator_loss: 1.2537181377410889 \tgenerator_loss: 0.8666713833808899\n",
            "For Step: 41120 recon_loss: 0.17996516823768616 \tdiscriminator_loss: 1.268528938293457 \tgenerator_loss: 0.808136522769928\n",
            "For Step: 41130 recon_loss: 0.1686059981584549 \tdiscriminator_loss: 1.332607388496399 \tgenerator_loss: 0.8350319862365723\n",
            "For Step: 41140 recon_loss: 0.17610785365104675 \tdiscriminator_loss: 1.3419873714447021 \tgenerator_loss: 0.7928316593170166\n",
            "For Step: 41150 recon_loss: 0.18004818260669708 \tdiscriminator_loss: 1.2974951267242432 \tgenerator_loss: 0.8247458934783936\n",
            "For Step: 41160 recon_loss: 0.18280036747455597 \tdiscriminator_loss: 1.3109781742095947 \tgenerator_loss: 0.8256667852401733\n",
            "For Step: 41170 recon_loss: 0.16675980389118195 \tdiscriminator_loss: 1.3295400142669678 \tgenerator_loss: 0.8106740713119507\n",
            "For Step: 41180 recon_loss: 0.16526812314987183 \tdiscriminator_loss: 1.2230433225631714 \tgenerator_loss: 0.8574468493461609\n",
            "For Step: 41190 recon_loss: 0.17145270109176636 \tdiscriminator_loss: 1.284874677658081 \tgenerator_loss: 0.8188980221748352\n",
            "For Step: 41200 recon_loss: 0.17021921277046204 \tdiscriminator_loss: 1.3459584712982178 \tgenerator_loss: 0.8204596042633057\n",
            "For Step: 41210 recon_loss: 0.19093622267246246 \tdiscriminator_loss: 1.354535460472107 \tgenerator_loss: 0.792895495891571\n",
            "For Step: 41220 recon_loss: 0.16163833439350128 \tdiscriminator_loss: 1.3503820896148682 \tgenerator_loss: 0.8409279584884644\n",
            "For Step: 41230 recon_loss: 0.17944394052028656 \tdiscriminator_loss: 1.4183279275894165 \tgenerator_loss: 0.7982456088066101\n",
            "For Step: 41240 recon_loss: 0.16385018825531006 \tdiscriminator_loss: 1.2648324966430664 \tgenerator_loss: 0.8354200720787048\n",
            "For Step: 41250 recon_loss: 0.15695029497146606 \tdiscriminator_loss: 1.3352601528167725 \tgenerator_loss: 0.8409152030944824\n",
            "For Step: 41260 recon_loss: 0.16981779038906097 \tdiscriminator_loss: 1.2140064239501953 \tgenerator_loss: 0.8861593008041382\n",
            "For Step: 41270 recon_loss: 0.16918793320655823 \tdiscriminator_loss: 1.3484859466552734 \tgenerator_loss: 0.8073970079421997\n",
            "For Step: 41280 recon_loss: 0.17636564373970032 \tdiscriminator_loss: 1.306749939918518 \tgenerator_loss: 0.8443057537078857\n",
            "For Step: 41290 recon_loss: 0.17792154848575592 \tdiscriminator_loss: 1.3068008422851562 \tgenerator_loss: 0.8091495633125305\n",
            "For Step: 41300 recon_loss: 0.18301181495189667 \tdiscriminator_loss: 1.2729012966156006 \tgenerator_loss: 0.8316273093223572\n",
            "For Step: 41310 recon_loss: 0.16689619421958923 \tdiscriminator_loss: 1.342987298965454 \tgenerator_loss: 0.8343563079833984\n",
            "For Step: 41320 recon_loss: 0.16984397172927856 \tdiscriminator_loss: 1.2335925102233887 \tgenerator_loss: 0.8220409154891968\n",
            "For Step: 41330 recon_loss: 0.1539449393749237 \tdiscriminator_loss: 1.301133394241333 \tgenerator_loss: 0.8430861234664917\n",
            "For Step: 41340 recon_loss: 0.1532638818025589 \tdiscriminator_loss: 1.3109583854675293 \tgenerator_loss: 0.8270872831344604\n",
            "For Step: 41350 recon_loss: 0.1855880618095398 \tdiscriminator_loss: 1.3250746726989746 \tgenerator_loss: 0.8112642765045166\n",
            "For Step: 41360 recon_loss: 0.1713753342628479 \tdiscriminator_loss: 1.2595216035842896 \tgenerator_loss: 0.8195787668228149\n",
            "For Step: 41370 recon_loss: 0.16776633262634277 \tdiscriminator_loss: 1.3424581289291382 \tgenerator_loss: 0.7785135507583618\n",
            "For Step: 41380 recon_loss: 0.17276743054389954 \tdiscriminator_loss: 1.3714931011199951 \tgenerator_loss: 0.8033199906349182\n",
            "For Step: 41390 recon_loss: 0.1789303570985794 \tdiscriminator_loss: 1.2007776498794556 \tgenerator_loss: 0.8493186235427856\n",
            "For Step: 41400 recon_loss: 0.1501937359571457 \tdiscriminator_loss: 1.3380565643310547 \tgenerator_loss: 0.7580034732818604\n",
            "For Step: 41410 recon_loss: 0.16464769840240479 \tdiscriminator_loss: 1.298261046409607 \tgenerator_loss: 0.8093597888946533\n",
            "For Step: 41420 recon_loss: 0.14967571198940277 \tdiscriminator_loss: 1.2610445022583008 \tgenerator_loss: 0.7689497470855713\n",
            "For Step: 41430 recon_loss: 0.16250993311405182 \tdiscriminator_loss: 1.2970789670944214 \tgenerator_loss: 0.836390495300293\n",
            "For Step: 41440 recon_loss: 0.16242054104804993 \tdiscriminator_loss: 1.3018052577972412 \tgenerator_loss: 0.8593801259994507\n",
            "For Step: 41450 recon_loss: 0.1740448772907257 \tdiscriminator_loss: 1.222558856010437 \tgenerator_loss: 0.8933720588684082\n",
            "For Step: 41460 recon_loss: 0.1664278507232666 \tdiscriminator_loss: 1.250117540359497 \tgenerator_loss: 0.874756932258606\n",
            "For Step: 41470 recon_loss: 0.1583772599697113 \tdiscriminator_loss: 1.175803780555725 \tgenerator_loss: 0.8355934619903564\n",
            "For Step: 41480 recon_loss: 0.1751457303762436 \tdiscriminator_loss: 1.326918363571167 \tgenerator_loss: 0.7685633897781372\n",
            "For Step: 41490 recon_loss: 0.16561411321163177 \tdiscriminator_loss: 1.414511799812317 \tgenerator_loss: 0.7759919762611389\n",
            "For Step: 41500 recon_loss: 0.16991204023361206 \tdiscriminator_loss: 1.3332393169403076 \tgenerator_loss: 0.811285138130188\n",
            "For Step: 41510 recon_loss: 0.1731511652469635 \tdiscriminator_loss: 1.223271131515503 \tgenerator_loss: 0.8281347751617432\n",
            "For Step: 41520 recon_loss: 0.1724984347820282 \tdiscriminator_loss: 1.2758476734161377 \tgenerator_loss: 0.8821287155151367\n",
            "For Step: 41530 recon_loss: 0.18264368176460266 \tdiscriminator_loss: 1.3130112886428833 \tgenerator_loss: 0.8109190464019775\n",
            "For Step: 41540 recon_loss: 0.16963450610637665 \tdiscriminator_loss: 1.3019280433654785 \tgenerator_loss: 0.8077958822250366\n",
            "For Step: 41550 recon_loss: 0.1627335250377655 \tdiscriminator_loss: 1.2885925769805908 \tgenerator_loss: 0.8093484044075012\n",
            "For Step: 41560 recon_loss: 0.16993746161460876 \tdiscriminator_loss: 1.2782297134399414 \tgenerator_loss: 0.8473935723304749\n",
            "For Step: 41570 recon_loss: 0.17489635944366455 \tdiscriminator_loss: 1.2374221086502075 \tgenerator_loss: 0.8066102266311646\n",
            "For Step: 41580 recon_loss: 0.1780041605234146 \tdiscriminator_loss: 1.2401340007781982 \tgenerator_loss: 0.834140419960022\n",
            "For Step: 41590 recon_loss: 0.17037786543369293 \tdiscriminator_loss: 1.2510875463485718 \tgenerator_loss: 0.8241910934448242\n",
            "For Step: 41600 recon_loss: 0.1780569702386856 \tdiscriminator_loss: 1.3470089435577393 \tgenerator_loss: 0.7931808829307556\n",
            "For Step: 41610 recon_loss: 0.1645491123199463 \tdiscriminator_loss: 1.2437058687210083 \tgenerator_loss: 0.8184439539909363\n",
            "For Step: 41620 recon_loss: 0.16508549451828003 \tdiscriminator_loss: 1.3281917572021484 \tgenerator_loss: 0.8172499537467957\n",
            "For Step: 41630 recon_loss: 0.16844378411769867 \tdiscriminator_loss: 1.21662175655365 \tgenerator_loss: 0.8885073065757751\n",
            "For Step: 41640 recon_loss: 0.16988542675971985 \tdiscriminator_loss: 1.3205938339233398 \tgenerator_loss: 0.8284056782722473\n",
            "For Step: 41650 recon_loss: 0.1712195724248886 \tdiscriminator_loss: 1.3256982564926147 \tgenerator_loss: 0.8344264626502991\n",
            "For Step: 41660 recon_loss: 0.16280701756477356 \tdiscriminator_loss: 1.3152769804000854 \tgenerator_loss: 0.8053227663040161\n",
            "For Step: 41670 recon_loss: 0.17867743968963623 \tdiscriminator_loss: 1.204223394393921 \tgenerator_loss: 0.8754628300666809\n",
            "For Step: 41680 recon_loss: 0.17741401493549347 \tdiscriminator_loss: 1.350109577178955 \tgenerator_loss: 0.7726988792419434\n",
            "For Step: 41690 recon_loss: 0.17005832493305206 \tdiscriminator_loss: 1.2632925510406494 \tgenerator_loss: 0.8419890999794006\n",
            "For Step: 41700 recon_loss: 0.17599870264530182 \tdiscriminator_loss: 1.2418431043624878 \tgenerator_loss: 0.8491858243942261\n",
            "For Step: 41710 recon_loss: 0.16320732235908508 \tdiscriminator_loss: 1.2710180282592773 \tgenerator_loss: 0.7859762907028198\n",
            "For Step: 41720 recon_loss: 0.16726747155189514 \tdiscriminator_loss: 1.326839804649353 \tgenerator_loss: 0.7891250252723694\n",
            "For Step: 41730 recon_loss: 0.165830597281456 \tdiscriminator_loss: 1.3150957822799683 \tgenerator_loss: 0.8171951770782471\n",
            "For Step: 41740 recon_loss: 0.17424297332763672 \tdiscriminator_loss: 1.2690861225128174 \tgenerator_loss: 0.8255043029785156\n",
            "For Step: 41750 recon_loss: 0.16979821026325226 \tdiscriminator_loss: 1.294987440109253 \tgenerator_loss: 0.799284040927887\n",
            "For Step: 41760 recon_loss: 0.1615906059741974 \tdiscriminator_loss: 1.2664644718170166 \tgenerator_loss: 0.8070557117462158\n",
            "For Step: 41770 recon_loss: 0.1648002713918686 \tdiscriminator_loss: 1.3434622287750244 \tgenerator_loss: 0.8031009435653687\n",
            "For Step: 41780 recon_loss: 0.16930323839187622 \tdiscriminator_loss: 1.251381754875183 \tgenerator_loss: 0.8944203853607178\n",
            "For Step: 41790 recon_loss: 0.1686033457517624 \tdiscriminator_loss: 1.32249915599823 \tgenerator_loss: 0.797913670539856\n",
            "For Step: 41800 recon_loss: 0.16622363030910492 \tdiscriminator_loss: 1.3963828086853027 \tgenerator_loss: 0.802956223487854\n",
            "For Step: 41810 recon_loss: 0.1655166745185852 \tdiscriminator_loss: 1.3336238861083984 \tgenerator_loss: 0.7977398037910461\n",
            "For Step: 41820 recon_loss: 0.16272804141044617 \tdiscriminator_loss: 1.2502613067626953 \tgenerator_loss: 0.8125182390213013\n",
            "For Step: 41830 recon_loss: 0.17111392319202423 \tdiscriminator_loss: 1.3482153415679932 \tgenerator_loss: 0.8143696784973145\n",
            "For Step: 41840 recon_loss: 0.16143620014190674 \tdiscriminator_loss: 1.2693860530853271 \tgenerator_loss: 0.8207250833511353\n",
            "For Step: 41850 recon_loss: 0.1754940003156662 \tdiscriminator_loss: 1.2775113582611084 \tgenerator_loss: 0.8509442806243896\n",
            "For Step: 41860 recon_loss: 0.17811527848243713 \tdiscriminator_loss: 1.2709324359893799 \tgenerator_loss: 0.8426206111907959\n",
            "For Step: 41870 recon_loss: 0.1764620691537857 \tdiscriminator_loss: 1.2486422061920166 \tgenerator_loss: 0.8032418489456177\n",
            "For Step: 41880 recon_loss: 0.1788170337677002 \tdiscriminator_loss: 1.2818324565887451 \tgenerator_loss: 0.8516745567321777\n",
            "For Step: 41890 recon_loss: 0.1791374236345291 \tdiscriminator_loss: 1.2794983386993408 \tgenerator_loss: 0.804999589920044\n",
            "For Step: 41900 recon_loss: 0.17353324592113495 \tdiscriminator_loss: 1.345857858657837 \tgenerator_loss: 0.811392068862915\n",
            "For Step: 41910 recon_loss: 0.17190492153167725 \tdiscriminator_loss: 1.3173351287841797 \tgenerator_loss: 0.7704163789749146\n",
            "For Step: 41920 recon_loss: 0.16436126828193665 \tdiscriminator_loss: 1.3505024909973145 \tgenerator_loss: 0.7723634243011475\n",
            "For Step: 41930 recon_loss: 0.16692347824573517 \tdiscriminator_loss: 1.2855838537216187 \tgenerator_loss: 0.8159453272819519\n",
            "For Step: 41940 recon_loss: 0.173827663064003 \tdiscriminator_loss: 1.3011192083358765 \tgenerator_loss: 0.7988104224205017\n",
            "For Step: 41950 recon_loss: 0.17379148304462433 \tdiscriminator_loss: 1.3248810768127441 \tgenerator_loss: 0.816182553768158\n",
            "For Step: 41960 recon_loss: 0.17495276033878326 \tdiscriminator_loss: 1.3017247915267944 \tgenerator_loss: 0.8398441076278687\n",
            "For Step: 41970 recon_loss: 0.17016275227069855 \tdiscriminator_loss: 1.3354755640029907 \tgenerator_loss: 0.822899580001831\n",
            "For Step: 41980 recon_loss: 0.16573192179203033 \tdiscriminator_loss: 1.2706804275512695 \tgenerator_loss: 0.8551998734474182\n",
            "For Step: 41990 recon_loss: 0.1837283968925476 \tdiscriminator_loss: 1.2945773601531982 \tgenerator_loss: 0.8462221026420593\n",
            "For Step: 42000 recon_loss: 0.1767013967037201 \tdiscriminator_loss: 1.2702012062072754 \tgenerator_loss: 0.813072919845581\n",
            "For Step: 42010 recon_loss: 0.1751897931098938 \tdiscriminator_loss: 1.2859597206115723 \tgenerator_loss: 0.8212606310844421\n",
            "For Step: 42020 recon_loss: 0.17242276668548584 \tdiscriminator_loss: 1.2740631103515625 \tgenerator_loss: 0.8441152572631836\n",
            "For Step: 42030 recon_loss: 0.16605213284492493 \tdiscriminator_loss: 1.2742482423782349 \tgenerator_loss: 0.8393236398696899\n",
            "For Step: 42040 recon_loss: 0.1774042844772339 \tdiscriminator_loss: 1.267103910446167 \tgenerator_loss: 0.8811739087104797\n",
            "For Step: 42050 recon_loss: 0.16388480365276337 \tdiscriminator_loss: 1.3393945693969727 \tgenerator_loss: 0.7850897312164307\n",
            "For Step: 42060 recon_loss: 0.1610371172428131 \tdiscriminator_loss: 1.3479900360107422 \tgenerator_loss: 0.8609013557434082\n",
            "For Step: 42070 recon_loss: 0.1761409193277359 \tdiscriminator_loss: 1.2429149150848389 \tgenerator_loss: 0.855274498462677\n",
            "For Step: 42080 recon_loss: 0.16923131048679352 \tdiscriminator_loss: 1.3777763843536377 \tgenerator_loss: 0.8253777027130127\n",
            "For Step: 42090 recon_loss: 0.1712908297777176 \tdiscriminator_loss: 1.1579985618591309 \tgenerator_loss: 0.8420145511627197\n",
            "For Step: 42100 recon_loss: 0.17437756061553955 \tdiscriminator_loss: 1.238955020904541 \tgenerator_loss: 0.791000247001648\n",
            "For Step: 42110 recon_loss: 0.18082891404628754 \tdiscriminator_loss: 1.3069981336593628 \tgenerator_loss: 0.8510595560073853\n",
            "For Step: 42120 recon_loss: 0.17202307283878326 \tdiscriminator_loss: 1.2305082082748413 \tgenerator_loss: 0.8358443379402161\n",
            "For Step: 42130 recon_loss: 0.16899797320365906 \tdiscriminator_loss: 1.3318994045257568 \tgenerator_loss: 0.7788655757904053\n",
            "For Step: 42140 recon_loss: 0.16817040741443634 \tdiscriminator_loss: 1.2322494983673096 \tgenerator_loss: 0.8126389980316162\n",
            "For Step: 42150 recon_loss: 0.15771982073783875 \tdiscriminator_loss: 1.257288932800293 \tgenerator_loss: 0.8480724096298218\n",
            "For Step: 42160 recon_loss: 0.17050732672214508 \tdiscriminator_loss: 1.2595021724700928 \tgenerator_loss: 0.8614936470985413\n",
            "For Step: 42170 recon_loss: 0.18449059128761292 \tdiscriminator_loss: 1.3754404783248901 \tgenerator_loss: 0.7993535995483398\n",
            "For Step: 42180 recon_loss: 0.174918994307518 \tdiscriminator_loss: 1.3334373235702515 \tgenerator_loss: 0.8303496837615967\n",
            "For Step: 42190 recon_loss: 0.1655714511871338 \tdiscriminator_loss: 1.3020603656768799 \tgenerator_loss: 0.8133436441421509\n",
            "For Step: 42200 recon_loss: 0.16217973828315735 \tdiscriminator_loss: 1.3274154663085938 \tgenerator_loss: 0.7828764915466309\n",
            "For Step: 42210 recon_loss: 0.16873575747013092 \tdiscriminator_loss: 1.3489022254943848 \tgenerator_loss: 0.7574973106384277\n",
            "For Step: 42220 recon_loss: 0.18304979801177979 \tdiscriminator_loss: 1.3496415615081787 \tgenerator_loss: 0.8120919466018677\n",
            "For Step: 42230 recon_loss: 0.18093374371528625 \tdiscriminator_loss: 1.252089023590088 \tgenerator_loss: 0.8248134255409241\n",
            "For Step: 42240 recon_loss: 0.17087610065937042 \tdiscriminator_loss: 1.2266732454299927 \tgenerator_loss: 0.8412820100784302\n",
            "For Step: 42250 recon_loss: 0.17610153555870056 \tdiscriminator_loss: 1.3112776279449463 \tgenerator_loss: 0.7954568862915039\n",
            "For Step: 42260 recon_loss: 0.1746784746646881 \tdiscriminator_loss: 1.3467843532562256 \tgenerator_loss: 0.8211886882781982\n",
            "For Step: 42270 recon_loss: 0.17745037376880646 \tdiscriminator_loss: 1.3413571119308472 \tgenerator_loss: 0.8595771193504333\n",
            "For Step: 42280 recon_loss: 0.17018508911132812 \tdiscriminator_loss: 1.364748477935791 \tgenerator_loss: 0.8220474720001221\n",
            "For Step: 42290 recon_loss: 0.17319871485233307 \tdiscriminator_loss: 1.268863320350647 \tgenerator_loss: 0.8424180746078491\n",
            "For Step: 42300 recon_loss: 0.1650187373161316 \tdiscriminator_loss: 1.2555654048919678 \tgenerator_loss: 0.8590114116668701\n",
            "For Step: 42310 recon_loss: 0.17933930456638336 \tdiscriminator_loss: 1.3479502201080322 \tgenerator_loss: 0.8411886692047119\n",
            "For Step: 42320 recon_loss: 0.1586080640554428 \tdiscriminator_loss: 1.3693528175354004 \tgenerator_loss: 0.8103516101837158\n",
            "For Step: 42330 recon_loss: 0.16602014005184174 \tdiscriminator_loss: 1.2641106843948364 \tgenerator_loss: 0.8264753222465515\n",
            "For Step: 42340 recon_loss: 0.18610377609729767 \tdiscriminator_loss: 1.367126703262329 \tgenerator_loss: 0.7870407700538635\n",
            "For Step: 42350 recon_loss: 0.17810377478599548 \tdiscriminator_loss: 1.27840256690979 \tgenerator_loss: 0.8140846490859985\n",
            "For Step: 42360 recon_loss: 0.1602722704410553 \tdiscriminator_loss: 1.2505731582641602 \tgenerator_loss: 0.83067387342453\n",
            "For Step: 42370 recon_loss: 0.18527932465076447 \tdiscriminator_loss: 1.2158386707305908 \tgenerator_loss: 0.8556905388832092\n",
            "For Step: 42380 recon_loss: 0.16978828608989716 \tdiscriminator_loss: 1.2760841846466064 \tgenerator_loss: 0.8577889800071716\n",
            "For Step: 42390 recon_loss: 0.17432093620300293 \tdiscriminator_loss: 1.4001293182373047 \tgenerator_loss: 0.7451956272125244\n",
            "For Step: 42400 recon_loss: 0.16642364859580994 \tdiscriminator_loss: 1.2983436584472656 \tgenerator_loss: 0.8040787577629089\n",
            "For Step: 42410 recon_loss: 0.17765823006629944 \tdiscriminator_loss: 1.330364465713501 \tgenerator_loss: 0.8467170000076294\n",
            "For Step: 42420 recon_loss: 0.1548299342393875 \tdiscriminator_loss: 1.3211439847946167 \tgenerator_loss: 0.7595657110214233\n",
            "For Step: 42430 recon_loss: 0.17839108407497406 \tdiscriminator_loss: 1.308293342590332 \tgenerator_loss: 0.8126375675201416\n",
            "For Step: 42440 recon_loss: 0.1772659420967102 \tdiscriminator_loss: 1.296087384223938 \tgenerator_loss: 0.8132567405700684\n",
            "For Step: 42450 recon_loss: 0.17020933330059052 \tdiscriminator_loss: 1.3249038457870483 \tgenerator_loss: 0.8067604899406433\n",
            "For Step: 42460 recon_loss: 0.16605760157108307 \tdiscriminator_loss: 1.4200427532196045 \tgenerator_loss: 0.8002475500106812\n",
            "For Step: 42470 recon_loss: 0.16879069805145264 \tdiscriminator_loss: 1.3227286338806152 \tgenerator_loss: 0.7967146635055542\n",
            "For Step: 42480 recon_loss: 0.1678863763809204 \tdiscriminator_loss: 1.3079979419708252 \tgenerator_loss: 0.8050202131271362\n",
            "For Step: 42490 recon_loss: 0.16797497868537903 \tdiscriminator_loss: 1.3400943279266357 \tgenerator_loss: 0.8219647407531738\n",
            "For Step: 42500 recon_loss: 0.18808972835540771 \tdiscriminator_loss: 1.291235089302063 \tgenerator_loss: 0.853028416633606\n",
            "For Step: 42510 recon_loss: 0.16898015141487122 \tdiscriminator_loss: 1.3332927227020264 \tgenerator_loss: 0.7820907235145569\n",
            "For Step: 42520 recon_loss: 0.1867278814315796 \tdiscriminator_loss: 1.3133893013000488 \tgenerator_loss: 0.7501670718193054\n",
            "For Step: 42530 recon_loss: 0.16481047868728638 \tdiscriminator_loss: 1.3884541988372803 \tgenerator_loss: 0.7987973093986511\n",
            "For Step: 42540 recon_loss: 0.18017809092998505 \tdiscriminator_loss: 1.3058528900146484 \tgenerator_loss: 0.8356097936630249\n",
            "For Step: 42550 recon_loss: 0.16139991581439972 \tdiscriminator_loss: 1.3167402744293213 \tgenerator_loss: 0.7778023481369019\n",
            "For Step: 42560 recon_loss: 0.17802631855010986 \tdiscriminator_loss: 1.303415060043335 \tgenerator_loss: 0.7754528522491455\n",
            "For Step: 42570 recon_loss: 0.17868046462535858 \tdiscriminator_loss: 1.2772256135940552 \tgenerator_loss: 0.8387313485145569\n",
            "For Step: 42580 recon_loss: 0.17820078134536743 \tdiscriminator_loss: 1.3109524250030518 \tgenerator_loss: 0.8238692283630371\n",
            "For Step: 42590 recon_loss: 0.18120329082012177 \tdiscriminator_loss: 1.3076331615447998 \tgenerator_loss: 0.8504931926727295\n",
            "For Step: 42600 recon_loss: 0.16923892498016357 \tdiscriminator_loss: 1.315497875213623 \tgenerator_loss: 0.8238221406936646\n",
            "For Step: 42610 recon_loss: 0.16842830181121826 \tdiscriminator_loss: 1.3424708843231201 \tgenerator_loss: 0.7548204660415649\n",
            "For Step: 42620 recon_loss: 0.17323176562786102 \tdiscriminator_loss: 1.2672410011291504 \tgenerator_loss: 0.8437179923057556\n",
            "For Step: 42630 recon_loss: 0.17061229050159454 \tdiscriminator_loss: 1.2920336723327637 \tgenerator_loss: 0.8256655335426331\n",
            "For Step: 42640 recon_loss: 0.1598128229379654 \tdiscriminator_loss: 1.2921195030212402 \tgenerator_loss: 0.8341830968856812\n",
            "For Step: 42650 recon_loss: 0.17820394039154053 \tdiscriminator_loss: 1.3021456003189087 \tgenerator_loss: 0.857690155506134\n",
            "For Step: 42660 recon_loss: 0.16898256540298462 \tdiscriminator_loss: 1.3663746118545532 \tgenerator_loss: 0.7857248187065125\n",
            "For Step: 42670 recon_loss: 0.16272085905075073 \tdiscriminator_loss: 1.218845009803772 \tgenerator_loss: 0.8420661687850952\n",
            "For Step: 42680 recon_loss: 0.17834076285362244 \tdiscriminator_loss: 1.3156733512878418 \tgenerator_loss: 0.8528225421905518\n",
            "For Step: 42690 recon_loss: 0.15738150477409363 \tdiscriminator_loss: 1.2725214958190918 \tgenerator_loss: 0.8030350208282471\n",
            "For Step: 42700 recon_loss: 0.17066434025764465 \tdiscriminator_loss: 1.3654732704162598 \tgenerator_loss: 0.7681386470794678\n",
            "For Step: 42710 recon_loss: 0.1718851923942566 \tdiscriminator_loss: 1.2929258346557617 \tgenerator_loss: 0.8010486960411072\n",
            "For Step: 42720 recon_loss: 0.174927219748497 \tdiscriminator_loss: 1.3820569515228271 \tgenerator_loss: 0.8052006363868713\n",
            "For Step: 42730 recon_loss: 0.1859731674194336 \tdiscriminator_loss: 1.3061603307724 \tgenerator_loss: 0.8498095273971558\n",
            "For Step: 42740 recon_loss: 0.17219772934913635 \tdiscriminator_loss: 1.2921288013458252 \tgenerator_loss: 0.8466898202896118\n",
            "For Step: 42750 recon_loss: 0.16754652559757233 \tdiscriminator_loss: 1.3114982843399048 \tgenerator_loss: 0.7970016002655029\n",
            "For Step: 42760 recon_loss: 0.16744840145111084 \tdiscriminator_loss: 1.310634732246399 \tgenerator_loss: 0.8286408185958862\n",
            "For Step: 42770 recon_loss: 0.1785588562488556 \tdiscriminator_loss: 1.2894268035888672 \tgenerator_loss: 0.7702579498291016\n",
            "For Step: 42780 recon_loss: 0.18166518211364746 \tdiscriminator_loss: 1.3168253898620605 \tgenerator_loss: 0.8190714120864868\n",
            "For Step: 42790 recon_loss: 0.17148679494857788 \tdiscriminator_loss: 1.3410608768463135 \tgenerator_loss: 0.8037562370300293\n",
            "For Step: 42800 recon_loss: 0.17072850465774536 \tdiscriminator_loss: 1.2373719215393066 \tgenerator_loss: 0.7894994020462036\n",
            "For Step: 42810 recon_loss: 0.1771049052476883 \tdiscriminator_loss: 1.3046990633010864 \tgenerator_loss: 0.8237144947052002\n",
            "For Step: 42820 recon_loss: 0.17169906198978424 \tdiscriminator_loss: 1.2808761596679688 \tgenerator_loss: 0.8225266337394714\n",
            "For Step: 42830 recon_loss: 0.17567330598831177 \tdiscriminator_loss: 1.2955089807510376 \tgenerator_loss: 0.8416078090667725\n",
            "For Step: 42840 recon_loss: 0.17581158876419067 \tdiscriminator_loss: 1.3451499938964844 \tgenerator_loss: 0.7714328765869141\n",
            "For Step: 42850 recon_loss: 0.1629987508058548 \tdiscriminator_loss: 1.2988579273223877 \tgenerator_loss: 0.8039935827255249\n",
            "For Step: 42860 recon_loss: 0.16450218856334686 \tdiscriminator_loss: 1.2811652421951294 \tgenerator_loss: 0.8565278649330139\n",
            "For Step: 42870 recon_loss: 0.1694892793893814 \tdiscriminator_loss: 1.2727783918380737 \tgenerator_loss: 0.8367887735366821\n",
            "For Step: 42880 recon_loss: 0.1773422807455063 \tdiscriminator_loss: 1.308988332748413 \tgenerator_loss: 0.7976021766662598\n",
            "For Step: 42890 recon_loss: 0.16102394461631775 \tdiscriminator_loss: 1.315685510635376 \tgenerator_loss: 0.7761438488960266\n",
            "For Step: 42900 recon_loss: 0.17619964480400085 \tdiscriminator_loss: 1.2526938915252686 \tgenerator_loss: 0.8164547681808472\n",
            "For Step: 42910 recon_loss: 0.16576384007930756 \tdiscriminator_loss: 1.2421873807907104 \tgenerator_loss: 0.7796988487243652\n",
            "For Step: 42920 recon_loss: 0.17632713913917542 \tdiscriminator_loss: 1.3944449424743652 \tgenerator_loss: 0.7939420938491821\n",
            "For Step: 42930 recon_loss: 0.18056896328926086 \tdiscriminator_loss: 1.2624613046646118 \tgenerator_loss: 0.8617058992385864\n",
            "For Step: 42940 recon_loss: 0.17973357439041138 \tdiscriminator_loss: 1.2892000675201416 \tgenerator_loss: 0.8322855234146118\n",
            "For Step: 42950 recon_loss: 0.1784772276878357 \tdiscriminator_loss: 1.313474416732788 \tgenerator_loss: 0.798429012298584\n",
            "For Step: 42960 recon_loss: 0.1731281876564026 \tdiscriminator_loss: 1.346245288848877 \tgenerator_loss: 0.783352255821228\n",
            "For Step: 42970 recon_loss: 0.17265409231185913 \tdiscriminator_loss: 1.2926414012908936 \tgenerator_loss: 0.8290554285049438\n",
            "For Step: 42980 recon_loss: 0.16601251065731049 \tdiscriminator_loss: 1.3713352680206299 \tgenerator_loss: 0.8357657194137573\n",
            "For Step: 42990 recon_loss: 0.16750623285770416 \tdiscriminator_loss: 1.3733552694320679 \tgenerator_loss: 0.8037089705467224\n",
            "For Step: 43000 recon_loss: 0.1644810289144516 \tdiscriminator_loss: 1.3664664030075073 \tgenerator_loss: 0.8349988460540771\n",
            "For Step: 43010 recon_loss: 0.16170567274093628 \tdiscriminator_loss: 1.2947630882263184 \tgenerator_loss: 0.8050121665000916\n",
            "For Step: 43020 recon_loss: 0.1744140088558197 \tdiscriminator_loss: 1.2220790386199951 \tgenerator_loss: 0.8135113716125488\n",
            "For Step: 43030 recon_loss: 0.17988279461860657 \tdiscriminator_loss: 1.318122386932373 \tgenerator_loss: 0.8184463977813721\n",
            "For Step: 43040 recon_loss: 0.17183418571949005 \tdiscriminator_loss: 1.2812271118164062 \tgenerator_loss: 0.8261213898658752\n",
            "For Step: 43050 recon_loss: 0.17005527019500732 \tdiscriminator_loss: 1.3032736778259277 \tgenerator_loss: 0.8010879755020142\n",
            "For Step: 43060 recon_loss: 0.182321697473526 \tdiscriminator_loss: 1.2798073291778564 \tgenerator_loss: 0.837878942489624\n",
            "For Step: 43070 recon_loss: 0.14999981224536896 \tdiscriminator_loss: 1.3029048442840576 \tgenerator_loss: 0.7832762002944946\n",
            "For Step: 43080 recon_loss: 0.1813071072101593 \tdiscriminator_loss: 1.3088765144348145 \tgenerator_loss: 0.7951464056968689\n",
            "For Step: 43090 recon_loss: 0.17043179273605347 \tdiscriminator_loss: 1.1456252336502075 \tgenerator_loss: 0.8833295106887817\n",
            "For Step: 43100 recon_loss: 0.16698938608169556 \tdiscriminator_loss: 1.235413670539856 \tgenerator_loss: 0.8398346304893494\n",
            "For Step: 43110 recon_loss: 0.15977033972740173 \tdiscriminator_loss: 1.2781882286071777 \tgenerator_loss: 0.8046735525131226\n",
            "For Step: 43120 recon_loss: 0.1739761382341385 \tdiscriminator_loss: 1.293226718902588 \tgenerator_loss: 0.7887084484100342\n",
            "For Step: 43130 recon_loss: 0.1714337021112442 \tdiscriminator_loss: 1.322434663772583 \tgenerator_loss: 0.8087942004203796\n",
            "For Step: 43140 recon_loss: 0.1692289561033249 \tdiscriminator_loss: 1.3477979898452759 \tgenerator_loss: 0.7938809990882874\n",
            "For Step: 43150 recon_loss: 0.17798735201358795 \tdiscriminator_loss: 1.2801400423049927 \tgenerator_loss: 0.8445171117782593\n",
            "For Step: 43160 recon_loss: 0.17081357538700104 \tdiscriminator_loss: 1.2757704257965088 \tgenerator_loss: 0.8482796549797058\n",
            "For Step: 43170 recon_loss: 0.1737426519393921 \tdiscriminator_loss: 1.3470169305801392 \tgenerator_loss: 0.7702272534370422\n",
            "For Step: 43180 recon_loss: 0.17767281830310822 \tdiscriminator_loss: 1.2898898124694824 \tgenerator_loss: 0.8289617896080017\n",
            "For Step: 43190 recon_loss: 0.18018707633018494 \tdiscriminator_loss: 1.3013826608657837 \tgenerator_loss: 0.7543508410453796\n",
            "For Step: 43200 recon_loss: 0.17345087230205536 \tdiscriminator_loss: 1.2838528156280518 \tgenerator_loss: 0.8064060211181641\n",
            "For Step: 43210 recon_loss: 0.17467619478702545 \tdiscriminator_loss: 1.3418735265731812 \tgenerator_loss: 0.76582932472229\n",
            "For Step: 43220 recon_loss: 0.1725333333015442 \tdiscriminator_loss: 1.3483476638793945 \tgenerator_loss: 0.7727692723274231\n",
            "For Step: 43230 recon_loss: 0.1734643280506134 \tdiscriminator_loss: 1.337756633758545 \tgenerator_loss: 0.8180896043777466\n",
            "For Step: 43240 recon_loss: 0.16641920804977417 \tdiscriminator_loss: 1.312431812286377 \tgenerator_loss: 0.7941564917564392\n",
            "For Step: 43250 recon_loss: 0.1743556410074234 \tdiscriminator_loss: 1.363490104675293 \tgenerator_loss: 0.796497106552124\n",
            "For Step: 43260 recon_loss: 0.16681836545467377 \tdiscriminator_loss: 1.2613853216171265 \tgenerator_loss: 0.8132517337799072\n",
            "For Step: 43270 recon_loss: 0.16621023416519165 \tdiscriminator_loss: 1.2885955572128296 \tgenerator_loss: 0.7837813496589661\n",
            "For Step: 43280 recon_loss: 0.17538656294345856 \tdiscriminator_loss: 1.2974612712860107 \tgenerator_loss: 0.7805521488189697\n",
            "For Step: 43290 recon_loss: 0.17174853384494781 \tdiscriminator_loss: 1.2353121042251587 \tgenerator_loss: 0.7947645783424377\n",
            "For Step: 43300 recon_loss: 0.17859019339084625 \tdiscriminator_loss: 1.3501063585281372 \tgenerator_loss: 0.8160789012908936\n",
            "For Step: 43310 recon_loss: 0.17193818092346191 \tdiscriminator_loss: 1.2787563800811768 \tgenerator_loss: 0.7879724502563477\n",
            "For Step: 43320 recon_loss: 0.1770491898059845 \tdiscriminator_loss: 1.3451298475265503 \tgenerator_loss: 0.7774727940559387\n",
            "For Step: 43330 recon_loss: 0.17058442533016205 \tdiscriminator_loss: 1.3235669136047363 \tgenerator_loss: 0.820170521736145\n",
            "For Step: 43340 recon_loss: 0.16741231083869934 \tdiscriminator_loss: 1.3223700523376465 \tgenerator_loss: 0.8350995779037476\n",
            "For Step: 43350 recon_loss: 0.16599030792713165 \tdiscriminator_loss: 1.2685092687606812 \tgenerator_loss: 0.8275845050811768\n",
            "For Step: 43360 recon_loss: 0.1669245958328247 \tdiscriminator_loss: 1.3954720497131348 \tgenerator_loss: 0.7899923324584961\n",
            "For Step: 43370 recon_loss: 0.17831160128116608 \tdiscriminator_loss: 1.2285096645355225 \tgenerator_loss: 0.8508739471435547\n",
            "For Step: 43380 recon_loss: 0.16241440176963806 \tdiscriminator_loss: 1.316008448600769 \tgenerator_loss: 0.7959772348403931\n",
            "For Step: 43390 recon_loss: 0.18288922309875488 \tdiscriminator_loss: 1.396172046661377 \tgenerator_loss: 0.7606992721557617\n",
            "For Step: 43400 recon_loss: 0.17285799980163574 \tdiscriminator_loss: 1.3321216106414795 \tgenerator_loss: 0.7903138399124146\n",
            "For Step: 43410 recon_loss: 0.16319333016872406 \tdiscriminator_loss: 1.3239333629608154 \tgenerator_loss: 0.8178433775901794\n",
            "For Step: 43420 recon_loss: 0.17075546085834503 \tdiscriminator_loss: 1.2369604110717773 \tgenerator_loss: 0.8529351949691772\n",
            "For Step: 43430 recon_loss: 0.1594412922859192 \tdiscriminator_loss: 1.3103703260421753 \tgenerator_loss: 0.852950394153595\n",
            "For Step: 43440 recon_loss: 0.18137219548225403 \tdiscriminator_loss: 1.3059073686599731 \tgenerator_loss: 0.8243887424468994\n",
            "For Step: 43450 recon_loss: 0.1713097244501114 \tdiscriminator_loss: 1.3387582302093506 \tgenerator_loss: 0.8131991028785706\n",
            "For Step: 43460 recon_loss: 0.1725420206785202 \tdiscriminator_loss: 1.232156753540039 \tgenerator_loss: 0.8120600581169128\n",
            "For Step: 43470 recon_loss: 0.16836096346378326 \tdiscriminator_loss: 1.3098149299621582 \tgenerator_loss: 0.8021228909492493\n",
            "For Step: 43480 recon_loss: 0.1688157469034195 \tdiscriminator_loss: 1.2352335453033447 \tgenerator_loss: 0.8655162453651428\n",
            "For Step: 43490 recon_loss: 0.17678380012512207 \tdiscriminator_loss: 1.2850481271743774 \tgenerator_loss: 0.8282041549682617\n",
            "For Step: 43500 recon_loss: 0.16202788054943085 \tdiscriminator_loss: 1.265449047088623 \tgenerator_loss: 0.8438105583190918\n",
            "For Step: 43510 recon_loss: 0.1762138307094574 \tdiscriminator_loss: 1.346139907836914 \tgenerator_loss: 0.7654709815979004\n",
            "For Step: 43520 recon_loss: 0.1699540913105011 \tdiscriminator_loss: 1.2855058908462524 \tgenerator_loss: 0.7774038314819336\n",
            "For Step: 43530 recon_loss: 0.17512691020965576 \tdiscriminator_loss: 1.2906343936920166 \tgenerator_loss: 0.8164138197898865\n",
            "For Step: 43540 recon_loss: 0.17551109194755554 \tdiscriminator_loss: 1.2279201745986938 \tgenerator_loss: 0.809829592704773\n",
            "For Step: 43550 recon_loss: 0.17127010226249695 \tdiscriminator_loss: 1.2564401626586914 \tgenerator_loss: 0.8214219808578491\n",
            "For Step: 43560 recon_loss: 0.17522946000099182 \tdiscriminator_loss: 1.3391927480697632 \tgenerator_loss: 0.800360918045044\n",
            "For Step: 43570 recon_loss: 0.17237916588783264 \tdiscriminator_loss: 1.2887948751449585 \tgenerator_loss: 0.784980058670044\n",
            "For Step: 43580 recon_loss: 0.17230324447155 \tdiscriminator_loss: 1.2677886486053467 \tgenerator_loss: 0.8447387218475342\n",
            "For Step: 43590 recon_loss: 0.17334716022014618 \tdiscriminator_loss: 1.2478684186935425 \tgenerator_loss: 0.8555903434753418\n",
            "For Step: 43600 recon_loss: 0.16924545168876648 \tdiscriminator_loss: 1.2297825813293457 \tgenerator_loss: 0.8199655413627625\n",
            "For Step: 43610 recon_loss: 0.16500236093997955 \tdiscriminator_loss: 1.2836111783981323 \tgenerator_loss: 0.8409550786018372\n",
            "For Step: 43620 recon_loss: 0.16687601804733276 \tdiscriminator_loss: 1.2601633071899414 \tgenerator_loss: 0.847177267074585\n",
            "For Step: 43630 recon_loss: 0.17537803947925568 \tdiscriminator_loss: 1.3260384798049927 \tgenerator_loss: 0.7780386209487915\n",
            "For Step: 43640 recon_loss: 0.16357102990150452 \tdiscriminator_loss: 1.2952377796173096 \tgenerator_loss: 0.8510500192642212\n",
            "For Step: 43650 recon_loss: 0.18016566336154938 \tdiscriminator_loss: 1.313298225402832 \tgenerator_loss: 0.8312744498252869\n",
            "For Step: 43660 recon_loss: 0.17288032174110413 \tdiscriminator_loss: 1.3041292428970337 \tgenerator_loss: 0.8615039587020874\n",
            "For Step: 43670 recon_loss: 0.16481077671051025 \tdiscriminator_loss: 1.3531036376953125 \tgenerator_loss: 0.8228352665901184\n",
            "For Step: 43680 recon_loss: 0.1613035351037979 \tdiscriminator_loss: 1.3317301273345947 \tgenerator_loss: 0.8820778131484985\n",
            "For Step: 43690 recon_loss: 0.17185288667678833 \tdiscriminator_loss: 1.314098596572876 \tgenerator_loss: 0.7928937673568726\n",
            "For Step: 43700 recon_loss: 0.17638789117336273 \tdiscriminator_loss: 1.2868084907531738 \tgenerator_loss: 0.834201455116272\n",
            "For Step: 43710 recon_loss: 0.16985446214675903 \tdiscriminator_loss: 1.2825554609298706 \tgenerator_loss: 0.8194085359573364\n",
            "For Step: 43720 recon_loss: 0.18351954221725464 \tdiscriminator_loss: 1.2999101877212524 \tgenerator_loss: 0.8138638734817505\n",
            "For Step: 43730 recon_loss: 0.16445860266685486 \tdiscriminator_loss: 1.2620327472686768 \tgenerator_loss: 0.8648139238357544\n",
            "For Step: 43740 recon_loss: 0.16349053382873535 \tdiscriminator_loss: 1.3115211725234985 \tgenerator_loss: 0.8091396689414978\n",
            "For Step: 43750 recon_loss: 0.1727842390537262 \tdiscriminator_loss: 1.240050196647644 \tgenerator_loss: 0.7889120578765869\n",
            "For Step: 43760 recon_loss: 0.17550145089626312 \tdiscriminator_loss: 1.3369951248168945 \tgenerator_loss: 0.8256741762161255\n",
            "For Step: 43770 recon_loss: 0.17303048074245453 \tdiscriminator_loss: 1.2539856433868408 \tgenerator_loss: 0.8644191026687622\n",
            "For Step: 43780 recon_loss: 0.17843563854694366 \tdiscriminator_loss: 1.2705353498458862 \tgenerator_loss: 0.8177453875541687\n",
            "For Step: 43790 recon_loss: 0.17066659033298492 \tdiscriminator_loss: 1.223168134689331 \tgenerator_loss: 0.8256214261054993\n",
            "For Step: 43800 recon_loss: 0.15637853741645813 \tdiscriminator_loss: 1.2580254077911377 \tgenerator_loss: 0.8371966481208801\n",
            "For Step: 43810 recon_loss: 0.17180536687374115 \tdiscriminator_loss: 1.354897379875183 \tgenerator_loss: 0.8194066286087036\n",
            "For Step: 43820 recon_loss: 0.16500738263130188 \tdiscriminator_loss: 1.2502634525299072 \tgenerator_loss: 0.8423600196838379\n",
            "For Step: 43830 recon_loss: 0.15920914709568024 \tdiscriminator_loss: 1.3435771465301514 \tgenerator_loss: 0.8064022064208984\n",
            "For Step: 43840 recon_loss: 0.16625183820724487 \tdiscriminator_loss: 1.236743688583374 \tgenerator_loss: 0.8263387680053711\n",
            "For Step: 43850 recon_loss: 0.17439422011375427 \tdiscriminator_loss: 1.2967803478240967 \tgenerator_loss: 0.8585331439971924\n",
            "For Step: 43860 recon_loss: 0.17680253088474274 \tdiscriminator_loss: 1.2952089309692383 \tgenerator_loss: 0.8205039501190186\n",
            "For Step: 43870 recon_loss: 0.17765547335147858 \tdiscriminator_loss: 1.2847719192504883 \tgenerator_loss: 0.8331812620162964\n",
            "For Step: 43880 recon_loss: 0.1839921474456787 \tdiscriminator_loss: 1.366741418838501 \tgenerator_loss: 0.7647932171821594\n",
            "For Step: 43890 recon_loss: 0.1546584963798523 \tdiscriminator_loss: 1.1968828439712524 \tgenerator_loss: 0.854803204536438\n",
            "For Step: 43900 recon_loss: 0.1609547734260559 \tdiscriminator_loss: 1.3225497007369995 \tgenerator_loss: 0.8055557608604431\n",
            "For Step: 43910 recon_loss: 0.18742375075817108 \tdiscriminator_loss: 1.42922043800354 \tgenerator_loss: 0.803329348564148\n",
            "For Step: 43920 recon_loss: 0.17547696828842163 \tdiscriminator_loss: 1.30928635597229 \tgenerator_loss: 0.8252428770065308\n",
            "For Step: 43930 recon_loss: 0.16639578342437744 \tdiscriminator_loss: 1.314072847366333 \tgenerator_loss: 0.8057786226272583\n",
            "For Step: 43940 recon_loss: 0.16879618167877197 \tdiscriminator_loss: 1.3359689712524414 \tgenerator_loss: 0.8043402433395386\n",
            "For Step: 43950 recon_loss: 0.17706596851348877 \tdiscriminator_loss: 1.3694674968719482 \tgenerator_loss: 0.784245491027832\n",
            "For Step: 43960 recon_loss: 0.16738641262054443 \tdiscriminator_loss: 1.2775073051452637 \tgenerator_loss: 0.829547643661499\n",
            "For Step: 43970 recon_loss: 0.1796969771385193 \tdiscriminator_loss: 1.3742799758911133 \tgenerator_loss: 0.7975965738296509\n",
            "For Step: 43980 recon_loss: 0.1832958161830902 \tdiscriminator_loss: 1.2239623069763184 \tgenerator_loss: 0.8760212063789368\n",
            "For Step: 43990 recon_loss: 0.17104817926883698 \tdiscriminator_loss: 1.319259762763977 \tgenerator_loss: 0.8387277722358704\n",
            "For Step: 44000 recon_loss: 0.16497540473937988 \tdiscriminator_loss: 1.329289436340332 \tgenerator_loss: 0.8068938255310059\n",
            "For Step: 44010 recon_loss: 0.17247608304023743 \tdiscriminator_loss: 1.2474396228790283 \tgenerator_loss: 0.8768004179000854\n",
            "For Step: 44020 recon_loss: 0.17729881405830383 \tdiscriminator_loss: 1.22829270362854 \tgenerator_loss: 0.8389965295791626\n",
            "For Step: 44030 recon_loss: 0.1782388687133789 \tdiscriminator_loss: 1.3511905670166016 \tgenerator_loss: 0.7728053331375122\n",
            "For Step: 44040 recon_loss: 0.17940175533294678 \tdiscriminator_loss: 1.2939056158065796 \tgenerator_loss: 0.8623791933059692\n",
            "For Step: 44050 recon_loss: 0.17580546438694 \tdiscriminator_loss: 1.3523170948028564 \tgenerator_loss: 0.8129363059997559\n",
            "For Step: 44060 recon_loss: 0.17466093599796295 \tdiscriminator_loss: 1.3195254802703857 \tgenerator_loss: 0.80359947681427\n",
            "For Step: 44070 recon_loss: 0.1670568287372589 \tdiscriminator_loss: 1.2396152019500732 \tgenerator_loss: 0.8382989168167114\n",
            "For Step: 44080 recon_loss: 0.17293505370616913 \tdiscriminator_loss: 1.3073086738586426 \tgenerator_loss: 0.8472537994384766\n",
            "For Step: 44090 recon_loss: 0.16853924095630646 \tdiscriminator_loss: 1.2596280574798584 \tgenerator_loss: 0.853056788444519\n",
            "For Step: 44100 recon_loss: 0.17452016472816467 \tdiscriminator_loss: 1.3602445125579834 \tgenerator_loss: 0.7880598306655884\n",
            "For Step: 44110 recon_loss: 0.1850496381521225 \tdiscriminator_loss: 1.28780198097229 \tgenerator_loss: 0.8611927032470703\n",
            "For Step: 44120 recon_loss: 0.18315622210502625 \tdiscriminator_loss: 1.2841209173202515 \tgenerator_loss: 0.8142668008804321\n",
            "For Step: 44130 recon_loss: 0.1657080352306366 \tdiscriminator_loss: 1.2784054279327393 \tgenerator_loss: 0.7742719054222107\n",
            "For Step: 44140 recon_loss: 0.1745591014623642 \tdiscriminator_loss: 1.3003730773925781 \tgenerator_loss: 0.8189473152160645\n",
            "For Step: 44150 recon_loss: 0.17348478734493256 \tdiscriminator_loss: 1.2884478569030762 \tgenerator_loss: 0.8206859827041626\n",
            "For Step: 44160 recon_loss: 0.16973069310188293 \tdiscriminator_loss: 1.3248969316482544 \tgenerator_loss: 0.7997548580169678\n",
            "For Step: 44170 recon_loss: 0.17390210926532745 \tdiscriminator_loss: 1.3089959621429443 \tgenerator_loss: 0.8184980750083923\n",
            "For Step: 44180 recon_loss: 0.1797730177640915 \tdiscriminator_loss: 1.2770569324493408 \tgenerator_loss: 0.8071473240852356\n",
            "For Step: 44190 recon_loss: 0.18753191828727722 \tdiscriminator_loss: 1.4142780303955078 \tgenerator_loss: 0.8297487497329712\n",
            "For Step: 44200 recon_loss: 0.16427375376224518 \tdiscriminator_loss: 1.4078761339187622 \tgenerator_loss: 0.773829460144043\n",
            "For Step: 44210 recon_loss: 0.17386575043201447 \tdiscriminator_loss: 1.288658618927002 \tgenerator_loss: 0.8023849725723267\n",
            "For Step: 44220 recon_loss: 0.1703193485736847 \tdiscriminator_loss: 1.2759897708892822 \tgenerator_loss: 0.8111776113510132\n",
            "For Step: 44230 recon_loss: 0.16494645178318024 \tdiscriminator_loss: 1.3162662982940674 \tgenerator_loss: 0.7718625664710999\n",
            "For Step: 44240 recon_loss: 0.16353009641170502 \tdiscriminator_loss: 1.3573651313781738 \tgenerator_loss: 0.8012135624885559\n",
            "For Step: 44250 recon_loss: 0.17126505076885223 \tdiscriminator_loss: 1.2327218055725098 \tgenerator_loss: 0.7864394187927246\n",
            "For Step: 44260 recon_loss: 0.16919630765914917 \tdiscriminator_loss: 1.313043236732483 \tgenerator_loss: 0.7984520196914673\n",
            "For Step: 44270 recon_loss: 0.1731806844472885 \tdiscriminator_loss: 1.2293730974197388 \tgenerator_loss: 0.7950732707977295\n",
            "For Step: 44280 recon_loss: 0.18669821321964264 \tdiscriminator_loss: 1.2517660856246948 \tgenerator_loss: 0.8317100405693054\n",
            "For Step: 44290 recon_loss: 0.16887526214122772 \tdiscriminator_loss: 1.304520606994629 \tgenerator_loss: 0.8146939277648926\n",
            "For Step: 44300 recon_loss: 0.15833474695682526 \tdiscriminator_loss: 1.3475795984268188 \tgenerator_loss: 0.8085620999336243\n",
            "For Step: 44310 recon_loss: 0.1731785237789154 \tdiscriminator_loss: 1.4111378192901611 \tgenerator_loss: 0.7782036066055298\n",
            "For Step: 44320 recon_loss: 0.177145317196846 \tdiscriminator_loss: 1.2134016752243042 \tgenerator_loss: 0.8162540793418884\n",
            "For Step: 44330 recon_loss: 0.17355206608772278 \tdiscriminator_loss: 1.331620693206787 \tgenerator_loss: 0.756257176399231\n",
            "For Step: 44340 recon_loss: 0.16454344987869263 \tdiscriminator_loss: 1.2994269132614136 \tgenerator_loss: 0.8050837516784668\n",
            "For Step: 44350 recon_loss: 0.16943776607513428 \tdiscriminator_loss: 1.368044376373291 \tgenerator_loss: 0.7683762907981873\n",
            "For Step: 44360 recon_loss: 0.17340680956840515 \tdiscriminator_loss: 1.327092170715332 \tgenerator_loss: 0.7984786033630371\n",
            "For Step: 44370 recon_loss: 0.15822342038154602 \tdiscriminator_loss: 1.2585492134094238 \tgenerator_loss: 0.7919223308563232\n",
            "For Step: 44380 recon_loss: 0.1705465316772461 \tdiscriminator_loss: 1.273561716079712 \tgenerator_loss: 0.8784431219100952\n",
            "For Step: 44390 recon_loss: 0.1686679720878601 \tdiscriminator_loss: 1.2242228984832764 \tgenerator_loss: 0.8836528658866882\n",
            "For Step: 44400 recon_loss: 0.16759084165096283 \tdiscriminator_loss: 1.399581789970398 \tgenerator_loss: 0.7790057063102722\n",
            "For Step: 44410 recon_loss: 0.17165379226207733 \tdiscriminator_loss: 1.311773419380188 \tgenerator_loss: 0.8297438621520996\n",
            "For Step: 44420 recon_loss: 0.17827631533145905 \tdiscriminator_loss: 1.3278477191925049 \tgenerator_loss: 0.8327329158782959\n",
            "For Step: 44430 recon_loss: 0.17519526183605194 \tdiscriminator_loss: 1.357498049736023 \tgenerator_loss: 0.8058890700340271\n",
            "For Step: 44440 recon_loss: 0.17739611864089966 \tdiscriminator_loss: 1.3101943731307983 \tgenerator_loss: 0.7987281084060669\n",
            "For Step: 44450 recon_loss: 0.16422493755817413 \tdiscriminator_loss: 1.261027455329895 \tgenerator_loss: 0.748278021812439\n",
            "For Step: 44460 recon_loss: 0.15691393613815308 \tdiscriminator_loss: 1.2795252799987793 \tgenerator_loss: 0.7790364623069763\n",
            "For Step: 44470 recon_loss: 0.16688589751720428 \tdiscriminator_loss: 1.3726375102996826 \tgenerator_loss: 0.8179492354393005\n",
            "For Step: 44480 recon_loss: 0.18573381006717682 \tdiscriminator_loss: 1.2851910591125488 \tgenerator_loss: 0.7978019118309021\n",
            "For Step: 44490 recon_loss: 0.16516798734664917 \tdiscriminator_loss: 1.3092008829116821 \tgenerator_loss: 0.8100902438163757\n",
            "For Step: 44500 recon_loss: 0.16794516146183014 \tdiscriminator_loss: 1.3298981189727783 \tgenerator_loss: 0.824934720993042\n",
            "For Step: 44510 recon_loss: 0.16907629370689392 \tdiscriminator_loss: 1.2677159309387207 \tgenerator_loss: 0.803085446357727\n",
            "For Step: 44520 recon_loss: 0.163983553647995 \tdiscriminator_loss: 1.3163461685180664 \tgenerator_loss: 0.8025655150413513\n",
            "For Step: 44530 recon_loss: 0.16930311918258667 \tdiscriminator_loss: 1.2130223512649536 \tgenerator_loss: 0.8489150404930115\n",
            "For Step: 44540 recon_loss: 0.17670054733753204 \tdiscriminator_loss: 1.3083670139312744 \tgenerator_loss: 0.8067853450775146\n",
            "For Step: 44550 recon_loss: 0.17574024200439453 \tdiscriminator_loss: 1.315216302871704 \tgenerator_loss: 0.8567380905151367\n",
            "For Step: 44560 recon_loss: 0.16266348958015442 \tdiscriminator_loss: 1.2369935512542725 \tgenerator_loss: 0.8629783391952515\n",
            "For Step: 44570 recon_loss: 0.18462516367435455 \tdiscriminator_loss: 1.2011767625808716 \tgenerator_loss: 0.8494617342948914\n",
            "For Step: 44580 recon_loss: 0.18379804491996765 \tdiscriminator_loss: 1.3350201845169067 \tgenerator_loss: 0.7655254602432251\n",
            "For Step: 44590 recon_loss: 0.17310559749603271 \tdiscriminator_loss: 1.3548142910003662 \tgenerator_loss: 0.8219186067581177\n",
            "For Step: 44600 recon_loss: 0.16984431445598602 \tdiscriminator_loss: 1.2328938245773315 \tgenerator_loss: 0.844092845916748\n",
            "For Step: 44610 recon_loss: 0.17253394424915314 \tdiscriminator_loss: 1.3820329904556274 \tgenerator_loss: 0.782082200050354\n",
            "For Step: 44620 recon_loss: 0.1787114441394806 \tdiscriminator_loss: 1.2727503776550293 \tgenerator_loss: 0.8191058039665222\n",
            "For Step: 44630 recon_loss: 0.18020905554294586 \tdiscriminator_loss: 1.2645413875579834 \tgenerator_loss: 0.8114460706710815\n",
            "For Step: 44640 recon_loss: 0.17178864777088165 \tdiscriminator_loss: 1.2469513416290283 \tgenerator_loss: 0.7696866989135742\n",
            "For Step: 44650 recon_loss: 0.16903340816497803 \tdiscriminator_loss: 1.3275108337402344 \tgenerator_loss: 0.8321059346199036\n",
            "For Step: 44660 recon_loss: 0.1808648407459259 \tdiscriminator_loss: 1.2898136377334595 \tgenerator_loss: 0.814157247543335\n",
            "For Step: 44670 recon_loss: 0.18924584984779358 \tdiscriminator_loss: 1.3123269081115723 \tgenerator_loss: 0.8643178939819336\n",
            "For Step: 44680 recon_loss: 0.16983667016029358 \tdiscriminator_loss: 1.316228985786438 \tgenerator_loss: 0.7533643841743469\n",
            "For Step: 44690 recon_loss: 0.16424451768398285 \tdiscriminator_loss: 1.322012186050415 \tgenerator_loss: 0.7633596658706665\n",
            "For Step: 44700 recon_loss: 0.18006210029125214 \tdiscriminator_loss: 1.3533995151519775 \tgenerator_loss: 0.7849576473236084\n",
            "For Step: 44710 recon_loss: 0.16752508282661438 \tdiscriminator_loss: 1.3133360147476196 \tgenerator_loss: 0.8766920566558838\n",
            "For Step: 44720 recon_loss: 0.1748906672000885 \tdiscriminator_loss: 1.1879557371139526 \tgenerator_loss: 0.8400135040283203\n",
            "For Step: 44730 recon_loss: 0.18204276263713837 \tdiscriminator_loss: 1.3103218078613281 \tgenerator_loss: 0.7624330520629883\n",
            "For Step: 44740 recon_loss: 0.18190045654773712 \tdiscriminator_loss: 1.252863883972168 \tgenerator_loss: 0.8345373868942261\n",
            "For Step: 44750 recon_loss: 0.1762021780014038 \tdiscriminator_loss: 1.4194222688674927 \tgenerator_loss: 0.7903990745544434\n",
            "For Step: 44760 recon_loss: 0.17143352329730988 \tdiscriminator_loss: 1.3575526475906372 \tgenerator_loss: 0.8000079393386841\n",
            "For Step: 44770 recon_loss: 0.16861966252326965 \tdiscriminator_loss: 1.3581751585006714 \tgenerator_loss: 0.81028151512146\n",
            "For Step: 44780 recon_loss: 0.17328007519245148 \tdiscriminator_loss: 1.3229800462722778 \tgenerator_loss: 0.7995815277099609\n",
            "For Step: 44790 recon_loss: 0.17772600054740906 \tdiscriminator_loss: 1.3320717811584473 \tgenerator_loss: 0.7691303491592407\n",
            "For Step: 44800 recon_loss: 0.1774655133485794 \tdiscriminator_loss: 1.1902375221252441 \tgenerator_loss: 0.8268810510635376\n",
            "For Step: 44810 recon_loss: 0.16573522984981537 \tdiscriminator_loss: 1.3764108419418335 \tgenerator_loss: 0.7770835757255554\n",
            "For Step: 44820 recon_loss: 0.17231552302837372 \tdiscriminator_loss: 1.3132975101470947 \tgenerator_loss: 0.7977140545845032\n",
            "For Step: 44830 recon_loss: 0.17100583016872406 \tdiscriminator_loss: 1.3148834705352783 \tgenerator_loss: 0.7810255289077759\n",
            "For Step: 44840 recon_loss: 0.16701196134090424 \tdiscriminator_loss: 1.4068799018859863 \tgenerator_loss: 0.7568550109863281\n",
            "For Step: 44850 recon_loss: 0.17516417801380157 \tdiscriminator_loss: 1.2262364625930786 \tgenerator_loss: 0.8720276355743408\n",
            "For Step: 44860 recon_loss: 0.17537009716033936 \tdiscriminator_loss: 1.3560034036636353 \tgenerator_loss: 0.7425004243850708\n",
            "For Step: 44870 recon_loss: 0.18740327656269073 \tdiscriminator_loss: 1.2807843685150146 \tgenerator_loss: 0.7966368198394775\n",
            "For Step: 44880 recon_loss: 0.17249684035778046 \tdiscriminator_loss: 1.2764954566955566 \tgenerator_loss: 0.8161152601242065\n",
            "For Step: 44890 recon_loss: 0.18436627089977264 \tdiscriminator_loss: 1.355238676071167 \tgenerator_loss: 0.8175176382064819\n",
            "For Step: 44900 recon_loss: 0.1804691106081009 \tdiscriminator_loss: 1.3113200664520264 \tgenerator_loss: 0.7894330024719238\n",
            "For Step: 44910 recon_loss: 0.17730998992919922 \tdiscriminator_loss: 1.329065203666687 \tgenerator_loss: 0.7979071140289307\n",
            "For Step: 44920 recon_loss: 0.1699182093143463 \tdiscriminator_loss: 1.28987455368042 \tgenerator_loss: 0.8199211359024048\n",
            "For Step: 44930 recon_loss: 0.1720927506685257 \tdiscriminator_loss: 1.355844497680664 \tgenerator_loss: 0.8035752773284912\n",
            "For Step: 44940 recon_loss: 0.17165151238441467 \tdiscriminator_loss: 1.2692408561706543 \tgenerator_loss: 0.8497085571289062\n",
            "For Step: 44950 recon_loss: 0.17679695785045624 \tdiscriminator_loss: 1.3292409181594849 \tgenerator_loss: 0.7928515076637268\n",
            "For Step: 44960 recon_loss: 0.15664193034172058 \tdiscriminator_loss: 1.2605834007263184 \tgenerator_loss: 0.8254610300064087\n",
            "For Step: 44970 recon_loss: 0.1716988980770111 \tdiscriminator_loss: 1.2297756671905518 \tgenerator_loss: 0.8330250382423401\n",
            "For Step: 44980 recon_loss: 0.16698379814624786 \tdiscriminator_loss: 1.2656655311584473 \tgenerator_loss: 0.8281676769256592\n",
            "For Step: 44990 recon_loss: 0.17064514756202698 \tdiscriminator_loss: 1.3701223134994507 \tgenerator_loss: 0.764445424079895\n",
            "For Step: 45000 recon_loss: 0.18323847651481628 \tdiscriminator_loss: 1.316619634628296 \tgenerator_loss: 0.7993122339248657\n",
            "For Step: 45010 recon_loss: 0.16623108088970184 \tdiscriminator_loss: 1.278700828552246 \tgenerator_loss: 0.8505798578262329\n",
            "For Step: 45020 recon_loss: 0.17135803401470184 \tdiscriminator_loss: 1.266867756843567 \tgenerator_loss: 0.8354895114898682\n",
            "For Step: 45030 recon_loss: 0.16579382121562958 \tdiscriminator_loss: 1.3293806314468384 \tgenerator_loss: 0.8492773771286011\n",
            "For Step: 45040 recon_loss: 0.18069413304328918 \tdiscriminator_loss: 1.2526931762695312 \tgenerator_loss: 0.8319321870803833\n",
            "For Step: 45050 recon_loss: 0.1603289097547531 \tdiscriminator_loss: 1.3321466445922852 \tgenerator_loss: 0.7947914004325867\n",
            "For Step: 45060 recon_loss: 0.16902223229408264 \tdiscriminator_loss: 1.2648266553878784 \tgenerator_loss: 0.7791280746459961\n",
            "For Step: 45070 recon_loss: 0.16267308592796326 \tdiscriminator_loss: 1.2491393089294434 \tgenerator_loss: 0.8326015472412109\n",
            "For Step: 45080 recon_loss: 0.17240943014621735 \tdiscriminator_loss: 1.3324787616729736 \tgenerator_loss: 0.7847758531570435\n",
            "For Step: 45090 recon_loss: 0.1738199144601822 \tdiscriminator_loss: 1.3043022155761719 \tgenerator_loss: 0.7861582636833191\n",
            "For Step: 45100 recon_loss: 0.17060716450214386 \tdiscriminator_loss: 1.3380086421966553 \tgenerator_loss: 0.83482825756073\n",
            "For Step: 45110 recon_loss: 0.1725521832704544 \tdiscriminator_loss: 1.3912955522537231 \tgenerator_loss: 0.7823537588119507\n",
            "For Step: 45120 recon_loss: 0.15828697383403778 \tdiscriminator_loss: 1.2844839096069336 \tgenerator_loss: 0.8197034597396851\n",
            "For Step: 45130 recon_loss: 0.1618424504995346 \tdiscriminator_loss: 1.376230001449585 \tgenerator_loss: 0.8087911605834961\n",
            "For Step: 45140 recon_loss: 0.17498934268951416 \tdiscriminator_loss: 1.3093390464782715 \tgenerator_loss: 0.7797759771347046\n",
            "For Step: 45150 recon_loss: 0.17127804458141327 \tdiscriminator_loss: 1.354648232460022 \tgenerator_loss: 0.7938461899757385\n",
            "For Step: 45160 recon_loss: 0.16743072867393494 \tdiscriminator_loss: 1.3700757026672363 \tgenerator_loss: 0.7829279899597168\n",
            "For Step: 45170 recon_loss: 0.17038816213607788 \tdiscriminator_loss: 1.301222324371338 \tgenerator_loss: 0.7824317216873169\n",
            "For Step: 45180 recon_loss: 0.178115576505661 \tdiscriminator_loss: 1.247950553894043 \tgenerator_loss: 0.8629533052444458\n",
            "For Step: 45190 recon_loss: 0.16630485653877258 \tdiscriminator_loss: 1.3130486011505127 \tgenerator_loss: 0.8011308312416077\n",
            "For Step: 45200 recon_loss: 0.1678266078233719 \tdiscriminator_loss: 1.276719093322754 \tgenerator_loss: 0.8221447467803955\n",
            "For Step: 45210 recon_loss: 0.16421353816986084 \tdiscriminator_loss: 1.2740122079849243 \tgenerator_loss: 0.8011043667793274\n",
            "For Step: 45220 recon_loss: 0.16885320842266083 \tdiscriminator_loss: 1.2741062641143799 \tgenerator_loss: 0.8888601064682007\n",
            "For Step: 45230 recon_loss: 0.1685379594564438 \tdiscriminator_loss: 1.2861430644989014 \tgenerator_loss: 0.849681556224823\n",
            "For Step: 45240 recon_loss: 0.1700534075498581 \tdiscriminator_loss: 1.3560243844985962 \tgenerator_loss: 0.7814765572547913\n",
            "For Step: 45250 recon_loss: 0.18166540563106537 \tdiscriminator_loss: 1.3830769062042236 \tgenerator_loss: 0.778801441192627\n",
            "For Step: 45260 recon_loss: 0.1606207937002182 \tdiscriminator_loss: 1.371741771697998 \tgenerator_loss: 0.8236322999000549\n",
            "For Step: 45270 recon_loss: 0.17224091291427612 \tdiscriminator_loss: 1.2623318433761597 \tgenerator_loss: 0.8351213932037354\n",
            "For Step: 45280 recon_loss: 0.1657961755990982 \tdiscriminator_loss: 1.2815766334533691 \tgenerator_loss: 0.8355768918991089\n",
            "For Step: 45290 recon_loss: 0.1722462922334671 \tdiscriminator_loss: 1.3293805122375488 \tgenerator_loss: 0.812605082988739\n",
            "For Step: 45300 recon_loss: 0.17070454359054565 \tdiscriminator_loss: 1.3618485927581787 \tgenerator_loss: 0.7944869995117188\n",
            "For Step: 45310 recon_loss: 0.17030996084213257 \tdiscriminator_loss: 1.2417795658111572 \tgenerator_loss: 0.8610957264900208\n",
            "For Step: 45320 recon_loss: 0.16669349372386932 \tdiscriminator_loss: 1.2911362648010254 \tgenerator_loss: 0.8420482873916626\n",
            "For Step: 45330 recon_loss: 0.16694502532482147 \tdiscriminator_loss: 1.2722017765045166 \tgenerator_loss: 0.8166250586509705\n",
            "For Step: 45340 recon_loss: 0.1770007461309433 \tdiscriminator_loss: 1.236527681350708 \tgenerator_loss: 0.8699755072593689\n",
            "For Step: 45350 recon_loss: 0.17604057490825653 \tdiscriminator_loss: 1.3709676265716553 \tgenerator_loss: 0.7976676821708679\n",
            "For Step: 45360 recon_loss: 0.16727492213249207 \tdiscriminator_loss: 1.307746171951294 \tgenerator_loss: 0.8120430707931519\n",
            "For Step: 45370 recon_loss: 0.18337635695934296 \tdiscriminator_loss: 1.355004072189331 \tgenerator_loss: 0.8157458305358887\n",
            "For Step: 45380 recon_loss: 0.1895972639322281 \tdiscriminator_loss: 1.227830410003662 \tgenerator_loss: 0.8186752200126648\n",
            "For Step: 45390 recon_loss: 0.1731383055448532 \tdiscriminator_loss: 1.2720752954483032 \tgenerator_loss: 0.7975395917892456\n",
            "For Step: 45400 recon_loss: 0.16502425074577332 \tdiscriminator_loss: 1.227412462234497 \tgenerator_loss: 0.790100634098053\n",
            "For Step: 45410 recon_loss: 0.17445620894432068 \tdiscriminator_loss: 1.2432177066802979 \tgenerator_loss: 0.8026349544525146\n",
            "For Step: 45420 recon_loss: 0.17096126079559326 \tdiscriminator_loss: 1.2736661434173584 \tgenerator_loss: 0.830920934677124\n",
            "For Step: 45430 recon_loss: 0.17451529204845428 \tdiscriminator_loss: 1.3239929676055908 \tgenerator_loss: 0.8082655668258667\n",
            "For Step: 45440 recon_loss: 0.16797207295894623 \tdiscriminator_loss: 1.2977263927459717 \tgenerator_loss: 0.7925004959106445\n",
            "For Step: 45450 recon_loss: 0.15833412110805511 \tdiscriminator_loss: 1.342896580696106 \tgenerator_loss: 0.7730860710144043\n",
            "For Step: 45460 recon_loss: 0.1669691503047943 \tdiscriminator_loss: 1.180253267288208 \tgenerator_loss: 0.8208686113357544\n",
            "For Step: 45470 recon_loss: 0.1968134641647339 \tdiscriminator_loss: 1.2445368766784668 \tgenerator_loss: 0.801099956035614\n",
            "For Step: 45480 recon_loss: 0.1752108335494995 \tdiscriminator_loss: 1.2817857265472412 \tgenerator_loss: 0.8213814496994019\n",
            "For Step: 45490 recon_loss: 0.16906890273094177 \tdiscriminator_loss: 1.3648641109466553 \tgenerator_loss: 0.8059051632881165\n",
            "For Step: 45500 recon_loss: 0.16778406500816345 \tdiscriminator_loss: 1.305277705192566 \tgenerator_loss: 0.8218561410903931\n",
            "For Step: 45510 recon_loss: 0.1615293025970459 \tdiscriminator_loss: 1.251849889755249 \tgenerator_loss: 0.8095666170120239\n",
            "For Step: 45520 recon_loss: 0.1720104217529297 \tdiscriminator_loss: 1.2409815788269043 \tgenerator_loss: 0.8310648202896118\n",
            "For Step: 45530 recon_loss: 0.1743726283311844 \tdiscriminator_loss: 1.3405060768127441 \tgenerator_loss: 0.7969062328338623\n",
            "For Step: 45540 recon_loss: 0.16239212453365326 \tdiscriminator_loss: 1.3330137729644775 \tgenerator_loss: 0.7977241277694702\n",
            "For Step: 45550 recon_loss: 0.17852897942066193 \tdiscriminator_loss: 1.3316560983657837 \tgenerator_loss: 0.7921994924545288\n",
            "For Step: 45560 recon_loss: 0.16990159451961517 \tdiscriminator_loss: 1.25867760181427 \tgenerator_loss: 0.8570510745048523\n",
            "For Step: 45570 recon_loss: 0.17916063964366913 \tdiscriminator_loss: 1.2875787019729614 \tgenerator_loss: 0.8091731071472168\n",
            "For Step: 45580 recon_loss: 0.18061859905719757 \tdiscriminator_loss: 1.330609679222107 \tgenerator_loss: 0.794631838798523\n",
            "For Step: 45590 recon_loss: 0.15440182387828827 \tdiscriminator_loss: 1.2639635801315308 \tgenerator_loss: 0.8086326122283936\n",
            "For Step: 45600 recon_loss: 0.1771841049194336 \tdiscriminator_loss: 1.2797776460647583 \tgenerator_loss: 0.8171387314796448\n",
            "For Step: 45610 recon_loss: 0.17393721640110016 \tdiscriminator_loss: 1.2840008735656738 \tgenerator_loss: 0.7731184959411621\n",
            "For Step: 45620 recon_loss: 0.15777525305747986 \tdiscriminator_loss: 1.3157081604003906 \tgenerator_loss: 0.7730878591537476\n",
            "For Step: 45630 recon_loss: 0.17156745493412018 \tdiscriminator_loss: 1.221247673034668 \tgenerator_loss: 0.8472570776939392\n",
            "For Step: 45640 recon_loss: 0.17180436849594116 \tdiscriminator_loss: 1.351468801498413 \tgenerator_loss: 0.8281137943267822\n",
            "For Step: 45650 recon_loss: 0.17861580848693848 \tdiscriminator_loss: 1.360994577407837 \tgenerator_loss: 0.8066451549530029\n",
            "For Step: 45660 recon_loss: 0.167763352394104 \tdiscriminator_loss: 1.21805739402771 \tgenerator_loss: 0.8335035443305969\n",
            "For Step: 45670 recon_loss: 0.17293599247932434 \tdiscriminator_loss: 1.2586886882781982 \tgenerator_loss: 0.8269709944725037\n",
            "For Step: 45680 recon_loss: 0.16605758666992188 \tdiscriminator_loss: 1.2413654327392578 \tgenerator_loss: 0.8538743257522583\n",
            "For Step: 45690 recon_loss: 0.17424145340919495 \tdiscriminator_loss: 1.2344727516174316 \tgenerator_loss: 0.8393067121505737\n",
            "For Step: 45700 recon_loss: 0.18124574422836304 \tdiscriminator_loss: 1.3146854639053345 \tgenerator_loss: 0.8123648166656494\n",
            "For Step: 45710 recon_loss: 0.17256012558937073 \tdiscriminator_loss: 1.2118608951568604 \tgenerator_loss: 0.8463740348815918\n",
            "For Step: 45720 recon_loss: 0.1736554652452469 \tdiscriminator_loss: 1.323283314704895 \tgenerator_loss: 0.8026547431945801\n",
            "For Step: 45730 recon_loss: 0.1865198165178299 \tdiscriminator_loss: 1.3553602695465088 \tgenerator_loss: 0.8010873794555664\n",
            "For Step: 45740 recon_loss: 0.17787480354309082 \tdiscriminator_loss: 1.3128739595413208 \tgenerator_loss: 0.8021408915519714\n",
            "For Step: 45750 recon_loss: 0.16588343679904938 \tdiscriminator_loss: 1.2942945957183838 \tgenerator_loss: 0.829228401184082\n",
            "For Step: 45760 recon_loss: 0.17578060925006866 \tdiscriminator_loss: 1.2468905448913574 \tgenerator_loss: 0.7670936584472656\n",
            "For Step: 45770 recon_loss: 0.1652311086654663 \tdiscriminator_loss: 1.338826060295105 \tgenerator_loss: 0.8132786750793457\n",
            "For Step: 45780 recon_loss: 0.18259546160697937 \tdiscriminator_loss: 1.3415671586990356 \tgenerator_loss: 0.794607400894165\n",
            "For Step: 45790 recon_loss: 0.1653352528810501 \tdiscriminator_loss: 1.3314687013626099 \tgenerator_loss: 0.7757844924926758\n",
            "For Step: 45800 recon_loss: 0.1805085688829422 \tdiscriminator_loss: 1.3392388820648193 \tgenerator_loss: 0.8004571199417114\n",
            "For Step: 45810 recon_loss: 0.17110253870487213 \tdiscriminator_loss: 1.3273630142211914 \tgenerator_loss: 0.7720968723297119\n",
            "For Step: 45820 recon_loss: 0.17307619750499725 \tdiscriminator_loss: 1.2829499244689941 \tgenerator_loss: 0.8186684846878052\n",
            "For Step: 45830 recon_loss: 0.1733250766992569 \tdiscriminator_loss: 1.253781795501709 \tgenerator_loss: 0.8087425231933594\n",
            "For Step: 45840 recon_loss: 0.17123563587665558 \tdiscriminator_loss: 1.247513771057129 \tgenerator_loss: 0.8134179711341858\n",
            "For Step: 45850 recon_loss: 0.1685069352388382 \tdiscriminator_loss: 1.2539405822753906 \tgenerator_loss: 0.8757778406143188\n",
            "For Step: 45860 recon_loss: 0.16989417374134064 \tdiscriminator_loss: 1.368943691253662 \tgenerator_loss: 0.8008051514625549\n",
            "For Step: 45870 recon_loss: 0.16564904153347015 \tdiscriminator_loss: 1.2822374105453491 \tgenerator_loss: 0.838410496711731\n",
            "For Step: 45880 recon_loss: 0.1521204560995102 \tdiscriminator_loss: 1.3011924028396606 \tgenerator_loss: 0.8083450794219971\n",
            "For Step: 45890 recon_loss: 0.15851038694381714 \tdiscriminator_loss: 1.3796495199203491 \tgenerator_loss: 0.7998229265213013\n",
            "For Step: 45900 recon_loss: 0.17937538027763367 \tdiscriminator_loss: 1.3028206825256348 \tgenerator_loss: 0.7763731479644775\n",
            "For Step: 45910 recon_loss: 0.17246845364570618 \tdiscriminator_loss: 1.2363003492355347 \tgenerator_loss: 0.8598102331161499\n",
            "For Step: 45920 recon_loss: 0.1690933108329773 \tdiscriminator_loss: 1.2891011238098145 \tgenerator_loss: 0.8141813278198242\n",
            "For Step: 45930 recon_loss: 0.18059881031513214 \tdiscriminator_loss: 1.3057461977005005 \tgenerator_loss: 0.8531581163406372\n",
            "For Step: 45940 recon_loss: 0.15578359365463257 \tdiscriminator_loss: 1.263999581336975 \tgenerator_loss: 0.8503373861312866\n",
            "For Step: 45950 recon_loss: 0.17119453847408295 \tdiscriminator_loss: 1.2069560289382935 \tgenerator_loss: 0.8485565185546875\n",
            "For Step: 45960 recon_loss: 0.18401972949504852 \tdiscriminator_loss: 1.4018433094024658 \tgenerator_loss: 0.7827951908111572\n",
            "For Step: 45970 recon_loss: 0.17464308440685272 \tdiscriminator_loss: 1.3310132026672363 \tgenerator_loss: 0.7931960821151733\n",
            "For Step: 45980 recon_loss: 0.1517692655324936 \tdiscriminator_loss: 1.3511736392974854 \tgenerator_loss: 0.8107397556304932\n",
            "For Step: 45990 recon_loss: 0.1745065152645111 \tdiscriminator_loss: 1.3657450675964355 \tgenerator_loss: 0.7749577760696411\n",
            "For Step: 46000 recon_loss: 0.17287684977054596 \tdiscriminator_loss: 1.1994390487670898 \tgenerator_loss: 0.8047873973846436\n",
            "For Step: 46010 recon_loss: 0.16869813203811646 \tdiscriminator_loss: 1.365725040435791 \tgenerator_loss: 0.8077348470687866\n",
            "For Step: 46020 recon_loss: 0.17382507026195526 \tdiscriminator_loss: 1.2244799137115479 \tgenerator_loss: 0.8420406579971313\n",
            "For Step: 46030 recon_loss: 0.18842647969722748 \tdiscriminator_loss: 1.307413101196289 \tgenerator_loss: 0.7878729104995728\n",
            "For Step: 46040 recon_loss: 0.18281301856040955 \tdiscriminator_loss: 1.3071801662445068 \tgenerator_loss: 0.7922697067260742\n",
            "For Step: 46050 recon_loss: 0.1699620485305786 \tdiscriminator_loss: 1.2777230739593506 \tgenerator_loss: 0.8279853463172913\n",
            "For Step: 46060 recon_loss: 0.17322805523872375 \tdiscriminator_loss: 1.3176017999649048 \tgenerator_loss: 0.7998141646385193\n",
            "For Step: 46070 recon_loss: 0.16148641705513 \tdiscriminator_loss: 1.2561938762664795 \tgenerator_loss: 0.8460980653762817\n",
            "For Step: 46080 recon_loss: 0.1609843373298645 \tdiscriminator_loss: 1.3790860176086426 \tgenerator_loss: 0.8018492460250854\n",
            "For Step: 46090 recon_loss: 0.16972817480564117 \tdiscriminator_loss: 1.2427644729614258 \tgenerator_loss: 0.8340750932693481\n",
            "For Step: 46100 recon_loss: 0.1795709729194641 \tdiscriminator_loss: 1.2859925031661987 \tgenerator_loss: 0.7869016528129578\n",
            "For Step: 46110 recon_loss: 0.17411315441131592 \tdiscriminator_loss: 1.315199375152588 \tgenerator_loss: 0.7945371866226196\n",
            "For Step: 46120 recon_loss: 0.17205236852169037 \tdiscriminator_loss: 1.284937858581543 \tgenerator_loss: 0.7792216539382935\n",
            "For Step: 46130 recon_loss: 0.1636788547039032 \tdiscriminator_loss: 1.3452324867248535 \tgenerator_loss: 0.7423293590545654\n",
            "For Step: 46140 recon_loss: 0.18205974996089935 \tdiscriminator_loss: 1.2065664529800415 \tgenerator_loss: 0.8864344358444214\n",
            "For Step: 46150 recon_loss: 0.17004574835300446 \tdiscriminator_loss: 1.3187304735183716 \tgenerator_loss: 0.7744581699371338\n",
            "For Step: 46160 recon_loss: 0.17260292172431946 \tdiscriminator_loss: 1.2752015590667725 \tgenerator_loss: 0.8153067827224731\n",
            "For Step: 46170 recon_loss: 0.17240294814109802 \tdiscriminator_loss: 1.2465260028839111 \tgenerator_loss: 0.8103878498077393\n",
            "For Step: 46180 recon_loss: 0.16696162521839142 \tdiscriminator_loss: 1.2588655948638916 \tgenerator_loss: 0.812083899974823\n",
            "For Step: 46190 recon_loss: 0.17722032964229584 \tdiscriminator_loss: 1.3064888715744019 \tgenerator_loss: 0.7526569962501526\n",
            "For Step: 46200 recon_loss: 0.17755398154258728 \tdiscriminator_loss: 1.2283967733383179 \tgenerator_loss: 0.8145074844360352\n",
            "For Step: 46210 recon_loss: 0.16411878168582916 \tdiscriminator_loss: 1.2866144180297852 \tgenerator_loss: 0.8509640693664551\n",
            "For Step: 46220 recon_loss: 0.1879454255104065 \tdiscriminator_loss: 1.2868578433990479 \tgenerator_loss: 0.8287312984466553\n",
            "For Step: 46230 recon_loss: 0.18181148171424866 \tdiscriminator_loss: 1.3534520864486694 \tgenerator_loss: 0.7813881635665894\n",
            "For Step: 46240 recon_loss: 0.1756068766117096 \tdiscriminator_loss: 1.2628445625305176 \tgenerator_loss: 0.852165699005127\n",
            "For Step: 46250 recon_loss: 0.1731652170419693 \tdiscriminator_loss: 1.2796376943588257 \tgenerator_loss: 0.7870588302612305\n",
            "For Step: 46260 recon_loss: 0.16486498713493347 \tdiscriminator_loss: 1.3576581478118896 \tgenerator_loss: 0.7732131481170654\n",
            "For Step: 46270 recon_loss: 0.15892714262008667 \tdiscriminator_loss: 1.3401470184326172 \tgenerator_loss: 0.8373980522155762\n",
            "For Step: 46280 recon_loss: 0.17184028029441833 \tdiscriminator_loss: 1.3446131944656372 \tgenerator_loss: 0.819251537322998\n",
            "For Step: 46290 recon_loss: 0.16509884595870972 \tdiscriminator_loss: 1.3015491962432861 \tgenerator_loss: 0.8292810320854187\n",
            "For Step: 46300 recon_loss: 0.18477097153663635 \tdiscriminator_loss: 1.3435553312301636 \tgenerator_loss: 0.7866392135620117\n",
            "For Step: 46310 recon_loss: 0.1725512146949768 \tdiscriminator_loss: 1.2926957607269287 \tgenerator_loss: 0.8155999183654785\n",
            "For Step: 46320 recon_loss: 0.17577825486660004 \tdiscriminator_loss: 1.3901225328445435 \tgenerator_loss: 0.7944680452346802\n",
            "For Step: 46330 recon_loss: 0.15932677686214447 \tdiscriminator_loss: 1.3004450798034668 \tgenerator_loss: 0.8205817937850952\n",
            "For Step: 46340 recon_loss: 0.1762409657239914 \tdiscriminator_loss: 1.4533060789108276 \tgenerator_loss: 0.7626148462295532\n",
            "For Step: 46350 recon_loss: 0.16726455092430115 \tdiscriminator_loss: 1.344217300415039 \tgenerator_loss: 0.775697648525238\n",
            "For Step: 46360 recon_loss: 0.16794446110725403 \tdiscriminator_loss: 1.3035469055175781 \tgenerator_loss: 0.845103919506073\n",
            "For Step: 46370 recon_loss: 0.17764116823673248 \tdiscriminator_loss: 1.2924633026123047 \tgenerator_loss: 0.8514072895050049\n",
            "For Step: 46380 recon_loss: 0.17441946268081665 \tdiscriminator_loss: 1.3962595462799072 \tgenerator_loss: 0.7674528956413269\n",
            "For Step: 46390 recon_loss: 0.17704030871391296 \tdiscriminator_loss: 1.3719830513000488 \tgenerator_loss: 0.8202391862869263\n",
            "For Step: 46400 recon_loss: 0.1693556010723114 \tdiscriminator_loss: 1.4191312789916992 \tgenerator_loss: 0.7724512815475464\n",
            "For Step: 46410 recon_loss: 0.17015425860881805 \tdiscriminator_loss: 1.3074291944503784 \tgenerator_loss: 0.7817656993865967\n",
            "For Step: 46420 recon_loss: 0.17896753549575806 \tdiscriminator_loss: 1.3187127113342285 \tgenerator_loss: 0.8557804822921753\n",
            "For Step: 46430 recon_loss: 0.1758568435907364 \tdiscriminator_loss: 1.3979663848876953 \tgenerator_loss: 0.7668970227241516\n",
            "For Step: 46440 recon_loss: 0.1718815714120865 \tdiscriminator_loss: 1.3327174186706543 \tgenerator_loss: 0.7777169942855835\n",
            "For Step: 46450 recon_loss: 0.18574155867099762 \tdiscriminator_loss: 1.2688074111938477 \tgenerator_loss: 0.794404149055481\n",
            "For Step: 46460 recon_loss: 0.1715691089630127 \tdiscriminator_loss: 1.3610951900482178 \tgenerator_loss: 0.8058524131774902\n",
            "For Step: 46470 recon_loss: 0.16512855887413025 \tdiscriminator_loss: 1.2944148778915405 \tgenerator_loss: 0.7920606136322021\n",
            "For Step: 46480 recon_loss: 0.1757817566394806 \tdiscriminator_loss: 1.3434944152832031 \tgenerator_loss: 0.8086385130882263\n",
            "For Step: 46490 recon_loss: 0.17728036642074585 \tdiscriminator_loss: 1.2938365936279297 \tgenerator_loss: 0.788000226020813\n",
            "For Step: 46500 recon_loss: 0.16145502030849457 \tdiscriminator_loss: 1.2623388767242432 \tgenerator_loss: 0.8006079196929932\n",
            "For Step: 46510 recon_loss: 0.16143153607845306 \tdiscriminator_loss: 1.3099054098129272 \tgenerator_loss: 0.800058126449585\n",
            "For Step: 46520 recon_loss: 0.17752011120319366 \tdiscriminator_loss: 1.2821722030639648 \tgenerator_loss: 0.8410241007804871\n",
            "For Step: 46530 recon_loss: 0.1645842343568802 \tdiscriminator_loss: 1.414642333984375 \tgenerator_loss: 0.8047869205474854\n",
            "For Step: 46540 recon_loss: 0.16810275614261627 \tdiscriminator_loss: 1.3853551149368286 \tgenerator_loss: 0.8023691773414612\n",
            "For Step: 46550 recon_loss: 0.16675622761249542 \tdiscriminator_loss: 1.2190933227539062 \tgenerator_loss: 0.8428361415863037\n",
            "For Step: 46560 recon_loss: 0.16616038978099823 \tdiscriminator_loss: 1.313349962234497 \tgenerator_loss: 0.8112409114837646\n",
            "For Step: 46570 recon_loss: 0.1763051301240921 \tdiscriminator_loss: 1.3322045803070068 \tgenerator_loss: 0.785834550857544\n",
            "For Step: 46580 recon_loss: 0.16189081966876984 \tdiscriminator_loss: 1.3039119243621826 \tgenerator_loss: 0.761320948600769\n",
            "For Step: 46590 recon_loss: 0.16691915690898895 \tdiscriminator_loss: 1.3233973979949951 \tgenerator_loss: 0.7962701916694641\n",
            "For Step: 46600 recon_loss: 0.17662186920642853 \tdiscriminator_loss: 1.2823517322540283 \tgenerator_loss: 0.843245267868042\n",
            "For Step: 46610 recon_loss: 0.16057509183883667 \tdiscriminator_loss: 1.290161371231079 \tgenerator_loss: 0.825508177280426\n",
            "For Step: 46620 recon_loss: 0.17855262756347656 \tdiscriminator_loss: 1.3201873302459717 \tgenerator_loss: 0.7592971324920654\n",
            "For Step: 46630 recon_loss: 0.16993026435375214 \tdiscriminator_loss: 1.3198270797729492 \tgenerator_loss: 0.793893575668335\n",
            "For Step: 46640 recon_loss: 0.16463248431682587 \tdiscriminator_loss: 1.3501436710357666 \tgenerator_loss: 0.805210292339325\n",
            "For Step: 46650 recon_loss: 0.16726498305797577 \tdiscriminator_loss: 1.3500689268112183 \tgenerator_loss: 0.7870205640792847\n",
            "For Step: 46660 recon_loss: 0.1638505905866623 \tdiscriminator_loss: 1.2654736042022705 \tgenerator_loss: 0.7700387239456177\n",
            "For Step: 46670 recon_loss: 0.18513108789920807 \tdiscriminator_loss: 1.274162769317627 \tgenerator_loss: 0.8313850164413452\n",
            "For Step: 46680 recon_loss: 0.16983358561992645 \tdiscriminator_loss: 1.2006815671920776 \tgenerator_loss: 0.8300268054008484\n",
            "For Step: 46690 recon_loss: 0.17903456091880798 \tdiscriminator_loss: 1.2855441570281982 \tgenerator_loss: 0.8451399207115173\n",
            "For Step: 46700 recon_loss: 0.1791839599609375 \tdiscriminator_loss: 1.261275291442871 \tgenerator_loss: 0.8385181427001953\n",
            "For Step: 46710 recon_loss: 0.16970661282539368 \tdiscriminator_loss: 1.2789098024368286 \tgenerator_loss: 0.8244278430938721\n",
            "For Step: 46720 recon_loss: 0.18598023056983948 \tdiscriminator_loss: 1.2686156034469604 \tgenerator_loss: 0.8251688480377197\n",
            "For Step: 46730 recon_loss: 0.16878466308116913 \tdiscriminator_loss: 1.2317743301391602 \tgenerator_loss: 0.8584684133529663\n",
            "For Step: 46740 recon_loss: 0.17657199501991272 \tdiscriminator_loss: 1.3459669351577759 \tgenerator_loss: 0.8087348937988281\n",
            "For Step: 46750 recon_loss: 0.1706952005624771 \tdiscriminator_loss: 1.3555138111114502 \tgenerator_loss: 0.7795426845550537\n",
            "For Step: 46760 recon_loss: 0.16479308903217316 \tdiscriminator_loss: 1.3214778900146484 \tgenerator_loss: 0.798889696598053\n",
            "For Step: 46770 recon_loss: 0.1654093861579895 \tdiscriminator_loss: 1.2864105701446533 \tgenerator_loss: 0.8280129432678223\n",
            "For Step: 46780 recon_loss: 0.16851477324962616 \tdiscriminator_loss: 1.336554765701294 \tgenerator_loss: 0.7994746565818787\n",
            "For Step: 46790 recon_loss: 0.17148877680301666 \tdiscriminator_loss: 1.2619009017944336 \tgenerator_loss: 0.8290165066719055\n",
            "For Step: 46800 recon_loss: 0.1773325651884079 \tdiscriminator_loss: 1.2922933101654053 \tgenerator_loss: 0.8121400475502014\n",
            "For Step: 46810 recon_loss: 0.17807075381278992 \tdiscriminator_loss: 1.275360345840454 \tgenerator_loss: 0.8394030332565308\n",
            "For Step: 46820 recon_loss: 0.15012428164482117 \tdiscriminator_loss: 1.3558675050735474 \tgenerator_loss: 0.7895853519439697\n",
            "For Step: 46830 recon_loss: 0.15761937201023102 \tdiscriminator_loss: 1.2603156566619873 \tgenerator_loss: 0.8160197734832764\n",
            "For Step: 46840 recon_loss: 0.18266533315181732 \tdiscriminator_loss: 1.2832036018371582 \tgenerator_loss: 0.8168284296989441\n",
            "For Step: 46850 recon_loss: 0.16655413806438446 \tdiscriminator_loss: 1.2355130910873413 \tgenerator_loss: 0.8012436032295227\n",
            "For Step: 46860 recon_loss: 0.1733558475971222 \tdiscriminator_loss: 1.3592716455459595 \tgenerator_loss: 0.7612869739532471\n",
            "For Step: 46870 recon_loss: 0.18636874854564667 \tdiscriminator_loss: 1.3280774354934692 \tgenerator_loss: 0.8109233379364014\n",
            "For Step: 46880 recon_loss: 0.16315901279449463 \tdiscriminator_loss: 1.283892035484314 \tgenerator_loss: 0.7893341779708862\n",
            "For Step: 46890 recon_loss: 0.16360685229301453 \tdiscriminator_loss: 1.3249616622924805 \tgenerator_loss: 0.7553359866142273\n",
            "For Step: 46900 recon_loss: 0.16751450300216675 \tdiscriminator_loss: 1.330657720565796 \tgenerator_loss: 0.7872145771980286\n",
            "For Step: 46910 recon_loss: 0.16761016845703125 \tdiscriminator_loss: 1.279707908630371 \tgenerator_loss: 0.8277673125267029\n",
            "For Step: 46920 recon_loss: 0.16637343168258667 \tdiscriminator_loss: 1.3666036128997803 \tgenerator_loss: 0.7586584091186523\n",
            "For Step: 46930 recon_loss: 0.17617185413837433 \tdiscriminator_loss: 1.2611037492752075 \tgenerator_loss: 0.7838789820671082\n",
            "For Step: 46940 recon_loss: 0.16816870868206024 \tdiscriminator_loss: 1.2552447319030762 \tgenerator_loss: 0.8144782781600952\n",
            "For Step: 46950 recon_loss: 0.17607206106185913 \tdiscriminator_loss: 1.3577306270599365 \tgenerator_loss: 0.7912920713424683\n",
            "For Step: 46960 recon_loss: 0.16508328914642334 \tdiscriminator_loss: 1.212902545928955 \tgenerator_loss: 0.8053427934646606\n",
            "For Step: 46970 recon_loss: 0.17514614760875702 \tdiscriminator_loss: 1.2733969688415527 \tgenerator_loss: 0.7964332103729248\n",
            "For Step: 46980 recon_loss: 0.17557086050510406 \tdiscriminator_loss: 1.3212249279022217 \tgenerator_loss: 0.8072265386581421\n",
            "For Step: 46990 recon_loss: 0.17266979813575745 \tdiscriminator_loss: 1.3469785451889038 \tgenerator_loss: 0.7310673594474792\n",
            "For Step: 47000 recon_loss: 0.17525385320186615 \tdiscriminator_loss: 1.3144946098327637 \tgenerator_loss: 0.809333086013794\n",
            "For Step: 47010 recon_loss: 0.1757776439189911 \tdiscriminator_loss: 1.2760250568389893 \tgenerator_loss: 0.8258705139160156\n",
            "For Step: 47020 recon_loss: 0.17215117812156677 \tdiscriminator_loss: 1.281674861907959 \tgenerator_loss: 0.8358564376831055\n",
            "For Step: 47030 recon_loss: 0.17801393568515778 \tdiscriminator_loss: 1.344292402267456 \tgenerator_loss: 0.8326723575592041\n",
            "For Step: 47040 recon_loss: 0.16154658794403076 \tdiscriminator_loss: 1.4303488731384277 \tgenerator_loss: 0.7572883367538452\n",
            "For Step: 47050 recon_loss: 0.17339332401752472 \tdiscriminator_loss: 1.2983182668685913 \tgenerator_loss: 0.7767634391784668\n",
            "For Step: 47060 recon_loss: 0.16796119511127472 \tdiscriminator_loss: 1.3769687414169312 \tgenerator_loss: 0.7819815874099731\n",
            "For Step: 47070 recon_loss: 0.18523897230625153 \tdiscriminator_loss: 1.3031959533691406 \tgenerator_loss: 0.8081525564193726\n",
            "For Step: 47080 recon_loss: 0.17853128910064697 \tdiscriminator_loss: 1.3680142164230347 \tgenerator_loss: 0.7765321731567383\n",
            "For Step: 47090 recon_loss: 0.16734223067760468 \tdiscriminator_loss: 1.2558951377868652 \tgenerator_loss: 0.7860463857650757\n",
            "For Step: 47100 recon_loss: 0.17187729477882385 \tdiscriminator_loss: 1.3186450004577637 \tgenerator_loss: 0.7954086661338806\n",
            "For Step: 47110 recon_loss: 0.16758932173252106 \tdiscriminator_loss: 1.3139657974243164 \tgenerator_loss: 0.8043950796127319\n",
            "For Step: 47120 recon_loss: 0.17394205927848816 \tdiscriminator_loss: 1.2405574321746826 \tgenerator_loss: 0.8334407806396484\n",
            "For Step: 47130 recon_loss: 0.17083920538425446 \tdiscriminator_loss: 1.4022560119628906 \tgenerator_loss: 0.7482013702392578\n",
            "For Step: 47140 recon_loss: 0.16428764164447784 \tdiscriminator_loss: 1.3501845598220825 \tgenerator_loss: 0.7933971881866455\n",
            "For Step: 47150 recon_loss: 0.17024575173854828 \tdiscriminator_loss: 1.2446825504302979 \tgenerator_loss: 0.8164162635803223\n",
            "For Step: 47160 recon_loss: 0.1602516770362854 \tdiscriminator_loss: 1.402928352355957 \tgenerator_loss: 0.7417671084403992\n",
            "For Step: 47170 recon_loss: 0.16962482035160065 \tdiscriminator_loss: 1.2842631340026855 \tgenerator_loss: 0.8048303127288818\n",
            "For Step: 47180 recon_loss: 0.17811763286590576 \tdiscriminator_loss: 1.320641279220581 \tgenerator_loss: 0.8042532205581665\n",
            "For Step: 47190 recon_loss: 0.16851215064525604 \tdiscriminator_loss: 1.2422550916671753 \tgenerator_loss: 0.8283427953720093\n",
            "For Step: 47200 recon_loss: 0.18314634263515472 \tdiscriminator_loss: 1.3186217546463013 \tgenerator_loss: 0.7993501424789429\n",
            "For Step: 47210 recon_loss: 0.17143219709396362 \tdiscriminator_loss: 1.3885141611099243 \tgenerator_loss: 0.7653091549873352\n",
            "For Step: 47220 recon_loss: 0.17075367271900177 \tdiscriminator_loss: 1.333361268043518 \tgenerator_loss: 0.7765616774559021\n",
            "For Step: 47230 recon_loss: 0.17159031331539154 \tdiscriminator_loss: 1.3042271137237549 \tgenerator_loss: 0.8115674257278442\n",
            "For Step: 47240 recon_loss: 0.1802242249250412 \tdiscriminator_loss: 1.3132874965667725 \tgenerator_loss: 0.7663352489471436\n",
            "For Step: 47250 recon_loss: 0.16231736540794373 \tdiscriminator_loss: 1.2585148811340332 \tgenerator_loss: 0.8102170825004578\n",
            "For Step: 47260 recon_loss: 0.16677288711071014 \tdiscriminator_loss: 1.2629284858703613 \tgenerator_loss: 0.8217982649803162\n",
            "For Step: 47270 recon_loss: 0.16894544661045074 \tdiscriminator_loss: 1.2788318395614624 \tgenerator_loss: 0.8318076133728027\n",
            "For Step: 47280 recon_loss: 0.180535688996315 \tdiscriminator_loss: 1.2875239849090576 \tgenerator_loss: 0.7926459908485413\n",
            "For Step: 47290 recon_loss: 0.17480672895908356 \tdiscriminator_loss: 1.3499536514282227 \tgenerator_loss: 0.8024300336837769\n",
            "For Step: 47300 recon_loss: 0.16220393776893616 \tdiscriminator_loss: 1.31891930103302 \tgenerator_loss: 0.8344297409057617\n",
            "For Step: 47310 recon_loss: 0.1703534871339798 \tdiscriminator_loss: 1.3082854747772217 \tgenerator_loss: 0.7558428049087524\n",
            "For Step: 47320 recon_loss: 0.18067917227745056 \tdiscriminator_loss: 1.3158810138702393 \tgenerator_loss: 0.8010261058807373\n",
            "For Step: 47330 recon_loss: 0.16774874925613403 \tdiscriminator_loss: 1.2130119800567627 \tgenerator_loss: 0.8223931193351746\n",
            "For Step: 47340 recon_loss: 0.17396648228168488 \tdiscriminator_loss: 1.3428478240966797 \tgenerator_loss: 0.814915657043457\n",
            "For Step: 47350 recon_loss: 0.16831283271312714 \tdiscriminator_loss: 1.2976267337799072 \tgenerator_loss: 0.8081408739089966\n",
            "For Step: 47360 recon_loss: 0.18070518970489502 \tdiscriminator_loss: 1.2462146282196045 \tgenerator_loss: 0.7707350254058838\n",
            "For Step: 47370 recon_loss: 0.1700318604707718 \tdiscriminator_loss: 1.374802589416504 \tgenerator_loss: 0.7871787548065186\n",
            "For Step: 47380 recon_loss: 0.16860651969909668 \tdiscriminator_loss: 1.247327446937561 \tgenerator_loss: 0.8423161506652832\n",
            "For Step: 47390 recon_loss: 0.171473428606987 \tdiscriminator_loss: 1.3318908214569092 \tgenerator_loss: 0.7490426301956177\n",
            "For Step: 47400 recon_loss: 0.18181025981903076 \tdiscriminator_loss: 1.3361594676971436 \tgenerator_loss: 0.7912276983261108\n",
            "For Step: 47410 recon_loss: 0.17940974235534668 \tdiscriminator_loss: 1.2896409034729004 \tgenerator_loss: 0.843968391418457\n",
            "For Step: 47420 recon_loss: 0.16978901624679565 \tdiscriminator_loss: 1.2878377437591553 \tgenerator_loss: 0.8259739875793457\n",
            "For Step: 47430 recon_loss: 0.16524840891361237 \tdiscriminator_loss: 1.3101482391357422 \tgenerator_loss: 0.8361140489578247\n",
            "For Step: 47440 recon_loss: 0.17359988391399384 \tdiscriminator_loss: 1.3817418813705444 \tgenerator_loss: 0.8042476177215576\n",
            "For Step: 47450 recon_loss: 0.18447314202785492 \tdiscriminator_loss: 1.2965362071990967 \tgenerator_loss: 0.8106017708778381\n",
            "For Step: 47460 recon_loss: 0.16289830207824707 \tdiscriminator_loss: 1.2333307266235352 \tgenerator_loss: 0.8267541527748108\n",
            "For Step: 47470 recon_loss: 0.17251956462860107 \tdiscriminator_loss: 1.2678728103637695 \tgenerator_loss: 0.7912221550941467\n",
            "For Step: 47480 recon_loss: 0.16159911453723907 \tdiscriminator_loss: 1.2295045852661133 \tgenerator_loss: 0.8269177675247192\n",
            "For Step: 47490 recon_loss: 0.16818642616271973 \tdiscriminator_loss: 1.360522985458374 \tgenerator_loss: 0.7481690049171448\n",
            "For Step: 47500 recon_loss: 0.18095864355564117 \tdiscriminator_loss: 1.288669466972351 \tgenerator_loss: 0.7715176343917847\n",
            "For Step: 47510 recon_loss: 0.1744706630706787 \tdiscriminator_loss: 1.3357034921646118 \tgenerator_loss: 0.8025088906288147\n",
            "For Step: 47520 recon_loss: 0.1814575046300888 \tdiscriminator_loss: 1.347965955734253 \tgenerator_loss: 0.7943333387374878\n",
            "For Step: 47530 recon_loss: 0.1687684953212738 \tdiscriminator_loss: 1.3136565685272217 \tgenerator_loss: 0.816921055316925\n",
            "For Step: 47540 recon_loss: 0.179490864276886 \tdiscriminator_loss: 1.3202507495880127 \tgenerator_loss: 0.8400941491127014\n",
            "For Step: 47550 recon_loss: 0.18393205106258392 \tdiscriminator_loss: 1.372096061706543 \tgenerator_loss: 0.8074027299880981\n",
            "For Step: 47560 recon_loss: 0.17888164520263672 \tdiscriminator_loss: 1.358637809753418 \tgenerator_loss: 0.8184453248977661\n",
            "For Step: 47570 recon_loss: 0.17482833564281464 \tdiscriminator_loss: 1.2870075702667236 \tgenerator_loss: 0.8237795233726501\n",
            "For Step: 47580 recon_loss: 0.1590486764907837 \tdiscriminator_loss: 1.3877062797546387 \tgenerator_loss: 0.7637150287628174\n",
            "For Step: 47590 recon_loss: 0.16816240549087524 \tdiscriminator_loss: 1.3462085723876953 \tgenerator_loss: 0.7992376089096069\n",
            "For Step: 47600 recon_loss: 0.17508070170879364 \tdiscriminator_loss: 1.2930116653442383 \tgenerator_loss: 0.8349420428276062\n",
            "For Step: 47610 recon_loss: 0.16780316829681396 \tdiscriminator_loss: 1.2078102827072144 \tgenerator_loss: 0.832992434501648\n",
            "For Step: 47620 recon_loss: 0.16991527378559113 \tdiscriminator_loss: 1.2353869676589966 \tgenerator_loss: 0.8213046789169312\n",
            "For Step: 47630 recon_loss: 0.17565378546714783 \tdiscriminator_loss: 1.3438563346862793 \tgenerator_loss: 0.8024587035179138\n",
            "For Step: 47640 recon_loss: 0.19077938795089722 \tdiscriminator_loss: 1.2915972471237183 \tgenerator_loss: 0.8285434246063232\n",
            "For Step: 47650 recon_loss: 0.16586816310882568 \tdiscriminator_loss: 1.3209232091903687 \tgenerator_loss: 0.7679839134216309\n",
            "For Step: 47660 recon_loss: 0.17570921778678894 \tdiscriminator_loss: 1.2657675743103027 \tgenerator_loss: 0.821670413017273\n",
            "For Step: 47670 recon_loss: 0.16165569424629211 \tdiscriminator_loss: 1.2928401231765747 \tgenerator_loss: 0.8018121123313904\n",
            "For Step: 47680 recon_loss: 0.1614149808883667 \tdiscriminator_loss: 1.3899767398834229 \tgenerator_loss: 0.8067356944084167\n",
            "For Step: 47690 recon_loss: 0.16657504439353943 \tdiscriminator_loss: 1.2109359502792358 \tgenerator_loss: 0.8281898498535156\n",
            "For Step: 47700 recon_loss: 0.1641872376203537 \tdiscriminator_loss: 1.304687738418579 \tgenerator_loss: 0.8120695948600769\n",
            "For Step: 47710 recon_loss: 0.180760458111763 \tdiscriminator_loss: 1.3730456829071045 \tgenerator_loss: 0.7519961595535278\n",
            "For Step: 47720 recon_loss: 0.16370078921318054 \tdiscriminator_loss: 1.2882896661758423 \tgenerator_loss: 0.8124613761901855\n",
            "For Step: 47730 recon_loss: 0.1720031052827835 \tdiscriminator_loss: 1.320426344871521 \tgenerator_loss: 0.8227518796920776\n",
            "For Step: 47740 recon_loss: 0.1701073795557022 \tdiscriminator_loss: 1.2340868711471558 \tgenerator_loss: 0.8209918737411499\n",
            "For Step: 47750 recon_loss: 0.17730145156383514 \tdiscriminator_loss: 1.3552898168563843 \tgenerator_loss: 0.7825552225112915\n",
            "For Step: 47760 recon_loss: 0.17084795236587524 \tdiscriminator_loss: 1.3290704488754272 \tgenerator_loss: 0.8122485876083374\n",
            "For Step: 47770 recon_loss: 0.16264450550079346 \tdiscriminator_loss: 1.2809332609176636 \tgenerator_loss: 0.8465216159820557\n",
            "For Step: 47780 recon_loss: 0.1722397357225418 \tdiscriminator_loss: 1.3239023685455322 \tgenerator_loss: 0.8062169551849365\n",
            "For Step: 47790 recon_loss: 0.16980011761188507 \tdiscriminator_loss: 1.3009355068206787 \tgenerator_loss: 0.8019301891326904\n",
            "For Step: 47800 recon_loss: 0.16766804456710815 \tdiscriminator_loss: 1.2905023097991943 \tgenerator_loss: 0.7867861986160278\n",
            "For Step: 47810 recon_loss: 0.17039497196674347 \tdiscriminator_loss: 1.3505523204803467 \tgenerator_loss: 0.7865190505981445\n",
            "For Step: 47820 recon_loss: 0.18748724460601807 \tdiscriminator_loss: 1.3333455324172974 \tgenerator_loss: 0.7931883335113525\n",
            "For Step: 47830 recon_loss: 0.16211363673210144 \tdiscriminator_loss: 1.2783783674240112 \tgenerator_loss: 0.8339689373970032\n",
            "For Step: 47840 recon_loss: 0.17592772841453552 \tdiscriminator_loss: 1.2285640239715576 \tgenerator_loss: 0.8620086908340454\n",
            "For Step: 47850 recon_loss: 0.16681978106498718 \tdiscriminator_loss: 1.3794500827789307 \tgenerator_loss: 0.8082101345062256\n",
            "For Step: 47860 recon_loss: 0.17214736342430115 \tdiscriminator_loss: 1.3250739574432373 \tgenerator_loss: 0.8107184171676636\n",
            "For Step: 47870 recon_loss: 0.17355215549468994 \tdiscriminator_loss: 1.3924225568771362 \tgenerator_loss: 0.8242340683937073\n",
            "For Step: 47880 recon_loss: 0.1745131015777588 \tdiscriminator_loss: 1.4265899658203125 \tgenerator_loss: 0.7232701778411865\n",
            "For Step: 47890 recon_loss: 0.17379765212535858 \tdiscriminator_loss: 1.4394015073776245 \tgenerator_loss: 0.778989315032959\n",
            "For Step: 47900 recon_loss: 0.17332468926906586 \tdiscriminator_loss: 1.326374888420105 \tgenerator_loss: 0.7972463369369507\n",
            "For Step: 47910 recon_loss: 0.1666705310344696 \tdiscriminator_loss: 1.2829465866088867 \tgenerator_loss: 0.8282663822174072\n",
            "For Step: 47920 recon_loss: 0.1709953248500824 \tdiscriminator_loss: 1.2831202745437622 \tgenerator_loss: 0.8354324102401733\n",
            "For Step: 47930 recon_loss: 0.17134565114974976 \tdiscriminator_loss: 1.321004033088684 \tgenerator_loss: 0.7666352987289429\n",
            "For Step: 47940 recon_loss: 0.1600753366947174 \tdiscriminator_loss: 1.3919878005981445 \tgenerator_loss: 0.7715437412261963\n",
            "For Step: 47950 recon_loss: 0.1759844273328781 \tdiscriminator_loss: 1.3365648984909058 \tgenerator_loss: 0.7929638624191284\n",
            "For Step: 47960 recon_loss: 0.17653048038482666 \tdiscriminator_loss: 1.2835586071014404 \tgenerator_loss: 0.8322404026985168\n",
            "For Step: 47970 recon_loss: 0.177962064743042 \tdiscriminator_loss: 1.3378161191940308 \tgenerator_loss: 0.7912940979003906\n",
            "For Step: 47980 recon_loss: 0.16411839425563812 \tdiscriminator_loss: 1.3733948469161987 \tgenerator_loss: 0.7562687993049622\n",
            "For Step: 47990 recon_loss: 0.16301631927490234 \tdiscriminator_loss: 1.3463120460510254 \tgenerator_loss: 0.7696806192398071\n",
            "For Step: 48000 recon_loss: 0.18006259202957153 \tdiscriminator_loss: 1.3268109560012817 \tgenerator_loss: 0.8545415997505188\n",
            "For Step: 48010 recon_loss: 0.1675526201725006 \tdiscriminator_loss: 1.416624665260315 \tgenerator_loss: 0.7522546052932739\n",
            "For Step: 48020 recon_loss: 0.18725259602069855 \tdiscriminator_loss: 1.3686978816986084 \tgenerator_loss: 0.7805715799331665\n",
            "For Step: 48030 recon_loss: 0.18957091867923737 \tdiscriminator_loss: 1.3714003562927246 \tgenerator_loss: 0.7782943248748779\n",
            "For Step: 48040 recon_loss: 0.17329050600528717 \tdiscriminator_loss: 1.3533351421356201 \tgenerator_loss: 0.7903130054473877\n",
            "For Step: 48050 recon_loss: 0.16055969893932343 \tdiscriminator_loss: 1.2863003015518188 \tgenerator_loss: 0.7981115579605103\n",
            "For Step: 48060 recon_loss: 0.17976826429367065 \tdiscriminator_loss: 1.404693603515625 \tgenerator_loss: 0.77457195520401\n",
            "For Step: 48070 recon_loss: 0.16723231971263885 \tdiscriminator_loss: 1.3239260911941528 \tgenerator_loss: 0.7882283926010132\n",
            "For Step: 48080 recon_loss: 0.1701841503381729 \tdiscriminator_loss: 1.3763427734375 \tgenerator_loss: 0.7689297795295715\n",
            "For Step: 48090 recon_loss: 0.16926153004169464 \tdiscriminator_loss: 1.2430671453475952 \tgenerator_loss: 0.80913245677948\n",
            "For Step: 48100 recon_loss: 0.1654900461435318 \tdiscriminator_loss: 1.3454625606536865 \tgenerator_loss: 0.7731181383132935\n",
            "For Step: 48110 recon_loss: 0.18033745884895325 \tdiscriminator_loss: 1.2925288677215576 \tgenerator_loss: 0.767947256565094\n",
            "For Step: 48120 recon_loss: 0.17762264609336853 \tdiscriminator_loss: 1.3129665851593018 \tgenerator_loss: 0.8132383823394775\n",
            "For Step: 48130 recon_loss: 0.18488915264606476 \tdiscriminator_loss: 1.31216299533844 \tgenerator_loss: 0.8073657751083374\n",
            "For Step: 48140 recon_loss: 0.16822661459445953 \tdiscriminator_loss: 1.2769923210144043 \tgenerator_loss: 0.7803905010223389\n",
            "For Step: 48150 recon_loss: 0.17256875336170197 \tdiscriminator_loss: 1.3196113109588623 \tgenerator_loss: 0.7672595977783203\n",
            "For Step: 48160 recon_loss: 0.17621268332004547 \tdiscriminator_loss: 1.3671071529388428 \tgenerator_loss: 0.8199834823608398\n",
            "For Step: 48170 recon_loss: 0.16690891981124878 \tdiscriminator_loss: 1.3166453838348389 \tgenerator_loss: 0.7790400981903076\n",
            "For Step: 48180 recon_loss: 0.1711612045764923 \tdiscriminator_loss: 1.3379251956939697 \tgenerator_loss: 0.7658458948135376\n",
            "For Step: 48190 recon_loss: 0.17721869051456451 \tdiscriminator_loss: 1.2775458097457886 \tgenerator_loss: 0.7939707636833191\n",
            "For Step: 48200 recon_loss: 0.17547009885311127 \tdiscriminator_loss: 1.3965177536010742 \tgenerator_loss: 0.7532763481140137\n",
            "For Step: 48210 recon_loss: 0.1703660488128662 \tdiscriminator_loss: 1.2592540979385376 \tgenerator_loss: 0.8124407529830933\n",
            "For Step: 48220 recon_loss: 0.17997577786445618 \tdiscriminator_loss: 1.3321470022201538 \tgenerator_loss: 0.7926493883132935\n",
            "For Step: 48230 recon_loss: 0.16564667224884033 \tdiscriminator_loss: 1.3347607851028442 \tgenerator_loss: 0.7772901058197021\n",
            "For Step: 48240 recon_loss: 0.17192427814006805 \tdiscriminator_loss: 1.304431676864624 \tgenerator_loss: 0.7745835185050964\n",
            "For Step: 48250 recon_loss: 0.16499651968479156 \tdiscriminator_loss: 1.2443732023239136 \tgenerator_loss: 0.8475465178489685\n",
            "For Step: 48260 recon_loss: 0.18121866881847382 \tdiscriminator_loss: 1.3389891386032104 \tgenerator_loss: 0.8578517436981201\n",
            "For Step: 48270 recon_loss: 0.17612819373607635 \tdiscriminator_loss: 1.2898927927017212 \tgenerator_loss: 0.8476973176002502\n",
            "For Step: 48280 recon_loss: 0.16528207063674927 \tdiscriminator_loss: 1.2260828018188477 \tgenerator_loss: 0.8297702670097351\n",
            "For Step: 48290 recon_loss: 0.16527658700942993 \tdiscriminator_loss: 1.3278642892837524 \tgenerator_loss: 0.8403019905090332\n",
            "For Step: 48300 recon_loss: 0.16963404417037964 \tdiscriminator_loss: 1.3634260892868042 \tgenerator_loss: 0.7660877704620361\n",
            "For Step: 48310 recon_loss: 0.17942728102207184 \tdiscriminator_loss: 1.3172450065612793 \tgenerator_loss: 0.8200538158416748\n",
            "For Step: 48320 recon_loss: 0.1739206165075302 \tdiscriminator_loss: 1.2750165462493896 \tgenerator_loss: 0.7733923196792603\n",
            "For Step: 48330 recon_loss: 0.1709427684545517 \tdiscriminator_loss: 1.3995386362075806 \tgenerator_loss: 0.7663506269454956\n",
            "For Step: 48340 recon_loss: 0.17112606763839722 \tdiscriminator_loss: 1.3023030757904053 \tgenerator_loss: 0.8093663454055786\n",
            "For Step: 48350 recon_loss: 0.17097939550876617 \tdiscriminator_loss: 1.2681858539581299 \tgenerator_loss: 0.842751145362854\n",
            "For Step: 48360 recon_loss: 0.17116014659404755 \tdiscriminator_loss: 1.340315341949463 \tgenerator_loss: 0.7951534986495972\n",
            "For Step: 48370 recon_loss: 0.17421074211597443 \tdiscriminator_loss: 1.2834117412567139 \tgenerator_loss: 0.8321353793144226\n",
            "For Step: 48380 recon_loss: 0.17142897844314575 \tdiscriminator_loss: 1.2581760883331299 \tgenerator_loss: 0.8430207371711731\n",
            "For Step: 48390 recon_loss: 0.17231673002243042 \tdiscriminator_loss: 1.2973556518554688 \tgenerator_loss: 0.8506938815116882\n",
            "For Step: 48400 recon_loss: 0.18262530863285065 \tdiscriminator_loss: 1.3843786716461182 \tgenerator_loss: 0.765731692314148\n",
            "For Step: 48410 recon_loss: 0.17819353938102722 \tdiscriminator_loss: 1.3421952724456787 \tgenerator_loss: 0.7672572135925293\n",
            "For Step: 48420 recon_loss: 0.17779244482517242 \tdiscriminator_loss: 1.3539104461669922 \tgenerator_loss: 0.792038083076477\n",
            "For Step: 48430 recon_loss: 0.17411811649799347 \tdiscriminator_loss: 1.405759334564209 \tgenerator_loss: 0.7350353002548218\n",
            "For Step: 48440 recon_loss: 0.18545295298099518 \tdiscriminator_loss: 1.2494370937347412 \tgenerator_loss: 0.7991719841957092\n",
            "For Step: 48450 recon_loss: 0.17228858172893524 \tdiscriminator_loss: 1.32379150390625 \tgenerator_loss: 0.7987833619117737\n",
            "For Step: 48460 recon_loss: 0.15559038519859314 \tdiscriminator_loss: 1.2996978759765625 \tgenerator_loss: 0.8038231134414673\n",
            "For Step: 48470 recon_loss: 0.17201800644397736 \tdiscriminator_loss: 1.3689777851104736 \tgenerator_loss: 0.835344672203064\n",
            "For Step: 48480 recon_loss: 0.16212743520736694 \tdiscriminator_loss: 1.2906336784362793 \tgenerator_loss: 0.8066627979278564\n",
            "For Step: 48490 recon_loss: 0.17521274089813232 \tdiscriminator_loss: 1.2629477977752686 \tgenerator_loss: 0.8344076871871948\n",
            "For Step: 48500 recon_loss: 0.16615793108940125 \tdiscriminator_loss: 1.2208564281463623 \tgenerator_loss: 0.7992938756942749\n",
            "For Step: 48510 recon_loss: 0.1750941127538681 \tdiscriminator_loss: 1.323143720626831 \tgenerator_loss: 0.7990288734436035\n",
            "For Step: 48520 recon_loss: 0.1600589007139206 \tdiscriminator_loss: 1.2944259643554688 \tgenerator_loss: 0.7921257019042969\n",
            "For Step: 48530 recon_loss: 0.16828995943069458 \tdiscriminator_loss: 1.3986172676086426 \tgenerator_loss: 0.7509723901748657\n",
            "For Step: 48540 recon_loss: 0.180911585688591 \tdiscriminator_loss: 1.2812793254852295 \tgenerator_loss: 0.7801539897918701\n",
            "For Step: 48550 recon_loss: 0.17046551406383514 \tdiscriminator_loss: 1.3075156211853027 \tgenerator_loss: 0.7969694137573242\n",
            "For Step: 48560 recon_loss: 0.16148832440376282 \tdiscriminator_loss: 1.3948643207550049 \tgenerator_loss: 0.8037329912185669\n",
            "For Step: 48570 recon_loss: 0.17969825863838196 \tdiscriminator_loss: 1.2474976778030396 \tgenerator_loss: 0.8294036388397217\n",
            "For Step: 48580 recon_loss: 0.15668489038944244 \tdiscriminator_loss: 1.2479476928710938 \tgenerator_loss: 0.8216167688369751\n",
            "For Step: 48590 recon_loss: 0.16831853985786438 \tdiscriminator_loss: 1.2777670621871948 \tgenerator_loss: 0.8077663779258728\n",
            "For Step: 48600 recon_loss: 0.1796485334634781 \tdiscriminator_loss: 1.2492051124572754 \tgenerator_loss: 0.7990942001342773\n",
            "For Step: 48610 recon_loss: 0.17432014644145966 \tdiscriminator_loss: 1.36716890335083 \tgenerator_loss: 0.796542227268219\n",
            "For Step: 48620 recon_loss: 0.15619337558746338 \tdiscriminator_loss: 1.3154127597808838 \tgenerator_loss: 0.7874755859375\n",
            "For Step: 48630 recon_loss: 0.1651555448770523 \tdiscriminator_loss: 1.3202595710754395 \tgenerator_loss: 0.7736427783966064\n",
            "For Step: 48640 recon_loss: 0.17003706097602844 \tdiscriminator_loss: 1.324422836303711 \tgenerator_loss: 0.7962139844894409\n",
            "For Step: 48650 recon_loss: 0.1742924600839615 \tdiscriminator_loss: 1.3747236728668213 \tgenerator_loss: 0.7815284729003906\n",
            "For Step: 48660 recon_loss: 0.1766049563884735 \tdiscriminator_loss: 1.312760353088379 \tgenerator_loss: 0.774603545665741\n",
            "For Step: 48670 recon_loss: 0.16708247363567352 \tdiscriminator_loss: 1.2715075016021729 \tgenerator_loss: 0.8245865106582642\n",
            "For Step: 48680 recon_loss: 0.17422722280025482 \tdiscriminator_loss: 1.3210779428482056 \tgenerator_loss: 0.8113527894020081\n",
            "For Step: 48690 recon_loss: 0.17323167622089386 \tdiscriminator_loss: 1.3819324970245361 \tgenerator_loss: 0.7551204562187195\n",
            "For Step: 48700 recon_loss: 0.16363465785980225 \tdiscriminator_loss: 1.3021366596221924 \tgenerator_loss: 0.8174753785133362\n",
            "For Step: 48710 recon_loss: 0.16487815976142883 \tdiscriminator_loss: 1.3477298021316528 \tgenerator_loss: 0.7891470193862915\n",
            "For Step: 48720 recon_loss: 0.17204603552818298 \tdiscriminator_loss: 1.3851356506347656 \tgenerator_loss: 0.7751126885414124\n",
            "For Step: 48730 recon_loss: 0.18018095195293427 \tdiscriminator_loss: 1.3547054529190063 \tgenerator_loss: 0.7499822974205017\n",
            "For Step: 48740 recon_loss: 0.16615739464759827 \tdiscriminator_loss: 1.2431964874267578 \tgenerator_loss: 0.8137755990028381\n",
            "For Step: 48750 recon_loss: 0.1548846960067749 \tdiscriminator_loss: 1.3320989608764648 \tgenerator_loss: 0.779138445854187\n",
            "For Step: 48760 recon_loss: 0.17038047313690186 \tdiscriminator_loss: 1.3065379858016968 \tgenerator_loss: 0.7870127558708191\n",
            "For Step: 48770 recon_loss: 0.17341913282871246 \tdiscriminator_loss: 1.315259337425232 \tgenerator_loss: 0.7835561037063599\n",
            "For Step: 48780 recon_loss: 0.1857781857252121 \tdiscriminator_loss: 1.289526104927063 \tgenerator_loss: 0.8272050619125366\n",
            "For Step: 48790 recon_loss: 0.15840111672878265 \tdiscriminator_loss: 1.3114385604858398 \tgenerator_loss: 0.7910964488983154\n",
            "For Step: 48800 recon_loss: 0.16545069217681885 \tdiscriminator_loss: 1.341734766960144 \tgenerator_loss: 0.7944029569625854\n",
            "For Step: 48810 recon_loss: 0.1737695038318634 \tdiscriminator_loss: 1.2619835138320923 \tgenerator_loss: 0.79444819688797\n",
            "For Step: 48820 recon_loss: 0.1892164796590805 \tdiscriminator_loss: 1.269620656967163 \tgenerator_loss: 0.83888840675354\n",
            "For Step: 48830 recon_loss: 0.15432734787464142 \tdiscriminator_loss: 1.2409045696258545 \tgenerator_loss: 0.8380696773529053\n",
            "For Step: 48840 recon_loss: 0.17695000767707825 \tdiscriminator_loss: 1.2649955749511719 \tgenerator_loss: 0.8285602331161499\n",
            "For Step: 48850 recon_loss: 0.16036124527454376 \tdiscriminator_loss: 1.4390079975128174 \tgenerator_loss: 0.7805696725845337\n",
            "For Step: 48860 recon_loss: 0.17102698981761932 \tdiscriminator_loss: 1.3798019886016846 \tgenerator_loss: 0.7887090444564819\n",
            "For Step: 48870 recon_loss: 0.16595005989074707 \tdiscriminator_loss: 1.3641338348388672 \tgenerator_loss: 0.7528061866760254\n",
            "For Step: 48880 recon_loss: 0.17559845745563507 \tdiscriminator_loss: 1.2729346752166748 \tgenerator_loss: 0.8297263383865356\n",
            "For Step: 48890 recon_loss: 0.14501531422138214 \tdiscriminator_loss: 1.3569422960281372 \tgenerator_loss: 0.8039790987968445\n",
            "For Step: 48900 recon_loss: 0.17288990318775177 \tdiscriminator_loss: 1.2552728652954102 \tgenerator_loss: 0.779000997543335\n",
            "For Step: 48910 recon_loss: 0.16668489575386047 \tdiscriminator_loss: 1.332339882850647 \tgenerator_loss: 0.7836145162582397\n",
            "For Step: 48920 recon_loss: 0.17261455953121185 \tdiscriminator_loss: 1.3290433883666992 \tgenerator_loss: 0.7769196033477783\n",
            "For Step: 48930 recon_loss: 0.17085649073123932 \tdiscriminator_loss: 1.3446296453475952 \tgenerator_loss: 0.7984905242919922\n",
            "For Step: 48940 recon_loss: 0.17780615389347076 \tdiscriminator_loss: 1.428922176361084 \tgenerator_loss: 0.730837345123291\n",
            "For Step: 48950 recon_loss: 0.17835694551467896 \tdiscriminator_loss: 1.278157353401184 \tgenerator_loss: 0.7782912254333496\n",
            "For Step: 48960 recon_loss: 0.1599910855293274 \tdiscriminator_loss: 1.3334996700286865 \tgenerator_loss: 0.7786455750465393\n",
            "For Step: 48970 recon_loss: 0.17274139821529388 \tdiscriminator_loss: 1.290036916732788 \tgenerator_loss: 0.8177286386489868\n",
            "For Step: 48980 recon_loss: 0.16596567630767822 \tdiscriminator_loss: 1.3640751838684082 \tgenerator_loss: 0.7880961894989014\n",
            "For Step: 48990 recon_loss: 0.17057368159294128 \tdiscriminator_loss: 1.2971125841140747 \tgenerator_loss: 0.7972609400749207\n",
            "For Step: 49000 recon_loss: 0.17321644723415375 \tdiscriminator_loss: 1.352613925933838 \tgenerator_loss: 0.8041964769363403\n",
            "For Step: 49010 recon_loss: 0.19046059250831604 \tdiscriminator_loss: 1.2547340393066406 \tgenerator_loss: 0.8185651302337646\n",
            "For Step: 49020 recon_loss: 0.1789909303188324 \tdiscriminator_loss: 1.3287320137023926 \tgenerator_loss: 0.8192868232727051\n",
            "For Step: 49030 recon_loss: 0.1733088195323944 \tdiscriminator_loss: 1.2854764461517334 \tgenerator_loss: 0.8329752683639526\n",
            "For Step: 49040 recon_loss: 0.16520445048809052 \tdiscriminator_loss: 1.2867600917816162 \tgenerator_loss: 0.7888725399971008\n",
            "For Step: 49050 recon_loss: 0.1734713912010193 \tdiscriminator_loss: 1.2367401123046875 \tgenerator_loss: 0.8092750310897827\n",
            "For Step: 49060 recon_loss: 0.1737583726644516 \tdiscriminator_loss: 1.2957687377929688 \tgenerator_loss: 0.7900266647338867\n",
            "For Step: 49070 recon_loss: 0.166535884141922 \tdiscriminator_loss: 1.2049354314804077 \tgenerator_loss: 0.8423568606376648\n",
            "For Step: 49080 recon_loss: 0.17754286527633667 \tdiscriminator_loss: 1.2864878177642822 \tgenerator_loss: 0.8340235948562622\n",
            "For Step: 49090 recon_loss: 0.1697331964969635 \tdiscriminator_loss: 1.3025639057159424 \tgenerator_loss: 0.830815851688385\n",
            "For Step: 49100 recon_loss: 0.1795569807291031 \tdiscriminator_loss: 1.2778372764587402 \tgenerator_loss: 0.7858810424804688\n",
            "For Step: 49110 recon_loss: 0.1609663963317871 \tdiscriminator_loss: 1.280135989189148 \tgenerator_loss: 0.8120316863059998\n",
            "For Step: 49120 recon_loss: 0.17835389077663422 \tdiscriminator_loss: 1.288110613822937 \tgenerator_loss: 0.7944183945655823\n",
            "For Step: 49130 recon_loss: 0.17830605804920197 \tdiscriminator_loss: 1.3370639085769653 \tgenerator_loss: 0.784437894821167\n",
            "For Step: 49140 recon_loss: 0.16604244709014893 \tdiscriminator_loss: 1.3034197092056274 \tgenerator_loss: 0.7888534665107727\n",
            "For Step: 49150 recon_loss: 0.17368680238723755 \tdiscriminator_loss: 1.3065557479858398 \tgenerator_loss: 0.7848244905471802\n",
            "For Step: 49160 recon_loss: 0.17460297048091888 \tdiscriminator_loss: 1.3361022472381592 \tgenerator_loss: 0.7671006917953491\n",
            "For Step: 49170 recon_loss: 0.1731049120426178 \tdiscriminator_loss: 1.3245604038238525 \tgenerator_loss: 0.8001934289932251\n",
            "For Step: 49180 recon_loss: 0.1677955687046051 \tdiscriminator_loss: 1.2248167991638184 \tgenerator_loss: 0.8095765709877014\n",
            "For Step: 49190 recon_loss: 0.17147743701934814 \tdiscriminator_loss: 1.325382947921753 \tgenerator_loss: 0.8056901693344116\n",
            "For Step: 49200 recon_loss: 0.16878308355808258 \tdiscriminator_loss: 1.3579471111297607 \tgenerator_loss: 0.7616273164749146\n",
            "For Step: 49210 recon_loss: 0.15270791947841644 \tdiscriminator_loss: 1.272226333618164 \tgenerator_loss: 0.8030209541320801\n",
            "For Step: 49220 recon_loss: 0.16683052480220795 \tdiscriminator_loss: 1.3229318857192993 \tgenerator_loss: 0.8010623455047607\n",
            "For Step: 49230 recon_loss: 0.15547487139701843 \tdiscriminator_loss: 1.3120356798171997 \tgenerator_loss: 0.8116636872291565\n",
            "For Step: 49240 recon_loss: 0.16310617327690125 \tdiscriminator_loss: 1.3152549266815186 \tgenerator_loss: 0.8276159167289734\n",
            "For Step: 49250 recon_loss: 0.1746903508901596 \tdiscriminator_loss: 1.3061323165893555 \tgenerator_loss: 0.7939972281455994\n",
            "For Step: 49260 recon_loss: 0.16792705655097961 \tdiscriminator_loss: 1.320561170578003 \tgenerator_loss: 0.806827187538147\n",
            "For Step: 49270 recon_loss: 0.1805313676595688 \tdiscriminator_loss: 1.418574333190918 \tgenerator_loss: 0.7728954553604126\n",
            "For Step: 49280 recon_loss: 0.18015778064727783 \tdiscriminator_loss: 1.3721213340759277 \tgenerator_loss: 0.7855929732322693\n",
            "For Step: 49290 recon_loss: 0.16905276477336884 \tdiscriminator_loss: 1.2303986549377441 \tgenerator_loss: 0.7830959558486938\n",
            "For Step: 49300 recon_loss: 0.15763811767101288 \tdiscriminator_loss: 1.3170082569122314 \tgenerator_loss: 0.8016279935836792\n",
            "For Step: 49310 recon_loss: 0.1735135167837143 \tdiscriminator_loss: 1.2538325786590576 \tgenerator_loss: 0.8657883405685425\n",
            "For Step: 49320 recon_loss: 0.17288042604923248 \tdiscriminator_loss: 1.255460262298584 \tgenerator_loss: 0.8114765286445618\n",
            "For Step: 49330 recon_loss: 0.1660231351852417 \tdiscriminator_loss: 1.3007562160491943 \tgenerator_loss: 0.7388705015182495\n",
            "For Step: 49340 recon_loss: 0.1808723509311676 \tdiscriminator_loss: 1.3244363069534302 \tgenerator_loss: 0.8269521594047546\n",
            "For Step: 49350 recon_loss: 0.18332843482494354 \tdiscriminator_loss: 1.286204218864441 \tgenerator_loss: 0.7932815551757812\n",
            "For Step: 49360 recon_loss: 0.1692887842655182 \tdiscriminator_loss: 1.260549545288086 \tgenerator_loss: 0.7892956733703613\n",
            "For Step: 49370 recon_loss: 0.1631842404603958 \tdiscriminator_loss: 1.295882225036621 \tgenerator_loss: 0.8009556531906128\n",
            "For Step: 49380 recon_loss: 0.17064283788204193 \tdiscriminator_loss: 1.29566490650177 \tgenerator_loss: 0.8428804278373718\n",
            "For Step: 49390 recon_loss: 0.17021790146827698 \tdiscriminator_loss: 1.401342511177063 \tgenerator_loss: 0.7493980526924133\n",
            "For Step: 49400 recon_loss: 0.1532202512025833 \tdiscriminator_loss: 1.3833023309707642 \tgenerator_loss: 0.7820695638656616\n",
            "For Step: 49410 recon_loss: 0.1613387018442154 \tdiscriminator_loss: 1.3318694829940796 \tgenerator_loss: 0.7987753748893738\n",
            "For Step: 49420 recon_loss: 0.16657213866710663 \tdiscriminator_loss: 1.3654980659484863 \tgenerator_loss: 0.8058549165725708\n",
            "For Step: 49430 recon_loss: 0.16815285384655 \tdiscriminator_loss: 1.306532382965088 \tgenerator_loss: 0.7885806560516357\n",
            "For Step: 49440 recon_loss: 0.16481031477451324 \tdiscriminator_loss: 1.2700101137161255 \tgenerator_loss: 0.82760089635849\n",
            "For Step: 49450 recon_loss: 0.1744634211063385 \tdiscriminator_loss: 1.3129148483276367 \tgenerator_loss: 0.7947105169296265\n",
            "For Step: 49460 recon_loss: 0.17215822637081146 \tdiscriminator_loss: 1.2708936929702759 \tgenerator_loss: 0.8288991451263428\n",
            "For Step: 49470 recon_loss: 0.16157163679599762 \tdiscriminator_loss: 1.2812882661819458 \tgenerator_loss: 0.8164991140365601\n",
            "For Step: 49480 recon_loss: 0.1611526906490326 \tdiscriminator_loss: 1.2645620107650757 \tgenerator_loss: 0.837976336479187\n",
            "For Step: 49490 recon_loss: 0.15387821197509766 \tdiscriminator_loss: 1.2861567735671997 \tgenerator_loss: 0.789147675037384\n",
            "For Step: 49500 recon_loss: 0.17103105783462524 \tdiscriminator_loss: 1.3587794303894043 \tgenerator_loss: 0.7253227233886719\n",
            "For Step: 49510 recon_loss: 0.17262591421604156 \tdiscriminator_loss: 1.3770685195922852 \tgenerator_loss: 0.7670908570289612\n",
            "For Step: 49520 recon_loss: 0.16306230425834656 \tdiscriminator_loss: 1.2720434665679932 \tgenerator_loss: 0.7906455397605896\n",
            "For Step: 49530 recon_loss: 0.16242022812366486 \tdiscriminator_loss: 1.3465133905410767 \tgenerator_loss: 0.7690229415893555\n",
            "For Step: 49540 recon_loss: 0.16348488628864288 \tdiscriminator_loss: 1.4194605350494385 \tgenerator_loss: 0.7536199688911438\n",
            "For Step: 49550 recon_loss: 0.16002468764781952 \tdiscriminator_loss: 1.373443603515625 \tgenerator_loss: 0.7502192258834839\n",
            "For Step: 49560 recon_loss: 0.16800808906555176 \tdiscriminator_loss: 1.3230047225952148 \tgenerator_loss: 0.8178910613059998\n",
            "For Step: 49570 recon_loss: 0.16859790682792664 \tdiscriminator_loss: 1.295554757118225 \tgenerator_loss: 0.8529238700866699\n",
            "For Step: 49580 recon_loss: 0.18324367702007294 \tdiscriminator_loss: 1.2946374416351318 \tgenerator_loss: 0.7819554209709167\n",
            "For Step: 49590 recon_loss: 0.1644735187292099 \tdiscriminator_loss: 1.360644817352295 \tgenerator_loss: 0.7802859544754028\n",
            "For Step: 49600 recon_loss: 0.1574113368988037 \tdiscriminator_loss: 1.298693299293518 \tgenerator_loss: 0.7649568319320679\n",
            "For Step: 49610 recon_loss: 0.1635851114988327 \tdiscriminator_loss: 1.3412702083587646 \tgenerator_loss: 0.8305578827857971\n",
            "For Step: 49620 recon_loss: 0.16143308579921722 \tdiscriminator_loss: 1.3517173528671265 \tgenerator_loss: 0.7525443434715271\n",
            "For Step: 49630 recon_loss: 0.16253750026226044 \tdiscriminator_loss: 1.2876404523849487 \tgenerator_loss: 0.8273235559463501\n",
            "For Step: 49640 recon_loss: 0.18128523230552673 \tdiscriminator_loss: 1.337480902671814 \tgenerator_loss: 0.7786422967910767\n",
            "For Step: 49650 recon_loss: 0.16586807370185852 \tdiscriminator_loss: 1.3371390104293823 \tgenerator_loss: 0.7951217889785767\n",
            "For Step: 49660 recon_loss: 0.17505672574043274 \tdiscriminator_loss: 1.303892970085144 \tgenerator_loss: 0.8027388453483582\n",
            "For Step: 49670 recon_loss: 0.17009805142879486 \tdiscriminator_loss: 1.382046103477478 \tgenerator_loss: 0.7844079732894897\n",
            "For Step: 49680 recon_loss: 0.16141408681869507 \tdiscriminator_loss: 1.4572315216064453 \tgenerator_loss: 0.750437319278717\n",
            "For Step: 49690 recon_loss: 0.1613214612007141 \tdiscriminator_loss: 1.271456241607666 \tgenerator_loss: 0.8543269038200378\n",
            "For Step: 49700 recon_loss: 0.17254497110843658 \tdiscriminator_loss: 1.3315825462341309 \tgenerator_loss: 0.7620958089828491\n",
            "For Step: 49710 recon_loss: 0.1697515994310379 \tdiscriminator_loss: 1.2993321418762207 \tgenerator_loss: 0.8416446447372437\n",
            "For Step: 49720 recon_loss: 0.17781974375247955 \tdiscriminator_loss: 1.3458616733551025 \tgenerator_loss: 0.7977560758590698\n",
            "For Step: 49730 recon_loss: 0.16637276113033295 \tdiscriminator_loss: 1.327092170715332 \tgenerator_loss: 0.7906632423400879\n",
            "For Step: 49740 recon_loss: 0.17098218202590942 \tdiscriminator_loss: 1.2378208637237549 \tgenerator_loss: 0.8115248680114746\n",
            "For Step: 49750 recon_loss: 0.1808721125125885 \tdiscriminator_loss: 1.3787076473236084 \tgenerator_loss: 0.7836089730262756\n",
            "For Step: 49760 recon_loss: 0.1698649823665619 \tdiscriminator_loss: 1.3679965734481812 \tgenerator_loss: 0.7956051826477051\n",
            "For Step: 49770 recon_loss: 0.1713630110025406 \tdiscriminator_loss: 1.3256765604019165 \tgenerator_loss: 0.7991434335708618\n",
            "For Step: 49780 recon_loss: 0.17141520977020264 \tdiscriminator_loss: 1.3941900730133057 \tgenerator_loss: 0.7439293265342712\n",
            "For Step: 49790 recon_loss: 0.16488684713840485 \tdiscriminator_loss: 1.2741338014602661 \tgenerator_loss: 0.7703152298927307\n",
            "For Step: 49800 recon_loss: 0.16551350057125092 \tdiscriminator_loss: 1.2320544719696045 \tgenerator_loss: 0.8030306100845337\n",
            "For Step: 49810 recon_loss: 0.15978404879570007 \tdiscriminator_loss: 1.309889554977417 \tgenerator_loss: 0.79689621925354\n",
            "For Step: 49820 recon_loss: 0.17475025355815887 \tdiscriminator_loss: 1.2173247337341309 \tgenerator_loss: 0.803761899471283\n",
            "For Step: 49830 recon_loss: 0.15236829221248627 \tdiscriminator_loss: 1.2524769306182861 \tgenerator_loss: 0.809542179107666\n",
            "For Step: 49840 recon_loss: 0.16280867159366608 \tdiscriminator_loss: 1.3228261470794678 \tgenerator_loss: 0.7620869874954224\n",
            "For Step: 49850 recon_loss: 0.16735470294952393 \tdiscriminator_loss: 1.2508141994476318 \tgenerator_loss: 0.8183721303939819\n",
            "For Step: 49860 recon_loss: 0.18085739016532898 \tdiscriminator_loss: 1.3763599395751953 \tgenerator_loss: 0.788686990737915\n",
            "For Step: 49870 recon_loss: 0.1785689890384674 \tdiscriminator_loss: 1.3541265726089478 \tgenerator_loss: 0.7683612108230591\n",
            "For Step: 49880 recon_loss: 0.16890574991703033 \tdiscriminator_loss: 1.292590618133545 \tgenerator_loss: 0.8241656422615051\n",
            "For Step: 49890 recon_loss: 0.181146040558815 \tdiscriminator_loss: 1.3538421392440796 \tgenerator_loss: 0.7750205993652344\n",
            "For Step: 49900 recon_loss: 0.16246524453163147 \tdiscriminator_loss: 1.2858389616012573 \tgenerator_loss: 0.8138853311538696\n",
            "For Step: 49910 recon_loss: 0.16671669483184814 \tdiscriminator_loss: 1.3484435081481934 \tgenerator_loss: 0.7779675722122192\n",
            "For Step: 49920 recon_loss: 0.16532368957996368 \tdiscriminator_loss: 1.3034801483154297 \tgenerator_loss: 0.7922896146774292\n",
            "For Step: 49930 recon_loss: 0.18870200216770172 \tdiscriminator_loss: 1.3042258024215698 \tgenerator_loss: 0.7574753761291504\n",
            "For Step: 49940 recon_loss: 0.1775953471660614 \tdiscriminator_loss: 1.3751823902130127 \tgenerator_loss: 0.7874019145965576\n",
            "For Step: 49950 recon_loss: 0.18783530592918396 \tdiscriminator_loss: 1.2126250267028809 \tgenerator_loss: 0.8060746788978577\n",
            "For Step: 49960 recon_loss: 0.1812008023262024 \tdiscriminator_loss: 1.2967805862426758 \tgenerator_loss: 0.7546829581260681\n",
            "For Step: 49970 recon_loss: 0.15935981273651123 \tdiscriminator_loss: 1.2794859409332275 \tgenerator_loss: 0.7953474521636963\n",
            "For Step: 49980 recon_loss: 0.16593410074710846 \tdiscriminator_loss: 1.2655773162841797 \tgenerator_loss: 0.7810291051864624\n",
            "For Step: 49990 recon_loss: 0.16632787883281708 \tdiscriminator_loss: 1.3109163045883179 \tgenerator_loss: 0.7722734212875366\n",
            "For Step: 50000 recon_loss: 0.16610483825206757 \tdiscriminator_loss: 1.2116644382476807 \tgenerator_loss: 0.8227850198745728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MJ4te1OfM-V2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AK4dETMYXXdh",
        "colab_type": "code",
        "outputId": "1cb93b86-c5f8-4dd6-fe9a-2b0a0dc3b629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(reconstruction_loss)\n",
        "#plt.plot(discriminator_loss)\n",
        "plt.plot(generator_loss)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6938395f28>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RPh6GrAHXaEf",
        "colab_type": "code",
        "outputId": "a87025ed-7287-4cdf-cbc4-49b9b1eb2824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(gen_learning_rate)\n",
        "plt.plot(reg_learning_rate)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6937f11080>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAD8CAYAAABU4IIeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6spgkSR19f8AAV8h6TA6Xm0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "lIry3MF_Xckp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_images():\n",
        "  noise = torch.randn(100, latent_dim+10, device = device)\n",
        "  generated = decoder(noise).reshape(-1, 28, 28)\n",
        "  generated = 0.5 * generated + 0.5\n",
        "  gen = generated.cpu()\n",
        "  return gen.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "61e6xYffXdk3",
        "colab_type": "code",
        "outputId": "c6a5dbc8-1f27-4ea2-b048-b92181fc08a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "gen_img = save_images()\n",
        "\n",
        "fig, axs = plt.subplots(10, 10)\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        axs[i,j].imshow(gen_img[count, :,:,], cmap='gray')\n",
        "        axs[i,j].axis('off')\n",
        "        count += 1"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD8CAYAAACxUoU3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncNYCgUCQAghnLRAIBCnA/wNTE30LfuaJSQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 100 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}